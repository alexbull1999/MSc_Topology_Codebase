Starting Surface Distance Metric Analysis job...
Job ID: 183983
Node: gpuvm14
Time: Sat 19 Jul 14:22:29 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 19 14:22:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   36C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_142242
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_142242/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1500
  Balanced sampling: True
  Train batches: 365
  Val batches: 365
  Test batches: 367
Data loading completed!
  Train: 549367 samples, 365 batches
  Val: 549367 samples, 365 batches
  Test: 549367 samples, 367 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 300
  Hidden dims: [1024, 768, 512, 400, 350]
  Dropout rate: 0.15
  Total parameters: 6,412,568
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
Pre-processing target diagrams and converting to tensors...
  entailment: shape torch.Size([1490, 2])
  neutral: shape torch.Size([2043, 2])
  contradiction: shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 6,412,568
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 1.0
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=3.1431 (C:2.0000, R:0.0110, T:1.1420(w:1.000)🚀)
Batch  25/365: Loss=3.1429 (C:2.0000, R:0.0104, T:1.1419(w:1.000)🚀)
Batch  50/365: Loss=3.1422 (C:1.9994, R:0.0101, T:1.1418(w:1.000)🚀)
Batch  75/365: Loss=3.0248 (C:1.8816, R:0.0100, T:1.1422(w:1.000)🚀)
Batch 100/365: Loss=2.9065 (C:1.7627, R:0.0100, T:1.1428(w:1.000)🚀)
Batch 125/365: Loss=2.8042 (C:1.6610, R:0.0100, T:1.1422(w:1.000)🚀)
Batch 150/365: Loss=2.7053 (C:1.5623, R:0.0100, T:1.1420(w:1.000)🚀)
Batch 175/365: Loss=2.6830 (C:1.5400, R:0.0100, T:1.1420(w:1.000)🚀)
Batch 200/365: Loss=2.5611 (C:1.4181, R:0.0100, T:1.1420(w:1.000)🚀)
Batch 225/365: Loss=2.5771 (C:1.4344, R:0.0099, T:1.1418(w:1.000)🚀)
Batch 250/365: Loss=2.5504 (C:1.4077, R:0.0100, T:1.1417(w:1.000)🚀)
Batch 275/365: Loss=2.5696 (C:1.4269, R:0.0100, T:1.1418(w:1.000)🚀)
Batch 300/365: Loss=2.5213 (C:1.3786, R:0.0099, T:1.1417(w:1.000)🚀)
Batch 325/365: Loss=2.5281 (C:1.3855, R:0.0099, T:1.1416(w:1.000)🚀)
Batch 350/365: Loss=2.5659 (C:1.4233, R:0.0100, T:1.1416(w:1.000)🚀)
🎉 MILESTONE: First topological learning detected at epoch 1!
   Initial topological loss: 1.1480
📈 New best topological loss: 1.1480

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 2.7438
  Contrastive: 1.5948
  Reconstruction: 0.0100
  Topological: 1.1480 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.4026
  Contrastive: 1.2603
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (382.9s)
Train Loss: 2.7438 (C:1.5948, R:0.0100, T:1.1480)
Val Loss:   2.4026 (C:1.2603, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2.4394 (C:1.2968, R:0.0100, T:1.1416(w:1.000)🚀)
Batch  25/365: Loss=2.4853 (C:1.3427, R:0.0100, T:1.1416(w:1.000)🚀)
Batch  50/365: Loss=2.4453 (C:1.3027, R:0.0099, T:1.1416(w:1.000)🚀)
Batch  75/365: Loss=2.4194 (C:1.2769, R:0.0099, T:1.1415(w:1.000)🚀)
Batch 100/365: Loss=2.5033 (C:1.3607, R:0.0100, T:1.1415(w:1.000)🚀)
Batch 125/365: Loss=2.4466 (C:1.3040, R:0.0100, T:1.1415(w:1.000)🚀)
Batch 150/365: Loss=2.3971 (C:1.2546, R:0.0099, T:1.1415(w:1.000)🚀)
Batch 175/365: Loss=2.4385 (C:1.2960, R:0.0100, T:1.1415(w:1.000)🚀)
Batch 200/365: Loss=2.4082 (C:1.2657, R:0.0100, T:1.1415(w:1.000)🚀)
Batch 225/365: Loss=2.4053 (C:1.2628, R:0.0100, T:1.1415(w:1.000)🚀)
Batch 250/365: Loss=2.4159 (C:1.2734, R:0.0100, T:1.1415(w:1.000)🚀)
Batch 275/365: Loss=2.4575 (C:1.3150, R:0.0099, T:1.1415(w:1.000)🚀)
Batch 300/365: Loss=2.3926 (C:1.2502, R:0.0099, T:1.1415(w:1.000)🚀)
Batch 325/365: Loss=2.4366 (C:1.2942, R:0.0099, T:1.1415(w:1.000)🚀)
Batch 350/365: Loss=2.3838 (C:1.2413, R:0.0100, T:1.1415(w:1.000)🚀)
📈 New best topological loss: 1.1415

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 2.4379
  Contrastive: 1.2953
  Reconstruction: 0.0100
  Topological: 1.1415 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.3483
  Contrastive: 1.2059
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (446.5s)
Train Loss: 2.4379 (C:1.2953, R:0.0100, T:1.1415)
Val Loss:   2.3483 (C:1.2059, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2.4379 (C:1.2955, R:0.0100, T:1.1415(w:1.000)🚀)
Batch  25/365: Loss=2.3701 (C:1.2276, R:0.0100, T:1.1415(w:1.000)🚀)
Batch  50/365: Loss=2.4071 (C:1.2647, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=2.3684 (C:1.2260, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=2.3724 (C:1.2300, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=2.3456 (C:1.2031, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=2.4149 (C:1.2725, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=2.3565 (C:1.2141, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=2.4338 (C:1.2914, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=2.3539 (C:1.2115, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=2.3685 (C:1.2261, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=2.3533 (C:1.2109, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=2.3731 (C:1.2307, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=2.3808 (C:1.2384, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=2.3547 (C:1.2122, R:0.0100, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 2.3862
  Contrastive: 1.2438
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.3136
  Contrastive: 1.1713
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 3/50 COMPLETE (544.1s)
Train Loss: 2.3862 (C:1.2438, R:0.0100, T:1.1414)
Val Loss:   2.3136 (C:1.1713, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2.3744 (C:1.2320, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=2.3901 (C:1.2477, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=2.3482 (C:1.2058, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=2.4086 (C:1.2662, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=2.3975 (C:1.2551, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=2.3403 (C:1.1979, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=2.3974 (C:1.2550, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=2.3732 (C:1.2308, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=2.3697 (C:1.2273, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=2.3182 (C:1.1758, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=2.3090 (C:1.1666, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=2.2787 (C:1.1363, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=2.3544 (C:1.2120, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=2.3631 (C:1.2208, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=2.3515 (C:1.2091, R:0.0099, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 2.3599
  Contrastive: 1.2175
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.2957
  Contrastive: 1.1533
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (477.0s)
Train Loss: 2.3599 (C:1.2175, R:0.0100, T:1.1414)
Val Loss:   2.2957 (C:1.1533, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2.3049 (C:1.1625, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=2.3936 (C:1.2512, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=2.3423 (C:1.1999, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=2.2953 (C:1.1530, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=2.3221 (C:1.1797, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=2.3217 (C:1.1793, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=2.3213 (C:1.1790, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=2.3443 (C:1.2019, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=2.3152 (C:1.1728, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=2.3535 (C:1.2111, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=2.3080 (C:1.1656, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=2.3687 (C:1.2263, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=2.3513 (C:1.2089, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=2.2985 (C:1.1561, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=2.3482 (C:1.2058, R:0.0099, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 2.3427
  Contrastive: 1.2004
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.2922
  Contrastive: 1.1498
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 5/50 COMPLETE (472.2s)
Train Loss: 2.3427 (C:1.2004, R:0.0100, T:1.1414)
Val Loss:   2.2922 (C:1.1498, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2.3530 (C:1.2106, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=2.3286 (C:1.1863, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=2.3436 (C:1.2012, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=2.3226 (C:1.1803, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=2.3717 (C:1.2294, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=2.3614 (C:1.2191, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=2.2745 (C:1.1321, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=2.2808 (C:1.1384, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=2.3320 (C:1.1897, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=2.3633 (C:1.2210, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=2.3355 (C:1.1931, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=2.2858 (C:1.1435, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=2.3169 (C:1.1745, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=2.3444 (C:1.2020, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=2.3556 (C:1.2133, R:0.0099, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 2.3284
  Contrastive: 1.1860
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.2752
  Contrastive: 1.1329
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 6/50 COMPLETE (305.6s)
Train Loss: 2.3284 (C:1.1860, R:0.0100, T:1.1414)
Val Loss:   2.2752 (C:1.1329, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2.2584 (C:1.1161, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=2.2917 (C:1.1493, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=2.2926 (C:1.1502, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=2.2558 (C:1.1134, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=2.2957 (C:1.1534, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=2.2957 (C:1.1533, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=2.3222 (C:1.1798, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=2.3074 (C:1.1651, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=2.3118 (C:1.1694, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=2.3031 (C:1.1608, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=2.2817 (C:1.1393, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=2.2764 (C:1.1340, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=2.3169 (C:1.1745, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=2.3281 (C:1.1858, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=2.3343 (C:1.1919, R:0.0099, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 2.3172
  Contrastive: 1.1748
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 2.2637
  Contrastive: 1.1213
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 7/50 COMPLETE (233.3s)
Train Loss: 2.3172 (C:1.1748, R:0.0100, T:1.1414)
Val Loss:   2.2637 (C:1.1213, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2.3417 (C:1.1994, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=2.2837 (C:1.1413, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=2.3170 (C:1.1747, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=2.2553 (C:1.1130, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=2.2824 (C:1.1401, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=2.2652 (C:1.1228, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=2.3426 (C:1.2002, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=2.2721 (C:1.1297, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=2.2911 (C:1.1488, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=2.3238 (C:1.1814, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=2.2822 (C:1.1398, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=2.2881 (C:1.1458, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=2.3166 (C:1.1743, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=2.2681 (C:1.1257, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=2.3965 (C:1.2541, R:0.0100, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 2.3065
  Contrastive: 1.1642
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
