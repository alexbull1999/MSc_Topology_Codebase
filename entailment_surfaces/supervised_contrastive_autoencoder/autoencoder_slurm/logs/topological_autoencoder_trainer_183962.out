Starting Surface Distance Metric Analysis job...
Job ID: 183962
Node: gpuvm13
Time: Sat 19 Jul 12:21:19 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 19 12:21:21 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_122128
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_122128/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1500
  Balanced sampling: True
  Train batches: 365
  Val batches: 365
  Test batches: 367
Data loading completed!
  Train: 549367 samples, 365 batches
  Val: 549367 samples, 365 batches
  Test: 549367 samples, 367 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 500
  Hidden dims: [1024, 768, 650, 575, 535]
  Dropout rate: 0.1
  Total parameters: 7,623,882
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
Pre-processing target diagrams and converting to tensors...
  > entailment: Stored wrapped prototype with tensor shape torch.Size([1490, 2])
  > neutral: Stored wrapped prototype with tensor shape torch.Size([2043, 2])
  > contradiction: Stored wrapped prototype with tensor shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 7,623,882
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 0.1
  Reconstruction weight: 0.1

======================================================================
ğŸ§  TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=2.1152 (C:2.0000, R:0.0109, T:0.1142(w:0.100)ğŸ‰)
Batch  25/365: Loss=2.0496 (C:1.9344, R:0.0104, T:0.1142(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.8779 (C:1.7627, R:0.0101, T:0.1142(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.7357 (C:1.6205, R:0.0100, T:0.1142(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.7202 (C:1.6051, R:0.0100, T:0.1142(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.5853 (C:1.4701, R:0.0100, T:0.1142(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.4902 (C:1.3750, R:0.0100, T:0.1142(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.5510 (C:1.4359, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.5416 (C:1.4264, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.4627 (C:1.3476, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.4914 (C:1.3762, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.4967 (C:1.3815, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.4952 (C:1.3801, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.4636 (C:1.3484, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.4621 (C:1.3469, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
ğŸ‰ MILESTONE: First topological learning detected at epoch 1!
   Initial topological loss: 0.1142
ğŸ“ˆ New best topological loss: 0.1142

ğŸ“Š EPOCH 1 TRAINING SUMMARY:
  Total Loss: 1.6189
  Contrastive: 1.5037
  Reconstruction: 0.0100
  Topological: 0.1142 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.3679
  Contrastive: 1.2527
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 1/50 COMPLETE (353.8s)
Train Loss: 1.6189 (C:1.5037, R:0.0100, T:0.1142)
Val Loss:   1.3679 (C:1.2527, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.4286 (C:1.3135, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.3745 (C:1.2594, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.4021 (C:1.2869, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.6756 (C:1.3156, R:0.0100, T:0.3590(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.4073 (C:1.2921, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.4048 (C:1.2897, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.3949 (C:1.2798, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.3900 (C:1.2749, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.3610 (C:1.2459, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.3590 (C:1.2439, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.3667 (C:1.2515, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.4232 (C:1.3081, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.4202 (C:1.3050, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.3948 (C:1.2796, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.3372 (C:1.2220, R:0.0099, T:0.1141(w:0.100)ğŸ‰)

ğŸ“Š EPOCH 2 TRAINING SUMMARY:
  Total Loss: 1.3957
  Contrastive: 1.2799
  Reconstruction: 0.0100
  Topological: 0.1148 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.3163
  Contrastive: 1.2012
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 2/50 COMPLETE (447.9s)
Train Loss: 1.3957 (C:1.2799, R:0.0100, T:0.1148)
Val Loss:   1.3163 (C:1.2012, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.3433 (C:1.2281, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.3984 (C:1.2832, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.3352 (C:1.2200, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.3809 (C:1.2658, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.3311 (C:1.2159, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.3638 (C:1.2487, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.3753 (C:1.2601, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.3430 (C:1.2279, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.4057 (C:1.2905, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.3700 (C:1.2549, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.3276 (C:1.2125, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.3531 (C:1.2379, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.2967 (C:1.1816, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.3813 (C:1.2662, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.3477 (C:1.2326, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
ğŸ“ˆ New best topological loss: 0.1141

ğŸ“Š EPOCH 3 TRAINING SUMMARY:
  Total Loss: 1.3501
  Contrastive: 1.2349
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.2954
  Contrastive: 1.1803
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 3/50 COMPLETE (488.9s)
Train Loss: 1.3501 (C:1.2349, R:0.0100, T:0.1141)
Val Loss:   1.2954 (C:1.1803, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.3804 (C:1.2653, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.3197 (C:1.2045, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.3101 (C:1.1949, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.3051 (C:1.1899, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.3281 (C:1.2130, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.3098 (C:1.1947, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.3050 (C:1.1899, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.3333 (C:1.2182, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.3185 (C:1.2034, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.2900 (C:1.1749, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.3452 (C:1.2301, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.3109 (C:1.1957, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.3424 (C:1.2273, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.3472 (C:1.2321, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.3254 (C:1.2103, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
ğŸ“ˆ New best topological loss: 0.1141

ğŸ“Š EPOCH 4 TRAINING SUMMARY:
  Total Loss: 1.3249
  Contrastive: 1.2097
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.2736
  Contrastive: 1.1585
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 4/50 COMPLETE (599.5s)
Train Loss: 1.3249 (C:1.2097, R:0.0100, T:0.1141)
Val Loss:   1.2736 (C:1.1585, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.3122 (C:1.1971, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.3695 (C:1.2544, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.2811 (C:1.1659, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.3075 (C:1.1924, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.3287 (C:1.2136, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.3811 (C:1.2660, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.3082 (C:1.1930, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.2540 (C:1.1389, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.3518 (C:1.2366, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.2231 (C:1.1080, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.2406 (C:1.1254, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.3456 (C:1.2305, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.3155 (C:1.2004, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.2366 (C:1.1215, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.2863 (C:1.1712, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
ğŸ“ˆ New best topological loss: 0.1141

ğŸ“Š EPOCH 5 TRAINING SUMMARY:
  Total Loss: 1.3096
  Contrastive: 1.1945
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.2611
  Contrastive: 1.1460
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 5/50 COMPLETE (741.1s)
Train Loss: 1.3096 (C:1.1945, R:0.0100, T:0.1141)
Val Loss:   1.2611 (C:1.1460, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.3192 (C:1.2041, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.2999 (C:1.1848, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.2851 (C:1.1700, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.2870 (C:1.1718, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.3274 (C:1.2123, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.2740 (C:1.1588, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.2918 (C:1.1767, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.2337 (C:1.1185, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.2307 (C:1.1156, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.3332 (C:1.2181, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.2803 (C:1.1652, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.3448 (C:1.2297, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.3532 (C:1.2381, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.3089 (C:1.1937, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.2600 (C:1.1448, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
ğŸ“ˆ New best topological loss: 0.1141

ğŸ“Š EPOCH 6 TRAINING SUMMARY:
  Total Loss: 1.2930
  Contrastive: 1.1778
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.2477
  Contrastive: 1.1325
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 6/50 COMPLETE (472.2s)
Train Loss: 1.2930 (C:1.1778, R:0.0100, T:0.1141)
Val Loss:   1.2477 (C:1.1325, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.2788 (C:1.1637, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.2650 (C:1.1498, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.2855 (C:1.1704, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.2451 (C:1.1299, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.2323 (C:1.1172, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.2688 (C:1.1537, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.2287 (C:1.1136, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.2546 (C:1.1394, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.2143 (C:1.0992, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.3072 (C:1.1921, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.2569 (C:1.1417, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.2878 (C:1.1727, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.2708 (C:1.1556, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.2533 (C:1.1382, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.2467 (C:1.1316, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
ğŸ“ˆ New best topological loss: 0.1141

ğŸ“Š EPOCH 7 TRAINING SUMMARY:
  Total Loss: 1.2779
  Contrastive: 1.1627
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.2367
  Contrastive: 1.1216
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 7/50 COMPLETE (329.8s)
Train Loss: 1.2779 (C:1.1627, R:0.0100, T:0.1141)
Val Loss:   1.2367 (C:1.1216, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.3050 (C:1.1898, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.2546 (C:1.1395, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.2812 (C:1.1660, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.3161 (C:1.2010, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.2479 (C:1.1327, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.2691 (C:1.1539, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.2726 (C:1.1575, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.2696 (C:1.1545, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.3101 (C:1.1949, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.2465 (C:1.1314, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.2987 (C:1.1835, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.2389 (C:1.1238, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.3321 (C:1.2170, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.2953 (C:1.1802, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.2177 (C:1.1026, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
ğŸ“ˆ New best topological loss: 0.1141

ğŸ“Š EPOCH 8 TRAINING SUMMARY:
  Total Loss: 1.2736
  Contrastive: 1.1585
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.2326
  Contrastive: 1.1174
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 8/50 COMPLETE (257.2s)
Train Loss: 1.2736 (C:1.1585, R:0.0100, T:0.1141)
Val Loss:   1.2326 (C:1.1174, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.2311 (C:1.1160, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.2640 (C:1.1489, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.2460 (C:1.1308, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.2353 (C:1.1202, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.2075 (C:1.0923, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.2317 (C:1.1166, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.2751 (C:1.1599, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.2780 (C:1.1629, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.2523 (C:1.1372, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.2536 (C:1.1385, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.1851 (C:1.0699, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.2890 (C:1.1738, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.3155 (C:1.2004, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.2818 (C:1.1667, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.2745 (C:1.1594, R:0.0100, T:0.1141(w:0.100)ğŸ‰)

ğŸ“Š EPOCH 9 TRAINING SUMMARY:
  Total Loss: 1.2640
  Contrastive: 1.1489
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.2208
  Contrastive: 1.1056
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 9/50 COMPLETE (265.7s)
Train Loss: 1.2640 (C:1.1489, R:0.0100, T:0.1141)
Val Loss:   1.2208 (C:1.1056, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.2294 (C:1.1143, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.2321 (C:1.1170, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.2315 (C:1.1163, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.2247 (C:1.1096, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.2582 (C:1.1430, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.2426 (C:1.1275, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.2315 (C:1.1164, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.2695 (C:1.1544, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.2115 (C:1.0964, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.2577 (C:1.1426, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.2537 (C:1.1386, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.2177 (C:1.1026, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.2777 (C:1.1626, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.2956 (C:1.1805, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.2418 (C:1.1266, R:0.0099, T:0.1141(w:0.100)ğŸ‰)

ğŸ“Š EPOCH 10 TRAINING SUMMARY:
  Total Loss: 1.2511
  Contrastive: 1.1359
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.2082
  Contrastive: 1.0931
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 10/50 COMPLETE (280.0s)
Train Loss: 1.2511 (C:1.1359, R:0.0100, T:0.1141)
Val Loss:   1.2082 (C:1.0931, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.2266 (C:1.1115, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.2146 (C:1.0995, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.3119 (C:1.1968, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.3116 (C:1.1964, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.2377 (C:1.1226, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.2071 (C:1.0920, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.2369 (C:1.1217, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.2540 (C:1.1389, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.2337 (C:1.1186, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.2861 (C:1.1710, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.2194 (C:1.1042, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.2356 (C:1.1205, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.1565 (C:1.0414, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.2628 (C:1.1476, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.2692 (C:1.1541, R:0.0100, T:0.1141(w:0.100)ğŸ‰)

ğŸ“Š EPOCH 11 TRAINING SUMMARY:
  Total Loss: 1.2438
  Contrastive: 1.1287
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.2187
  Contrastive: 1.1036
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ¯ EPOCH 11/50 COMPLETE (265.2s)
Train Loss: 1.2438 (C:1.1287, R:0.0100, T:0.1141)
Val Loss:   1.2187 (C:1.1036, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.1798 (C:1.0647, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.1983 (C:1.0832, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.2700 (C:1.1548, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.2244 (C:1.1092, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.2671 (C:1.1520, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.1961 (C:1.0810, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.1672 (C:1.0521, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.2201 (C:1.1049, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.2246 (C:1.1095, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.2549 (C:1.1397, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.2558 (C:1.1407, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.1960 (C:1.0808, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.2195 (C:1.1043, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.2746 (C:1.1595, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.1537 (C:1.0385, R:0.0099, T:0.1141(w:0.100)ğŸ‰)

ğŸ“Š EPOCH 12 TRAINING SUMMARY:
  Total Loss: 1.2387
  Contrastive: 1.1235
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.2035
  Contrastive: 1.0884
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 12/50 COMPLETE (267.8s)
Train Loss: 1.2387 (C:1.1235, R:0.0100, T:0.1141)
Val Loss:   1.2035 (C:1.0884, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.2490 (C:1.1339, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.1994 (C:1.0843, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.2401 (C:1.1249, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.2677 (C:1.1526, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.2514 (C:1.1363, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.2426 (C:1.1275, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.2206 (C:1.1055, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.1911 (C:1.0760, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.2587 (C:1.1436, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.2637 (C:1.1486, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.2355 (C:1.1204, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.1971 (C:1.0820, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.1920 (C:1.0768, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.2544 (C:1.1392, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.2793 (C:1.1642, R:0.0100, T:0.1141(w:0.100)ğŸ‰)

ğŸ“Š EPOCH 13 TRAINING SUMMARY:
  Total Loss: 1.2321
  Contrastive: 1.1170
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.1903
  Contrastive: 1.0751
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 13/50 COMPLETE (275.8s)
Train Loss: 1.2321 (C:1.1170, R:0.0100, T:0.1141)
Val Loss:   1.1903 (C:1.0751, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.1855 (C:1.0704, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.2338 (C:1.1187, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.2115 (C:1.0964, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.2498 (C:1.1347, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.2027 (C:1.0876, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.2397 (C:1.1245, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.2779 (C:1.1627, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.2084 (C:1.0933, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.2056 (C:1.0905, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.2097 (C:1.0946, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.2263 (C:1.1111, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.2012 (C:1.0861, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.2129 (C:1.0977, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.2585 (C:1.1433, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.2251 (C:1.1100, R:0.0100, T:0.1141(w:0.100)ğŸ‰)

ğŸ“Š EPOCH 14 TRAINING SUMMARY:
  Total Loss: 1.2252
  Contrastive: 1.1100
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.1814
  Contrastive: 1.0663
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 14/50 COMPLETE (303.2s)
Train Loss: 1.2252 (C:1.1100, R:0.0100, T:0.1141)
Val Loss:   1.1814 (C:1.0663, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.2463 (C:1.1312, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.2025 (C:1.0873, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.2517 (C:1.1366, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.1688 (C:1.0536, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.1883 (C:1.0732, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.1840 (C:1.0689, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.2007 (C:1.0856, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.1949 (C:1.0798, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.2117 (C:1.0965, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.2299 (C:1.1148, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.1521 (C:1.0369, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.2532 (C:1.1381, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.2442 (C:1.1291, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.1918 (C:1.0767, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.2027 (C:1.0876, R:0.0100, T:0.1141(w:0.100)ğŸ‰)

ğŸ“Š EPOCH 15 TRAINING SUMMARY:
  Total Loss: 1.2187
  Contrastive: 1.1036
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.1843
  Contrastive: 1.0692
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ¯ EPOCH 15/50 COMPLETE (294.1s)
Train Loss: 1.2187 (C:1.1036, R:0.0100, T:0.1141)
Val Loss:   1.1843 (C:1.0692, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.2005 (C:1.0853, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.1542 (C:1.0391, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.2274 (C:1.1122, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.2182 (C:1.1031, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.2239 (C:1.1087, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.1849 (C:1.0697, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.2262 (C:1.1111, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.1824 (C:1.0673, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.2378 (C:1.1227, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.2505 (C:1.1354, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 250/365: Loss=1.2485 (C:1.1333, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 275/365: Loss=1.1851 (C:1.0700, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 300/365: Loss=1.2810 (C:1.1658, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 325/365: Loss=1.2524 (C:1.1373, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 350/365: Loss=1.2775 (C:1.1623, R:0.0099, T:0.1141(w:0.100)ğŸ‰)

ğŸ“Š EPOCH 16 TRAINING SUMMARY:
  Total Loss: 1.2138
  Contrastive: 1.0987
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 1.1824
  Contrastive: 1.0673
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

ğŸ¯ EPOCH 16/50 COMPLETE (274.1s)
Train Loss: 1.2138 (C:1.0987, R:0.0100, T:0.1141)
Val Loss:   1.1824 (C:1.0673, R:0.0100, T:0.1141)
ğŸ‰ Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 365 | Topological Weight: 0.1000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=1.1957 (C:1.0805, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  25/365: Loss=1.2283 (C:1.1132, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch  50/365: Loss=1.2497 (C:1.1346, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch  75/365: Loss=1.1421 (C:1.0269, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 100/365: Loss=1.2474 (C:1.1323, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 125/365: Loss=1.2091 (C:1.0940, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 150/365: Loss=1.2565 (C:1.1414, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 175/365: Loss=1.1863 (C:1.0712, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
Batch 200/365: Loss=1.1612 (C:1.0461, R:0.0100, T:0.1141(w:0.100)ğŸ‰)
Batch 225/365: Loss=1.2029 (C:1.0877, R:0.0099, T:0.1141(w:0.100)ğŸ‰)
