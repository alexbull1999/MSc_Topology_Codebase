Starting Surface Distance Metric Analysis job...
Job ID: 184031
Node: gpuvm13
Time: Sat 19 Jul 16:22:31 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 19 16:22:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   38C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_162239
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_162239/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1500
  Balanced sampling: True
  Train batches: 365
  Val batches: 365
  Test batches: 367
Data loading completed!
  Train: 549367 samples, 365 batches
  Val: 549367 samples, 365 batches
  Test: 549367 samples, 367 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 50
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 5,852,466
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
Pre-processing target diagrams and converting to tensors...
  entailment: shape torch.Size([1490, 2])
  neutral: shape torch.Size([2043, 2])
  contradiction: shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 5,852,466
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 10.0
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=8.3718 (C:2.0000, R:0.0110, T:6.3707(w:10.000)🚀)
Batch  25/365: Loss=8.3655 (C:1.9999, R:0.0100, T:6.3645(w:10.000)🚀)
Batch  50/365: Loss=8.3636 (C:1.9981, R:0.0100, T:6.3645(w:10.000)🚀)
Batch  75/365: Loss=8.3486 (C:1.9679, R:0.0099, T:6.3797(w:10.000)🚀)
Batch 100/365: Loss=8.3332 (C:1.9475, R:0.0099, T:6.3847(w:10.000)🚀)
Batch 125/365: Loss=8.2442 (C:1.8671, R:0.0100, T:6.3760(w:10.000)🚀)
Batch 150/365: Loss=8.2131 (C:1.8404, R:0.0100, T:6.3717(w:10.000)🚀)
Batch 175/365: Loss=8.1974 (C:1.8279, R:0.0099, T:6.3685(w:10.000)🚀)
Batch 200/365: Loss=8.1888 (C:1.8186, R:0.0099, T:6.3692(w:10.000)🚀)
Batch 225/365: Loss=8.1480 (C:1.7793, R:0.0100, T:6.3677(w:10.000)🚀)
Batch 250/365: Loss=8.0802 (C:1.7131, R:0.0099, T:6.3662(w:10.000)🚀)
Batch 275/365: Loss=8.0403 (C:1.6731, R:0.0100, T:6.3662(w:10.000)🚀)
Batch 300/365: Loss=7.9753 (C:1.6087, R:0.0100, T:6.3655(w:10.000)🚀)
Batch 325/365: Loss=7.9584 (C:1.5925, R:0.0100, T:6.3650(w:10.000)🚀)
Batch 350/365: Loss=7.9237 (C:1.5580, R:0.0100, T:6.3647(w:10.000)🚀)
🎉 MILESTONE: First topological learning detected at epoch 1!
   Initial topological loss: 6.3694
📈 New best topological loss: 6.3694

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 8.1769
  Contrastive: 1.8065
  Reconstruction: 0.0100
  Topological: 6.3694 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 7.8295
  Contrastive: 1.4644
  Reconstruction: 0.0100
  Topological: 6.3641 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (130.0s)
Train Loss: 8.1769 (C:1.8065, R:0.0100, T:6.3694)
Val Loss:   7.8295 (C:1.4644, R:0.0100, T:6.3641)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=7.8530 (C:1.4874, R:0.0099, T:6.3646(w:10.000)🚀)
Batch  25/365: Loss=7.8871 (C:1.5218, R:0.0099, T:6.3644(w:10.000)🚀)
Batch  50/365: Loss=7.8839 (C:1.5186, R:0.0099, T:6.3643(w:10.000)🚀)
Batch  75/365: Loss=7.8434 (C:1.4781, R:0.0100, T:6.3642(w:10.000)🚀)
Batch 100/365: Loss=7.8730 (C:1.5079, R:0.0099, T:6.3642(w:10.000)🚀)
Batch 125/365: Loss=7.7669 (C:1.4018, R:0.0099, T:6.3642(w:10.000)🚀)
Batch 150/365: Loss=7.8225 (C:1.4572, R:0.0100, T:6.3644(w:10.000)🚀)
Batch 175/365: Loss=7.8205 (C:1.4551, R:0.0100, T:6.3644(w:10.000)🚀)
Batch 200/365: Loss=7.8289 (C:1.4637, R:0.0099, T:6.3642(w:10.000)🚀)
Batch 225/365: Loss=7.8322 (C:1.4670, R:0.0100, T:6.3642(w:10.000)🚀)
Batch 250/365: Loss=7.7874 (C:1.4222, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 275/365: Loss=7.7766 (C:1.4114, R:0.0100, T:6.3642(w:10.000)🚀)
Batch 300/365: Loss=7.8096 (C:1.4444, R:0.0099, T:6.3642(w:10.000)🚀)
Batch 325/365: Loss=7.7705 (C:1.4054, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 350/365: Loss=7.7561 (C:1.3910, R:0.0099, T:6.3641(w:10.000)🚀)
📈 New best topological loss: 6.3642

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 7.8194
  Contrastive: 1.4541
  Reconstruction: 0.0100
  Topological: 6.3642 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 7.6937
  Contrastive: 1.3287
  Reconstruction: 0.0100
  Topological: 6.3641 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (121.3s)
Train Loss: 7.8194 (C:1.4541, R:0.0100, T:6.3642)
Val Loss:   7.6937 (C:1.3287, R:0.0100, T:6.3641)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=7.7789 (C:1.4137, R:0.0100, T:6.3641(w:10.000)🚀)
Batch  25/365: Loss=7.7210 (C:1.3558, R:0.0099, T:6.3641(w:10.000)🚀)
Batch  50/365: Loss=7.7262 (C:1.3612, R:0.0099, T:6.3641(w:10.000)🚀)
Batch  75/365: Loss=7.7102 (C:1.3451, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 100/365: Loss=7.7259 (C:1.3608, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 125/365: Loss=7.7203 (C:1.3552, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 150/365: Loss=7.6881 (C:1.3230, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 175/365: Loss=7.7450 (C:1.3799, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 200/365: Loss=7.6867 (C:1.3216, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 225/365: Loss=7.7892 (C:1.4241, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 250/365: Loss=7.8109 (C:1.4458, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 275/365: Loss=7.7228 (C:1.3577, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 300/365: Loss=7.7489 (C:1.3839, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 325/365: Loss=7.7363 (C:1.3712, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 350/365: Loss=7.7402 (C:1.3751, R:0.0100, T:6.3641(w:10.000)🚀)
📈 New best topological loss: 6.3641

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 7.7425
  Contrastive: 1.3774
  Reconstruction: 0.0100
  Topological: 6.3641 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 7.6520
  Contrastive: 1.2869
  Reconstruction: 0.0100
  Topological: 6.3641 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 3/50 COMPLETE (117.2s)
Train Loss: 7.7425 (C:1.3774, R:0.0100, T:6.3641)
Val Loss:   7.6520 (C:1.2869, R:0.0100, T:6.3641)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=7.7766 (C:1.4116, R:0.0099, T:6.3641(w:10.000)🚀)
Batch  25/365: Loss=7.7527 (C:1.3876, R:0.0099, T:6.3641(w:10.000)🚀)
Batch  50/365: Loss=7.7202 (C:1.3551, R:0.0100, T:6.3641(w:10.000)🚀)
Batch  75/365: Loss=7.6849 (C:1.3198, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 100/365: Loss=7.6698 (C:1.3047, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 125/365: Loss=7.7746 (C:1.4096, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 150/365: Loss=7.7558 (C:1.3907, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 175/365: Loss=7.7045 (C:1.3394, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 200/365: Loss=7.7227 (C:1.3576, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 225/365: Loss=7.7212 (C:1.3562, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 250/365: Loss=7.6692 (C:1.3042, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 275/365: Loss=7.7072 (C:1.3422, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 300/365: Loss=7.7489 (C:1.3838, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 325/365: Loss=7.7673 (C:1.4022, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 350/365: Loss=7.7029 (C:1.3378, R:0.0100, T:6.3641(w:10.000)🚀)
📈 New best topological loss: 6.3641

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 7.7174
  Contrastive: 1.3524
  Reconstruction: 0.0100
  Topological: 6.3641 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 7.6149
  Contrastive: 1.2498
  Reconstruction: 0.0100
  Topological: 6.3641 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (124.6s)
Train Loss: 7.7174 (C:1.3524, R:0.0100, T:6.3641)
Val Loss:   7.6149 (C:1.2498, R:0.0100, T:6.3641)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=7.7169 (C:1.3518, R:0.0100, T:6.3641(w:10.000)🚀)
Batch  25/365: Loss=7.7116 (C:1.3465, R:0.0100, T:6.3641(w:10.000)🚀)
Batch  50/365: Loss=7.7284 (C:1.3634, R:0.0100, T:6.3641(w:10.000)🚀)
Batch  75/365: Loss=7.7264 (C:1.3614, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 100/365: Loss=7.7090 (C:1.3440, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 125/365: Loss=7.7233 (C:1.3582, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 150/365: Loss=7.7363 (C:1.3712, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 175/365: Loss=7.6649 (C:1.2998, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 200/365: Loss=7.7598 (C:1.3947, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 225/365: Loss=7.6443 (C:1.2792, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 250/365: Loss=7.6992 (C:1.3341, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 275/365: Loss=7.7202 (C:1.3552, R:0.0100, T:6.3641(w:10.000)🚀)
Batch 300/365: Loss=7.6963 (C:1.3313, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 325/365: Loss=7.7485 (C:1.3834, R:0.0099, T:6.3641(w:10.000)🚀)
Batch 350/365: Loss=7.6764 (C:1.3114, R:0.0100, T:6.3641(w:10.000)🚀)
📈 New best topological loss: 6.3641

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 7.7049
  Contrastive: 1.3398
  Reconstruction: 0.0100
  Topological: 6.3641 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)
