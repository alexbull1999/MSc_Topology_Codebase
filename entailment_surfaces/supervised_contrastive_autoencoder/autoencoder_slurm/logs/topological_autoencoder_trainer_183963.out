Starting Surface Distance Metric Analysis job...
Job ID: 183963
Node: gpuvm14
Time: Sat 19 Jul 12:22:08 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 19 12:22:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_122222
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_122222/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1500
  Balanced sampling: True
  Train batches: 365
  Val batches: 365
  Test batches: 367
Data loading completed!
  Train: 549367 samples, 365 batches
  Val: 549367 samples, 365 batches
  Test: 549367 samples, 367 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 300
  Hidden dims: [1024, 768, 512, 400, 350]
  Dropout rate: 0.15
  Total parameters: 6,412,568
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
Pre-processing target diagrams and converting to tensors...
  > entailment: Stored wrapped prototype with tensor shape torch.Size([1490, 2])
  > neutral: Stored wrapped prototype with tensor shape torch.Size([2043, 2])
  > contradiction: Stored wrapped prototype with tensor shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 6,412,568
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 0.1
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=2.1153 (C:2.0000, R:0.0110, T:0.1142(w:0.100)🎉)
Batch  25/365: Loss=2.0995 (C:1.9842, R:0.0104, T:0.1142(w:0.100)🎉)
Batch  50/365: Loss=1.8886 (C:1.7733, R:0.0101, T:0.1143(w:0.100)🎉)
Batch  75/365: Loss=1.8118 (C:1.6966, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 100/365: Loss=1.7095 (C:1.5943, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 125/365: Loss=1.6745 (C:1.5593, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 150/365: Loss=1.6217 (C:1.5065, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 175/365: Loss=1.5499 (C:1.4348, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 200/365: Loss=1.5832 (C:1.4680, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 225/365: Loss=1.5244 (C:1.4092, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 250/365: Loss=1.5516 (C:1.4365, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 275/365: Loss=1.5081 (C:1.3929, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 300/365: Loss=1.4821 (C:1.3669, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 325/365: Loss=1.4530 (C:1.3378, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 350/365: Loss=1.4913 (C:1.3762, R:0.0099, T:0.1142(w:0.100)🎉)
🎉 MILESTONE: First topological learning detected at epoch 1!
   Initial topological loss: 0.1142
📈 New best topological loss: 0.1142

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 1.6430
  Contrastive: 1.5278
  Reconstruction: 0.0100
  Topological: 0.1142 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3652
  Contrastive: 1.2501
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (382.2s)
Train Loss: 1.6430 (C:1.5278, R:0.0100, T:0.1142)
Val Loss:   1.3652 (C:1.2501, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.4281 (C:1.3130, R:0.0100, T:0.1142(w:0.100)🎉)
Batch  25/365: Loss=1.4718 (C:1.3567, R:0.0100, T:0.1142(w:0.100)🎉)
Batch  50/365: Loss=1.3662 (C:1.2511, R:0.0100, T:0.1142(w:0.100)🎉)
Batch  75/365: Loss=1.3863 (C:1.2711, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 100/365: Loss=1.3786 (C:1.2635, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 125/365: Loss=1.4677 (C:1.3526, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 150/365: Loss=1.4195 (C:1.3044, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 175/365: Loss=1.3703 (C:1.2551, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 200/365: Loss=1.4356 (C:1.3204, R:0.0099, T:0.1142(w:0.100)🎉)
Batch 225/365: Loss=1.4043 (C:1.2891, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 250/365: Loss=1.4246 (C:1.3094, R:0.0099, T:0.1142(w:0.100)🎉)
Batch 275/365: Loss=1.4063 (C:1.2912, R:0.0100, T:0.1142(w:0.100)🎉)
Batch 300/365: Loss=1.3671 (C:1.2519, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.4177 (C:1.3025, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.4236 (C:1.3084, R:0.0099, T:0.1141(w:0.100)🎉)

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 1.4096
  Contrastive: 1.2938
  Reconstruction: 0.0100
  Topological: 0.1149 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3170
  Contrastive: 1.2019
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (468.4s)
Train Loss: 1.4096 (C:1.2938, R:0.0100, T:0.1149)
Val Loss:   1.3170 (C:1.2019, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.3836 (C:1.2684, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.4054 (C:1.2903, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.3606 (C:1.2454, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.4013 (C:1.2862, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.3865 (C:1.2713, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.3944 (C:1.2793, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.3871 (C:1.2719, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.3729 (C:1.2577, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.3308 (C:1.2157, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2733 (C:1.1582, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.3681 (C:1.2529, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.4183 (C:1.3031, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.3994 (C:1.2842, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.3979 (C:1.2828, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.3926 (C:1.2775, R:0.0100, T:0.1141(w:0.100)🎉)

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 1.3644
  Contrastive: 1.2473
  Reconstruction: 0.0100
  Topological: 0.1161 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2912
  Contrastive: 1.1760
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 3/50 COMPLETE (528.3s)
Train Loss: 1.3644 (C:1.2473, R:0.0100, T:0.1161)
Val Loss:   1.2912 (C:1.1760, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.3190 (C:1.2039, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.3681 (C:1.2529, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.3226 (C:1.2075, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.3125 (C:1.1974, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.3385 (C:1.2234, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.3109 (C:1.1958, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.3303 (C:1.2152, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2941 (C:1.1790, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.3877 (C:1.2725, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.3558 (C:1.2407, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.3339 (C:1.2188, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.3303 (C:1.2151, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.3442 (C:1.2291, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.3790 (C:1.2639, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.3494 (C:1.2342, R:0.0100, T:0.1141(w:0.100)🎉)
📈 New best topological loss: 0.1141

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 1.3326
  Contrastive: 1.2175
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2664
  Contrastive: 1.1513
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (604.6s)
Train Loss: 1.3326 (C:1.2175, R:0.0100, T:0.1141)
Val Loss:   1.2664 (C:1.1513, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.3251 (C:1.2100, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.3661 (C:1.2510, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.3934 (C:1.2782, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.2429 (C:1.1277, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.2896 (C:1.1745, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.3153 (C:1.2002, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2950 (C:1.1799, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.3169 (C:1.2018, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.2932 (C:1.1780, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2707 (C:1.1555, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.3943 (C:1.2791, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.3172 (C:1.2021, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.3946 (C:1.2794, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.3248 (C:1.2096, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.3300 (C:1.2149, R:0.0099, T:0.1141(w:0.100)🎉)
📈 New best topological loss: 0.1141

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 1.3132
  Contrastive: 1.1981
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2590
  Contrastive: 1.1439
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 5/50 COMPLETE (366.3s)
Train Loss: 1.3132 (C:1.1981, R:0.0100, T:0.1141)
Val Loss:   1.2590 (C:1.1439, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2707 (C:1.1555, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.3487 (C:1.2336, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.2633 (C:1.1482, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.3257 (C:1.2106, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.2779 (C:1.1628, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.3394 (C:1.2243, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2580 (C:1.1428, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.3434 (C:1.2283, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.3240 (C:1.2089, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2679 (C:1.1528, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.3222 (C:1.2071, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.3070 (C:1.1918, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.2974 (C:1.1822, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.3664 (C:1.2513, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.3505 (C:1.2353, R:0.0099, T:0.1141(w:0.100)🎉)
📈 New best topological loss: 0.1141

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 1.3015
  Contrastive: 1.1863
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2508
  Contrastive: 1.1356
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 6/50 COMPLETE (296.8s)
Train Loss: 1.3015 (C:1.1863, R:0.0100, T:0.1141)
Val Loss:   1.2508 (C:1.1356, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2677 (C:1.1526, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.2321 (C:1.1169, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.2673 (C:1.1522, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.3201 (C:1.2050, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.3187 (C:1.2036, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2688 (C:1.1537, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2897 (C:1.1745, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2357 (C:1.1206, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.2507 (C:1.1356, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2880 (C:1.1728, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2711 (C:1.1560, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.2712 (C:1.1561, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.3052 (C:1.1900, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.3302 (C:1.2150, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.3864 (C:1.2713, R:0.0100, T:0.1141(w:0.100)🎉)
📈 New best topological loss: 0.1141

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 1.2907
  Contrastive: 1.1755
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2374
  Contrastive: 1.1222
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 7/50 COMPLETE (237.9s)
Train Loss: 1.2907 (C:1.1755, R:0.0100, T:0.1141)
Val Loss:   1.2374 (C:1.1222, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2991 (C:1.1840, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.3201 (C:1.2049, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.3140 (C:1.1989, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.2675 (C:1.1524, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.2563 (C:1.1412, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2732 (C:1.1581, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2568 (C:1.1416, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2521 (C:1.1370, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.3123 (C:1.1972, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2859 (C:1.1708, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2755 (C:1.1603, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.2409 (C:1.1258, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.3107 (C:1.1956, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.2876 (C:1.1725, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.2827 (C:1.1676, R:0.0099, T:0.1141(w:0.100)🎉)
📈 New best topological loss: 0.1141

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 1.2758
  Contrastive: 1.1607
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2244
  Contrastive: 1.1093
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 8/50 COMPLETE (206.0s)
Train Loss: 1.2758 (C:1.1607, R:0.0100, T:0.1141)
Val Loss:   1.2244 (C:1.1093, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.3073 (C:1.1921, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.3038 (C:1.1887, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.2375 (C:1.1223, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.2772 (C:1.1621, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.3076 (C:1.1924, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2899 (C:1.1747, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2691 (C:1.1540, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.1953 (C:1.0802, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.2580 (C:1.1429, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2294 (C:1.1143, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2808 (C:1.1656, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.2878 (C:1.1726, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.2003 (C:1.0851, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.3336 (C:1.2185, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.3320 (C:1.2168, R:0.0099, T:0.1141(w:0.100)🎉)

📊 EPOCH 9 TRAINING SUMMARY:
  Total Loss: 1.2688
  Contrastive: 1.1537
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2288
  Contrastive: 1.1136
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 9/50 COMPLETE (208.3s)
Train Loss: 1.2688 (C:1.1537, R:0.0100, T:0.1141)
Val Loss:   1.2288 (C:1.1136, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2959 (C:1.1808, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.2833 (C:1.1682, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.1760 (C:1.0609, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.2450 (C:1.1298, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.2743 (C:1.1592, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2508 (C:1.1356, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2380 (C:1.1229, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2643 (C:1.1492, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.2366 (C:1.1215, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2246 (C:1.1094, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2924 (C:1.1773, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.2671 (C:1.1519, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.2250 (C:1.1099, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.2434 (C:1.1283, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.2712 (C:1.1561, R:0.0100, T:0.1141(w:0.100)🎉)

📊 EPOCH 10 TRAINING SUMMARY:
  Total Loss: 1.2624
  Contrastive: 1.1473
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2075
  Contrastive: 1.0924
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 10/50 COMPLETE (215.1s)
Train Loss: 1.2624 (C:1.1473, R:0.0100, T:0.1141)
Val Loss:   1.2075 (C:1.0924, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2707 (C:1.1556, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.2397 (C:1.1245, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.2833 (C:1.1682, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.2533 (C:1.1382, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.3156 (C:1.2005, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2771 (C:1.1620, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2529 (C:1.1378, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2346 (C:1.1194, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.1645 (C:1.0493, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.3177 (C:1.2026, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2616 (C:1.1465, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.2406 (C:1.1255, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.3057 (C:1.1906, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.2301 (C:1.1149, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.3041 (C:1.1890, R:0.0099, T:0.1141(w:0.100)🎉)

📊 EPOCH 11 TRAINING SUMMARY:
  Total Loss: 1.2534
  Contrastive: 1.1382
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2000
  Contrastive: 1.0849
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 11/50 COMPLETE (200.4s)
Train Loss: 1.2534 (C:1.1382, R:0.0100, T:0.1141)
Val Loss:   1.2000 (C:1.0849, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2175 (C:1.1023, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.2764 (C:1.1612, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.2694 (C:1.1542, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.3281 (C:1.2129, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.2318 (C:1.1167, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2343 (C:1.1191, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2711 (C:1.1560, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2433 (C:1.1281, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.2763 (C:1.1612, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2252 (C:1.1101, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2452 (C:1.1301, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.2859 (C:1.1708, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.1996 (C:1.0845, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.2231 (C:1.1080, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.2439 (C:1.1288, R:0.0099, T:0.1141(w:0.100)🎉)

📊 EPOCH 12 TRAINING SUMMARY:
  Total Loss: 1.2438
  Contrastive: 1.1286
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1903
  Contrastive: 1.0752
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 12/50 COMPLETE (211.9s)
Train Loss: 1.2438 (C:1.1286, R:0.0100, T:0.1141)
Val Loss:   1.1903 (C:1.0752, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2393 (C:1.1242, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.2655 (C:1.1504, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.2584 (C:1.1433, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.1866 (C:1.0715, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.2202 (C:1.1051, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2217 (C:1.1066, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2434 (C:1.1283, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2372 (C:1.1220, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.1837 (C:1.0686, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2153 (C:1.1002, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2582 (C:1.1431, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.2484 (C:1.1333, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.2731 (C:1.1580, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.1891 (C:1.0740, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.2632 (C:1.1480, R:0.0100, T:0.1141(w:0.100)🎉)

📊 EPOCH 13 TRAINING SUMMARY:
  Total Loss: 1.2366
  Contrastive: 1.1214
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1864
  Contrastive: 1.0713
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 13/50 COMPLETE (215.6s)
Train Loss: 1.2366 (C:1.1214, R:0.0100, T:0.1141)
Val Loss:   1.1864 (C:1.0713, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2477 (C:1.1326, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.2761 (C:1.1610, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.2254 (C:1.1102, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.2187 (C:1.1035, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.2194 (C:1.1043, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2747 (C:1.1596, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2607 (C:1.1456, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2401 (C:1.1249, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.1882 (C:1.0731, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.1822 (C:1.0671, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2695 (C:1.1543, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.2651 (C:1.1500, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.2487 (C:1.1336, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.2322 (C:1.1171, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.2327 (C:1.1176, R:0.0099, T:0.1141(w:0.100)🎉)

📊 EPOCH 14 TRAINING SUMMARY:
  Total Loss: 1.2323
  Contrastive: 1.1171
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1771
  Contrastive: 1.0619
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 14/50 COMPLETE (217.5s)
Train Loss: 1.2323 (C:1.1171, R:0.0100, T:0.1141)
Val Loss:   1.1771 (C:1.0619, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2207 (C:1.1056, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.2083 (C:1.0931, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.2504 (C:1.1352, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.2122 (C:1.0971, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.2422 (C:1.1271, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2483 (C:1.1332, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2035 (C:1.0884, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2349 (C:1.1198, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.2182 (C:1.1030, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2356 (C:1.1205, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2560 (C:1.1409, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.1902 (C:1.0751, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.2472 (C:1.1321, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.2146 (C:1.0995, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.1900 (C:1.0748, R:0.0100, T:0.1141(w:0.100)🎉)

📊 EPOCH 15 TRAINING SUMMARY:
  Total Loss: 1.2274
  Contrastive: 1.1123
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1754
  Contrastive: 1.0603
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 15/50 COMPLETE (217.2s)
Train Loss: 1.2274 (C:1.1123, R:0.0100, T:0.1141)
Val Loss:   1.1754 (C:1.0603, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2202 (C:1.1051, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.2439 (C:1.1288, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.2522 (C:1.1370, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.2637 (C:1.1485, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.2883 (C:1.1731, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2974 (C:1.1823, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2456 (C:1.1305, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.1884 (C:1.0733, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.1982 (C:1.0830, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2308 (C:1.1157, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.1905 (C:1.0754, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.1663 (C:1.0512, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.2363 (C:1.1212, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.2419 (C:1.1268, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.2509 (C:1.1358, R:0.0099, T:0.1141(w:0.100)🎉)

📊 EPOCH 16 TRAINING SUMMARY:
  Total Loss: 1.2203
  Contrastive: 1.1051
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1863
  Contrastive: 1.0712
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 16/50 COMPLETE (219.8s)
Train Loss: 1.2203 (C:1.1051, R:0.0100, T:0.1141)
Val Loss:   1.1863 (C:1.0712, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2303 (C:1.1152, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.2067 (C:1.0915, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.1651 (C:1.0500, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.2220 (C:1.1069, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.2350 (C:1.1199, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2323 (C:1.1172, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2289 (C:1.1138, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.1944 (C:1.0792, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.1399 (C:1.0247, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2375 (C:1.1224, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2477 (C:1.1325, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.2651 (C:1.1500, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.2093 (C:1.0942, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.1632 (C:1.0481, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.1751 (C:1.0600, R:0.0100, T:0.1141(w:0.100)🎉)

📊 EPOCH 17 TRAINING SUMMARY:
  Total Loss: 1.2178
  Contrastive: 1.1026
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1680
  Contrastive: 1.0529
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 17/50 COMPLETE (230.4s)
Train Loss: 1.2178 (C:1.1026, R:0.0100, T:0.1141)
Val Loss:   1.1680 (C:1.0529, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2335 (C:1.1184, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.1739 (C:1.0588, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.2620 (C:1.1469, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.2641 (C:1.1490, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.1684 (C:1.0533, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.1555 (C:1.0404, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.1675 (C:1.0524, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2104 (C:1.0952, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.2349 (C:1.1198, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.1902 (C:1.0750, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2416 (C:1.1264, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.2276 (C:1.1125, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.2416 (C:1.1265, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.1902 (C:1.0750, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.1896 (C:1.0745, R:0.0099, T:0.1141(w:0.100)🎉)

📊 EPOCH 18 TRAINING SUMMARY:
  Total Loss: 1.2121
  Contrastive: 1.0970
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1639
  Contrastive: 1.0487
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 18/50 COMPLETE (215.4s)
Train Loss: 1.2121 (C:1.0970, R:0.0100, T:0.1141)
Val Loss:   1.1639 (C:1.0487, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 19 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2231 (C:1.1080, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.2111 (C:1.0960, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.1458 (C:1.0307, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.1682 (C:1.0531, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.1618 (C:1.0467, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2241 (C:1.1089, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.1516 (C:1.0365, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2621 (C:1.1469, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.2618 (C:1.1466, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.1892 (C:1.0740, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2178 (C:1.1027, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.1491 (C:1.0340, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.2178 (C:1.1027, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.2326 (C:1.1175, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.1797 (C:1.0646, R:0.0100, T:0.1141(w:0.100)🎉)

📊 EPOCH 19 TRAINING SUMMARY:
  Total Loss: 1.2082
  Contrastive: 1.0930
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1530
  Contrastive: 1.0378
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 19/50 COMPLETE (212.1s)
Train Loss: 1.2082 (C:1.0930, R:0.0100, T:0.1141)
Val Loss:   1.1530 (C:1.0378, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 20 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2029 (C:1.0878, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.1500 (C:1.0349, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.1911 (C:1.0760, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.1712 (C:1.0560, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.2083 (C:1.0932, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2393 (C:1.1242, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2439 (C:1.1287, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2056 (C:1.0905, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.2574 (C:1.1422, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.1986 (C:1.0835, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2162 (C:1.1010, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.1700 (C:1.0548, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.2665 (C:1.1514, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.2203 (C:1.1052, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.2152 (C:1.1001, R:0.0100, T:0.1141(w:0.100)🎉)

📊 EPOCH 20 TRAINING SUMMARY:
  Total Loss: 1.2028
  Contrastive: 1.0877
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1540
  Contrastive: 1.0389
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 20/50 COMPLETE (208.5s)
Train Loss: 1.2028 (C:1.0877, R:0.0100, T:0.1141)
Val Loss:   1.1540 (C:1.0389, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
------------------------------------------------------------

============================================================
EPOCH 21 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2096 (C:1.0944, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.1824 (C:1.0673, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.2134 (C:1.0982, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.2007 (C:1.0856, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.1253 (C:1.0101, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2246 (C:1.1094, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.2269 (C:1.1118, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.1945 (C:1.0794, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.1827 (C:1.0676, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.2550 (C:1.1398, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.1837 (C:1.0686, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.2241 (C:1.1090, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 300/365: Loss=1.2071 (C:1.0919, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 325/365: Loss=1.1866 (C:1.0715, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 350/365: Loss=1.1839 (C:1.0688, R:0.0100, T:0.1141(w:0.100)🎉)

📊 EPOCH 21 TRAINING SUMMARY:
  Total Loss: 1.1993
  Contrastive: 1.0841
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 1.0312
  Reconstruction: 0.0100
  Topological: 0.1141 (weight: 0.100)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 21/50 COMPLETE (215.2s)
Train Loss: 1.1993 (C:1.0841, R:0.0100, T:0.1141)
Val Loss:   1.1463 (C:1.0312, R:0.0100, T:0.1141)
🎉 Topological complexity detected!
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 22 | Batches: 365 | Topological Weight: 0.1000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.2065 (C:1.0914, R:0.0100, T:0.1141(w:0.100)🎉)
Batch  25/365: Loss=1.1749 (C:1.0598, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  50/365: Loss=1.1698 (C:1.0547, R:0.0099, T:0.1141(w:0.100)🎉)
Batch  75/365: Loss=1.1966 (C:1.0815, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 100/365: Loss=1.1569 (C:1.0417, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 125/365: Loss=1.2219 (C:1.1067, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 150/365: Loss=1.1533 (C:1.0382, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 175/365: Loss=1.2452 (C:1.1301, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 200/365: Loss=1.2050 (C:1.0898, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 225/365: Loss=1.1532 (C:1.0380, R:0.0099, T:0.1141(w:0.100)🎉)
Batch 250/365: Loss=1.2182 (C:1.1031, R:0.0100, T:0.1141(w:0.100)🎉)
Batch 275/365: Loss=1.2504 (C:1.1352, R:0.0099, T:0.1141(w:0.100)🎉)
