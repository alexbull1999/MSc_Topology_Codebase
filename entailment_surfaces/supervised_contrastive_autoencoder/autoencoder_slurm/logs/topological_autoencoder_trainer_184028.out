Starting Surface Distance Metric Analysis job...
Job ID: 184028
Node: gpuvm13
Time: Sat 19 Jul 15:57:22 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 19 15:57:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   34C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_155731
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_155731/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1500
  Balanced sampling: True
  Train batches: 365
  Val batches: 365
  Test batches: 367
Data loading completed!
  Train: 549367 samples, 365 batches
  Val: 549367 samples, 365 batches
  Test: 549367 samples, 367 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 50
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 5,852,466
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
Pre-processing target diagrams and converting to tensors...
  entailment: shape torch.Size([1490, 2])
  neutral: shape torch.Size([2043, 2])
  contradiction: shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 5,852,466
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 10.0
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=2.0011 (C:2.0000, R:0.0110, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.7805 (C:1.7795, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.5680 (C:1.5670, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.5514 (C:1.5504, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.4814 (C:1.4804, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.5365 (C:1.5355, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.4812 (C:1.4802, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.4480 (C:1.4470, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.4257 (C:1.4247, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.4162 (C:1.4152, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.4076 (C:1.4066, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.4908 (C:1.4898, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.4601 (C:1.4591, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3673 (C:1.3663, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.4490 (C:1.4480, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 1.5020
  Contrastive: 1.5010
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3257
  Contrastive: 1.3247
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (22.2s)
Train Loss: 1.5020 (C:1.5010, R:0.0100, T:0.0000)
Val Loss:   1.3257 (C:1.3247, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3897 (C:1.3887, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3370 (C:1.3360, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.4528 (C:1.4518, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3638 (C:1.3629, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.4462 (C:1.4452, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3959 (C:1.3949, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.4068 (C:1.4058, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3692 (C:1.3682, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3919 (C:1.3909, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.4028 (C:1.4018, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3495 (C:1.3485, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3644 (C:1.3634, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3702 (C:1.3693, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3847 (C:1.3837, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3352 (C:1.3342, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 1.3801
  Contrastive: 1.3791
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.3038
  Contrastive: 1.3028
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (23.9s)
Train Loss: 1.3801 (C:1.3791, R:0.0100, T:0.0000)
Val Loss:   1.3038 (C:1.3028, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3658 (C:1.3648, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3229 (C:1.3219, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.4036 (C:1.4026, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3327 (C:1.3317, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3817 (C:1.3807, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3784 (C:1.3774, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3383 (C:1.3373, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.3957 (C:1.3947, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3305 (C:1.3295, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3477 (C:1.3467, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3989 (C:1.3979, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3499 (C:1.3489, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3555 (C:1.3545, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2660 (C:1.2650, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3841 (C:1.3831, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 1.3524
  Contrastive: 1.3514
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2842
  Contrastive: 1.2832
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 3/50 COMPLETE (23.1s)
Train Loss: 1.3524 (C:1.3514, R:0.0100, T:0.0000)
Val Loss:   1.2842 (C:1.2832, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2940 (C:1.2930, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3048 (C:1.3038, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3486 (C:1.3476, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3327 (C:1.3317, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3417 (C:1.3407, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3106 (C:1.3096, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3799 (C:1.3789, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2714 (C:1.2704, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.2992 (C:1.2982, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3399 (C:1.3389, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3161 (C:1.3151, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3680 (C:1.3670, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3373 (C:1.3363, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3482 (C:1.3472, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3313 (C:1.3304, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 1.3358
  Contrastive: 1.3348
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2556
  Contrastive: 1.2546
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (23.9s)
Train Loss: 1.3358 (C:1.3348, R:0.0100, T:0.0000)
Val Loss:   1.2556 (C:1.2546, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.3369 (C:1.3359, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3202 (C:1.3192, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3093 (C:1.3083, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3300 (C:1.3290, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3182 (C:1.3172, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2900 (C:1.2890, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3048 (C:1.3038, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2862 (C:1.2852, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.2897 (C:1.2887, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3321 (C:1.3311, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3078 (C:1.3068, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3002 (C:1.2992, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2891 (C:1.2881, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.3192 (C:1.3182, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3458 (C:1.3448, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 1.3163
  Contrastive: 1.3153
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2543
  Contrastive: 1.2533
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 5/50 COMPLETE (23.1s)
Train Loss: 1.3163 (C:1.3153, R:0.0100, T:0.0000)
Val Loss:   1.2543 (C:1.2533, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2459 (C:1.2449, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2961 (C:1.2951, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.2531 (C:1.2521, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2985 (C:1.2975, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3312 (C:1.3303, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2945 (C:1.2935, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2548 (C:1.2538, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2919 (C:1.2910, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.2313 (C:1.2303, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3295 (C:1.3285, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2517 (C:1.2507, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3466 (C:1.3456, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3558 (C:1.3548, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2624 (C:1.2614, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2915 (C:1.2905, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 1.3056
  Contrastive: 1.3046
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2266
  Contrastive: 1.2256
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 6/50 COMPLETE (23.2s)
Train Loss: 1.3056 (C:1.3046, R:0.0100, T:0.0000)
Val Loss:   1.2266 (C:1.2256, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2737 (C:1.2728, R:0.0099, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2667 (C:1.2657, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.2440 (C:1.2430, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3402 (C:1.3392, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.3339 (C:1.3329, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3253 (C:1.3243, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2946 (C:1.2936, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2656 (C:1.2646, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3139 (C:1.3129, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2967 (C:1.2957, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.3368 (C:1.3358, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2535 (C:1.2525, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3003 (C:1.2993, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2895 (C:1.2885, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.2468 (C:1.2458, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 1.2965
  Contrastive: 1.2955
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2389
  Contrastive: 1.2379
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

🎯 EPOCH 7/50 COMPLETE (23.0s)
Train Loss: 1.2965 (C:1.2955, R:0.0100, T:0.0000)
Val Loss:   1.2389 (C:1.2379, R:0.0100, T:0.0000)
❌ No topological learning yet
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2992 (C:1.2982, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2643 (C:1.2633, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3283 (C:1.3273, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3082 (C:1.3072, R:0.0099, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2540 (C:1.2530, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2447 (C:1.2437, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3083 (C:1.3073, R:0.0099, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2922 (C:1.2912, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.3149 (C:1.3139, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.3100 (C:1.3090, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2994 (C:1.2984, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2568 (C:1.2558, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3450 (C:1.3440, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2919 (C:1.2909, R:0.0100, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3619 (C:1.3609, R:0.0099, T:0.0000(w:0.000)❌)

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 1.2860
  Contrastive: 1.2850
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2153
  Contrastive: 1.2143
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 8/50 COMPLETE (23.3s)
Train Loss: 1.2860 (C:1.2850, R:0.0100, T:0.0000)
Val Loss:   1.2153 (C:1.2143, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2957 (C:1.2947, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.2504 (C:1.2494, R:0.0100, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.3199 (C:1.3189, R:0.0099, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.3016 (C:1.3006, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2762 (C:1.2752, R:0.0099, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.3051 (C:1.3041, R:0.0100, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.3159 (C:1.3149, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2391 (C:1.2381, R:0.0099, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.1987 (C:1.1977, R:0.0100, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2723 (C:1.2713, R:0.0100, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2838 (C:1.2828, R:0.0100, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.2547 (C:1.2537, R:0.0099, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.3028 (C:1.3018, R:0.0100, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2945 (C:1.2936, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.3042 (C:1.3032, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 9 TRAINING SUMMARY:
  Total Loss: 1.2773
  Contrastive: 1.2763
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.2028
  Contrastive: 1.2018
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 9/50 COMPLETE (23.4s)
Train Loss: 1.2773 (C:1.2763, R:0.0100, T:0.0000)
Val Loss:   1.2028 (C:1.2018, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 365 | Topological Weight: 0.0000
🔄 Topological warmup phase
============================================================
Batch   0/365: Loss=1.2933 (C:1.2923, R:0.0100, T:0.0000(w:0.000)❌)
Batch  25/365: Loss=1.3405 (C:1.3395, R:0.0099, T:0.0000(w:0.000)❌)
Batch  50/365: Loss=1.2470 (C:1.2460, R:0.0100, T:0.0000(w:0.000)❌)
Batch  75/365: Loss=1.2563 (C:1.2554, R:0.0100, T:0.0000(w:0.000)❌)
Batch 100/365: Loss=1.2960 (C:1.2950, R:0.0100, T:0.0000(w:0.000)❌)
Batch 125/365: Loss=1.2938 (C:1.2928, R:0.0099, T:0.0000(w:0.000)❌)
Batch 150/365: Loss=1.2766 (C:1.2756, R:0.0100, T:0.0000(w:0.000)❌)
Batch 175/365: Loss=1.2673 (C:1.2663, R:0.0100, T:0.0000(w:0.000)❌)
Batch 200/365: Loss=1.2025 (C:1.2015, R:0.0099, T:0.0000(w:0.000)❌)
Batch 225/365: Loss=1.2603 (C:1.2593, R:0.0099, T:0.0000(w:0.000)❌)
Batch 250/365: Loss=1.2443 (C:1.2433, R:0.0099, T:0.0000(w:0.000)❌)
Batch 275/365: Loss=1.3087 (C:1.3077, R:0.0100, T:0.0000(w:0.000)❌)
Batch 300/365: Loss=1.2458 (C:1.2448, R:0.0099, T:0.0000(w:0.000)❌)
Batch 325/365: Loss=1.2358 (C:1.2348, R:0.0099, T:0.0000(w:0.000)❌)
Batch 350/365: Loss=1.1998 (C:1.1989, R:0.0100, T:0.0000(w:0.000)❌)

📊 EPOCH 10 TRAINING SUMMARY:
  Total Loss: 1.2627
  Contrastive: 1.2617
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1810
  Contrastive: 1.1800
  Reconstruction: 0.0100
  Topological: 0.0000 (weight: 0.000)
  Batches with topology: 0/365 (0.0%)
✅ New best model saved!

🎯 EPOCH 10/50 COMPLETE (23.1s)
Train Loss: 1.2627 (C:1.2617, R:0.0100, T:0.0000)
Val Loss:   1.1810 (C:1.1800, R:0.0100, T:0.0000)
❌ No topological learning yet
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=12.6593 (C:1.2447, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch  25/365: Loss=12.6114 (C:1.1967, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  50/365: Loss=12.6953 (C:1.2807, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  75/365: Loss=12.6468 (C:1.2322, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 100/365: Loss=12.6372 (C:1.2226, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 125/365: Loss=12.6171 (C:1.2024, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 150/365: Loss=12.6337 (C:1.2191, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 175/365: Loss=12.7583 (C:1.3437, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 200/365: Loss=12.6414 (C:1.2268, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 225/365: Loss=12.6575 (C:1.2429, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 250/365: Loss=12.6455 (C:1.2309, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 275/365: Loss=12.6189 (C:1.2043, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 300/365: Loss=12.6463 (C:1.2316, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 325/365: Loss=12.6586 (C:1.2440, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 350/365: Loss=12.6674 (C:1.2528, R:0.0099, T:11.4136(w:10.000)⚠️)
🎉 MILESTONE: First topological learning detected at epoch 11!
   Initial topological loss: 11.4136
📈 New best topological loss: 11.4136

📊 EPOCH 11 TRAINING SUMMARY:
  Total Loss: 12.6671
  Contrastive: 1.2525
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 12.5984
  Contrastive: 1.1838
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 11/50 COMPLETE (156.2s)
Train Loss: 12.6671 (C:1.2525, R:0.0100, T:11.4136)
Val Loss:   12.5984 (C:1.1838, R:0.0100, T:11.4136)
⚠️  High topological loss - may need adjustment
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=12.6757 (C:1.2610, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  25/365: Loss=12.6227 (C:1.2081, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  50/365: Loss=12.6341 (C:1.2195, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch  75/365: Loss=12.6537 (C:1.2391, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 100/365: Loss=12.6684 (C:1.2538, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 125/365: Loss=12.6025 (C:1.1879, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 150/365: Loss=12.6511 (C:1.2365, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 175/365: Loss=12.6646 (C:1.2500, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 200/365: Loss=12.6859 (C:1.2713, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 225/365: Loss=12.6485 (C:1.2338, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 250/365: Loss=12.6142 (C:1.1996, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 275/365: Loss=12.6393 (C:1.2247, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 300/365: Loss=12.6451 (C:1.2305, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 325/365: Loss=12.6678 (C:1.2532, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 350/365: Loss=12.6692 (C:1.2546, R:0.0100, T:11.4136(w:10.000)⚠️)

📊 EPOCH 12 TRAINING SUMMARY:
  Total Loss: 12.6680
  Contrastive: 1.2534
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 12.6032
  Contrastive: 1.1886
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 12/50 COMPLETE (155.2s)
Train Loss: 12.6680 (C:1.2534, R:0.0100, T:11.4136)
Val Loss:   12.6032 (C:1.1886, R:0.0100, T:11.4136)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=12.6317 (C:1.2171, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  25/365: Loss=12.6548 (C:1.2402, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch  50/365: Loss=12.7417 (C:1.3271, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  75/365: Loss=12.6604 (C:1.2458, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 100/365: Loss=12.6477 (C:1.2331, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 125/365: Loss=12.5980 (C:1.1834, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 150/365: Loss=12.6408 (C:1.2262, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 175/365: Loss=12.6304 (C:1.2158, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 200/365: Loss=12.6438 (C:1.2292, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 225/365: Loss=12.6824 (C:1.2678, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 250/365: Loss=12.7314 (C:1.3168, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 275/365: Loss=12.6980 (C:1.2834, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 300/365: Loss=12.6010 (C:1.1864, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 325/365: Loss=12.7050 (C:1.2904, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 350/365: Loss=12.6390 (C:1.2244, R:0.0099, T:11.4136(w:10.000)⚠️)
📈 New best topological loss: 11.4136

📊 EPOCH 13 TRAINING SUMMARY:
  Total Loss: 12.6645
  Contrastive: 1.2499
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 12.5914
  Contrastive: 1.1768
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 13/50 COMPLETE (158.3s)
Train Loss: 12.6645 (C:1.2499, R:0.0100, T:11.4136)
Val Loss:   12.5914 (C:1.1768, R:0.0100, T:11.4136)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=12.6216 (C:1.2070, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch  25/365: Loss=12.6164 (C:1.2017, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  50/365: Loss=12.6140 (C:1.1993, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  75/365: Loss=12.6660 (C:1.2514, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 100/365: Loss=12.7080 (C:1.2934, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 125/365: Loss=12.6141 (C:1.1995, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 150/365: Loss=12.6575 (C:1.2428, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 175/365: Loss=12.6771 (C:1.2625, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 200/365: Loss=12.6533 (C:1.2387, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 225/365: Loss=12.6338 (C:1.2192, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 250/365: Loss=12.6418 (C:1.2272, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 275/365: Loss=12.6841 (C:1.2694, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 300/365: Loss=12.6762 (C:1.2615, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 325/365: Loss=12.6490 (C:1.2344, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 350/365: Loss=12.7125 (C:1.2978, R:0.0100, T:11.4136(w:10.000)⚠️)

📊 EPOCH 14 TRAINING SUMMARY:
  Total Loss: 12.6555
  Contrastive: 1.2409
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 12.5893
  Contrastive: 1.1746
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 14/50 COMPLETE (161.8s)
Train Loss: 12.6555 (C:1.2409, R:0.0100, T:11.4136)
Val Loss:   12.5893 (C:1.1746, R:0.0100, T:11.4136)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=12.6411 (C:1.2264, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  25/365: Loss=12.5696 (C:1.1550, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch  50/365: Loss=12.6143 (C:1.1997, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  75/365: Loss=12.6477 (C:1.2330, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 100/365: Loss=12.6636 (C:1.2489, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 125/365: Loss=12.6929 (C:1.2782, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 150/365: Loss=12.6250 (C:1.2104, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 175/365: Loss=12.6015 (C:1.1869, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 200/365: Loss=12.6811 (C:1.2664, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 225/365: Loss=12.6310 (C:1.2164, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 250/365: Loss=12.6105 (C:1.1959, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 275/365: Loss=12.6439 (C:1.2292, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 300/365: Loss=12.6531 (C:1.2385, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 325/365: Loss=12.6298 (C:1.2152, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 350/365: Loss=12.6581 (C:1.2435, R:0.0099, T:11.4136(w:10.000)⚠️)

📊 EPOCH 15 TRAINING SUMMARY:
  Total Loss: 12.6494
  Contrastive: 1.2348
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 12.5752
  Contrastive: 1.1606
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 15/50 COMPLETE (160.9s)
Train Loss: 12.6494 (C:1.2348, R:0.0100, T:11.4136)
Val Loss:   12.5752 (C:1.1606, R:0.0100, T:11.4136)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=12.6334 (C:1.2188, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  25/365: Loss=12.6296 (C:1.2149, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch  50/365: Loss=12.6189 (C:1.2043, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  75/365: Loss=12.6659 (C:1.2513, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 100/365: Loss=12.6091 (C:1.1945, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 125/365: Loss=12.6325 (C:1.2179, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 150/365: Loss=12.6139 (C:1.1993, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 175/365: Loss=12.6494 (C:1.2348, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 200/365: Loss=12.6446 (C:1.2299, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 225/365: Loss=12.5781 (C:1.1634, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 250/365: Loss=12.6468 (C:1.2322, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 275/365: Loss=12.6317 (C:1.2170, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 300/365: Loss=12.6387 (C:1.2241, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 325/365: Loss=12.7117 (C:1.2971, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 350/365: Loss=12.6974 (C:1.2827, R:0.0100, T:11.4136(w:10.000)⚠️)

📊 EPOCH 16 TRAINING SUMMARY:
  Total Loss: 12.6455
  Contrastive: 1.2308
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 12.5746
  Contrastive: 1.1599
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 16/50 COMPLETE (163.6s)
Train Loss: 12.6455 (C:1.2308, R:0.0100, T:11.4136)
Val Loss:   12.5746 (C:1.1599, R:0.0100, T:11.4136)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=12.6294 (C:1.2148, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  25/365: Loss=12.6288 (C:1.2142, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  50/365: Loss=12.6140 (C:1.1994, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch  75/365: Loss=12.6719 (C:1.2573, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 100/365: Loss=12.6398 (C:1.2252, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 125/365: Loss=12.6553 (C:1.2407, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 150/365: Loss=12.6468 (C:1.2322, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 175/365: Loss=12.6279 (C:1.2133, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 200/365: Loss=12.6254 (C:1.2108, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 225/365: Loss=12.6450 (C:1.2303, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 250/365: Loss=12.6380 (C:1.2234, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 275/365: Loss=12.6671 (C:1.2525, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 300/365: Loss=12.6401 (C:1.2255, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 325/365: Loss=12.6086 (C:1.1939, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 350/365: Loss=12.6431 (C:1.2284, R:0.0099, T:11.4136(w:10.000)⚠️)

📊 EPOCH 17 TRAINING SUMMARY:
  Total Loss: 12.6432
  Contrastive: 1.2286
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 12.5654
  Contrastive: 1.1508
  Reconstruction: 0.0100
  Topological: 11.4136 (weight: 10.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 17/50 COMPLETE (150.8s)
Train Loss: 12.6432 (C:1.2286, R:0.0100, T:11.4136)
Val Loss:   12.5654 (C:1.1508, R:0.0100, T:11.4136)
⚠️  High topological loss - may need adjustment
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 365 | Topological Weight: 10.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=12.6421 (C:1.2275, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch  25/365: Loss=12.6192 (C:1.2046, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch  50/365: Loss=12.6280 (C:1.2134, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch  75/365: Loss=12.6230 (C:1.2084, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 100/365: Loss=12.5810 (C:1.1664, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 125/365: Loss=12.6412 (C:1.2266, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 150/365: Loss=12.6580 (C:1.2433, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 175/365: Loss=12.6545 (C:1.2398, R:0.0099, T:11.4136(w:10.000)⚠️)
Batch 200/365: Loss=12.5142 (C:1.0996, R:0.0100, T:11.4136(w:10.000)⚠️)
Batch 225/365: Loss=12.6584 (C:1.2438, R:0.0100, T:11.4136(w:10.000)⚠️)
