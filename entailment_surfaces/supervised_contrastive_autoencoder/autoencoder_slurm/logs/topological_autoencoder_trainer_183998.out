Starting Surface Distance Metric Analysis job...
Job ID: 183998
Node: gpuvm13
Time: Sat 19 Jul 14:57:34 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 19 14:57:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   37C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_145743
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_145743/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1500
  Balanced sampling: True
  Train batches: 365
  Val batches: 365
  Test batches: 367
Data loading completed!
  Train: 549367 samples, 365 batches
  Val: 549367 samples, 365 batches
  Test: 549367 samples, 367 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 500
  Hidden dims: [1280, 1024, 768, 600, 550]
  Dropout rate: 0.1
  Total parameters: 10,268,544
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
Pre-processing target diagrams and converting to tensors...
  entailment: shape torch.Size([1490, 2])
  neutral: shape torch.Size([2043, 2])
  contradiction: shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 10,268,544
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 5.0
  Reconstruction weight: 0.1

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=7.7084 (C:2.0000, R:0.0109, T:5.7073(w:5.000)🚀)
Batch  25/365: Loss=7.7084 (C:2.0000, R:0.0103, T:5.7073(w:5.000)🚀)
Batch  50/365: Loss=7.7083 (C:2.0000, R:0.0100, T:5.7073(w:5.000)🚀)
Batch  75/365: Loss=7.7080 (C:1.9997, R:0.0100, T:5.7073(w:5.000)🚀)
Batch 100/365: Loss=7.5661 (C:1.8549, R:0.0100, T:5.7102(w:5.000)🚀)
Batch 125/365: Loss=7.3891 (C:1.6795, R:0.0100, T:5.7086(w:5.000)🚀)
Batch 150/365: Loss=7.2888 (C:1.5796, R:0.0100, T:5.7083(w:5.000)🚀)
Batch 175/365: Loss=7.2043 (C:1.4956, R:0.0100, T:5.7077(w:5.000)🚀)
Batch 200/365: Loss=7.1982 (C:1.4896, R:0.0100, T:5.7076(w:5.000)🚀)
Batch 225/365: Loss=7.1460 (C:1.4375, R:0.0099, T:5.7075(w:5.000)🚀)
Batch 250/365: Loss=7.0197 (C:1.3112, R:0.0100, T:5.7075(w:5.000)🚀)
Batch 275/365: Loss=7.0638 (C:1.3554, R:0.0100, T:5.7074(w:5.000)🚀)
Batch 300/365: Loss=7.0871 (C:1.3788, R:0.0100, T:5.7072(w:5.000)🚀)
Batch 325/365: Loss=7.0368 (C:1.3286, R:0.0099, T:5.7072(w:5.000)🚀)
Batch 350/365: Loss=7.0835 (C:1.3753, R:0.0100, T:5.7072(w:5.000)🚀)
🎉 MILESTONE: First topological learning detected at epoch 1!
   Initial topological loss: 5.7076
📈 New best topological loss: 5.7076

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 7.3182
  Contrastive: 1.6096
  Reconstruction: 0.0100
  Topological: 5.7076 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 6.9782
  Contrastive: 1.2704
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (337.0s)
Train Loss: 7.3182 (C:1.6096, R:0.0100, T:5.7076)
Val Loss:   6.9782 (C:1.2704, R:0.0100, T:5.7068)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=6.9869 (C:1.2786, R:0.0100, T:5.7073(w:5.000)🚀)
Batch  25/365: Loss=7.0674 (C:1.3593, R:0.0100, T:5.7071(w:5.000)🚀)
Batch  50/365: Loss=7.0230 (C:1.3148, R:0.0099, T:5.7072(w:5.000)🚀)
Batch  75/365: Loss=7.0326 (C:1.3244, R:0.0099, T:5.7072(w:5.000)🚀)
Batch 100/365: Loss=6.9366 (C:1.2285, R:0.0100, T:5.7071(w:5.000)🚀)
Batch 125/365: Loss=6.9954 (C:1.2872, R:0.0099, T:5.7072(w:5.000)🚀)
Batch 150/365: Loss=7.0283 (C:1.3201, R:0.0099, T:5.7072(w:5.000)🚀)
Batch 175/365: Loss=7.0043 (C:1.2962, R:0.0100, T:5.7071(w:5.000)🚀)
Batch 200/365: Loss=6.9681 (C:1.2600, R:0.0099, T:5.7070(w:5.000)🚀)
Batch 225/365: Loss=6.9804 (C:1.2722, R:0.0100, T:5.7071(w:5.000)🚀)
Batch 250/365: Loss=6.9409 (C:1.2329, R:0.0100, T:5.7070(w:5.000)🚀)
Batch 275/365: Loss=6.9417 (C:1.2337, R:0.0100, T:5.7070(w:5.000)🚀)
Batch 300/365: Loss=7.0341 (C:1.3261, R:0.0100, T:5.7070(w:5.000)🚀)
Batch 325/365: Loss=6.9842 (C:1.2762, R:0.0099, T:5.7070(w:5.000)🚀)
Batch 350/365: Loss=7.0187 (C:1.3107, R:0.0100, T:5.7070(w:5.000)🚀)
📈 New best topological loss: 5.7071

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 6.9981
  Contrastive: 1.2900
  Reconstruction: 0.0100
  Topological: 5.7071 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 6.9228
  Contrastive: 1.2150
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (409.7s)
Train Loss: 6.9981 (C:1.2900, R:0.0100, T:5.7071)
Val Loss:   6.9228 (C:1.2150, R:0.0100, T:5.7068)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=6.9844 (C:1.2764, R:0.0099, T:5.7070(w:5.000)🚀)
Batch  25/365: Loss=6.9679 (C:1.2599, R:0.0100, T:5.7070(w:5.000)🚀)
Batch  50/365: Loss=6.9150 (C:1.2070, R:0.0099, T:5.7070(w:5.000)🚀)
Batch  75/365: Loss=7.0087 (C:1.3007, R:0.0099, T:5.7070(w:5.000)🚀)
Batch 100/365: Loss=6.9187 (C:1.2107, R:0.0099, T:5.7070(w:5.000)🚀)
Batch 125/365: Loss=6.9838 (C:1.2758, R:0.0100, T:5.7070(w:5.000)🚀)
Batch 150/365: Loss=6.9551 (C:1.2471, R:0.0100, T:5.7070(w:5.000)🚀)
Batch 175/365: Loss=6.9955 (C:1.2875, R:0.0100, T:5.7070(w:5.000)🚀)
Batch 200/365: Loss=6.9257 (C:1.2178, R:0.0100, T:5.7069(w:5.000)🚀)
Batch 225/365: Loss=6.8854 (C:1.1775, R:0.0100, T:5.7070(w:5.000)🚀)
Batch 250/365: Loss=6.9234 (C:1.2155, R:0.0100, T:5.7069(w:5.000)🚀)
Batch 275/365: Loss=6.9004 (C:1.1925, R:0.0099, T:5.7069(w:5.000)🚀)
Batch 300/365: Loss=6.9248 (C:1.2169, R:0.0099, T:5.7069(w:5.000)🚀)
Batch 325/365: Loss=6.9367 (C:1.2288, R:0.0100, T:5.7069(w:5.000)🚀)
Batch 350/365: Loss=6.9771 (C:1.2692, R:0.0100, T:5.7069(w:5.000)🚀)
📈 New best topological loss: 5.7070

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 6.9485
  Contrastive: 1.2406
  Reconstruction: 0.0100
  Topological: 5.7070 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 6.8883
  Contrastive: 1.1804
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 3/50 COMPLETE (453.8s)
Train Loss: 6.9485 (C:1.2406, R:0.0100, T:5.7070)
Val Loss:   6.8883 (C:1.1804, R:0.0100, T:5.7068)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=6.9324 (C:1.2245, R:0.0100, T:5.7069(w:5.000)🚀)
Batch  25/365: Loss=6.9186 (C:1.2107, R:0.0100, T:5.7069(w:5.000)🚀)
Batch  50/365: Loss=6.9208 (C:1.2129, R:0.0100, T:5.7069(w:5.000)🚀)
Batch  75/365: Loss=6.9117 (C:1.2038, R:0.0100, T:5.7069(w:5.000)🚀)
Batch 100/365: Loss=6.9265 (C:1.2186, R:0.0100, T:5.7069(w:5.000)🚀)
Batch 125/365: Loss=6.9008 (C:1.1929, R:0.0099, T:5.7069(w:5.000)🚀)
Batch 150/365: Loss=6.9347 (C:1.2268, R:0.0099, T:5.7069(w:5.000)🚀)
Batch 175/365: Loss=6.9135 (C:1.2056, R:0.0100, T:5.7069(w:5.000)🚀)
Batch 200/365: Loss=6.9490 (C:1.2411, R:0.0100, T:5.7069(w:5.000)🚀)
Batch 225/365: Loss=6.9542 (C:1.2463, R:0.0099, T:5.7069(w:5.000)🚀)
Batch 250/365: Loss=6.9106 (C:1.2027, R:0.0100, T:5.7069(w:5.000)🚀)
Batch 275/365: Loss=6.9839 (C:1.2760, R:0.0099, T:5.7069(w:5.000)🚀)
Batch 300/365: Loss=6.8534 (C:1.1456, R:0.0100, T:5.7069(w:5.000)🚀)
Batch 325/365: Loss=6.9887 (C:1.2808, R:0.0100, T:5.7069(w:5.000)🚀)
Batch 350/365: Loss=6.9221 (C:1.2143, R:0.0100, T:5.7069(w:5.000)🚀)
📈 New best topological loss: 5.7069

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 6.9224
  Contrastive: 1.2146
  Reconstruction: 0.0100
  Topological: 5.7069 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 6.8777
  Contrastive: 1.1699
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 4/50 COMPLETE (519.4s)
Train Loss: 6.9224 (C:1.2146, R:0.0100, T:5.7069)
Val Loss:   6.8777 (C:1.1699, R:0.0100, T:5.7068)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=6.7922 (C:1.0843, R:0.0099, T:5.7069(w:5.000)🚀)
Batch  25/365: Loss=6.8944 (C:1.1866, R:0.0099, T:5.7069(w:5.000)🚀)
Batch  50/365: Loss=6.9042 (C:1.1964, R:0.0099, T:5.7069(w:5.000)🚀)
Batch  75/365: Loss=6.9046 (C:1.1968, R:0.0100, T:5.7068(w:5.000)🚀)
Batch 100/365: Loss=6.8688 (C:1.1610, R:0.0100, T:5.7068(w:5.000)🚀)
Batch 125/365: Loss=6.8877 (C:1.1798, R:0.0100, T:5.7068(w:5.000)🚀)
Batch 150/365: Loss=6.8512 (C:1.1434, R:0.0100, T:5.7068(w:5.000)🚀)
Batch 175/365: Loss=6.8822 (C:1.1743, R:0.0100, T:5.7068(w:5.000)🚀)
Batch 200/365: Loss=6.9241 (C:1.2163, R:0.0100, T:5.7068(w:5.000)🚀)
Batch 225/365: Loss=6.9118 (C:1.2040, R:0.0100, T:5.7068(w:5.000)🚀)
Batch 250/365: Loss=6.8876 (C:1.1798, R:0.0099, T:5.7068(w:5.000)🚀)
Batch 275/365: Loss=6.8884 (C:1.1805, R:0.0100, T:5.7068(w:5.000)🚀)
Batch 300/365: Loss=6.8961 (C:1.1883, R:0.0100, T:5.7068(w:5.000)🚀)
Batch 325/365: Loss=6.8614 (C:1.1536, R:0.0100, T:5.7068(w:5.000)🚀)
Batch 350/365: Loss=6.8878 (C:1.1800, R:0.0099, T:5.7068(w:5.000)🚀)
📈 New best topological loss: 5.7068

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 6.9061
  Contrastive: 1.1983
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 6.8604
  Contrastive: 1.1526
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 5/50 COMPLETE (596.0s)
Train Loss: 6.9061 (C:1.1983, R:0.0100, T:5.7068)
Val Loss:   6.8604 (C:1.1526, R:0.0100, T:5.7068)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 365 | Topological Weight: 5.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=6.8512 (C:1.1434, R:0.0100, T:5.7068(w:5.000)🚀)
Batch  25/365: Loss=6.9039 (C:1.1960, R:0.0099, T:5.7068(w:5.000)🚀)
