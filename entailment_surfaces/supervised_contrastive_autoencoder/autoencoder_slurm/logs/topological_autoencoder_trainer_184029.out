Starting Surface Distance Metric Analysis job...
Job ID: 184029
Node: gpuvm14
Time: Sat 19 Jul 15:57:52 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 19 15:57:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_155807
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_155807/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1500
  Balanced sampling: True
  Train batches: 365
  Val batches: 365
  Test batches: 367
Data loading completed!
  Train: 549367 samples, 365 batches
  Val: 549367 samples, 365 batches
  Test: 549367 samples, 367 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 50
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 5,852,466
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 1.0
  Base reconstruction weight: 0.1
  Scheduled reconstruction: warmup=10 epochs, max_weight=0.3
Pre-processing target diagrams and converting to tensors...
  entailment: shape torch.Size([1490, 2])
  neutral: shape torch.Size([2043, 2])
  contradiction: shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 5,852,466
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 1.0
  Topological weight: 5.0
  Reconstruction weight: 0.1

======================================================================
ğŸ§  TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 365 | Topological Weight: 5.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=7.7115 (C:2.0000, R:0.0109, T:5.7104(w:5.000)ğŸš€)
Batch  25/365: Loss=7.6786 (C:1.9594, R:0.0100, T:5.7182(w:5.000)ğŸš€)
Batch  50/365: Loss=7.6648 (C:1.9335, R:0.0100, T:5.7303(w:5.000)ğŸš€)
Batch  75/365: Loss=7.6089 (C:1.8911, R:0.0100, T:5.7168(w:5.000)ğŸš€)
Batch 100/365: Loss=7.5297 (C:1.8156, R:0.0100, T:5.7131(w:5.000)ğŸš€)
Batch 125/365: Loss=7.4251 (C:1.7139, R:0.0100, T:5.7102(w:5.000)ğŸš€)
Batch 150/365: Loss=7.3428 (C:1.6328, R:0.0100, T:5.7090(w:5.000)ğŸš€)
Batch 175/365: Loss=7.2665 (C:1.5573, R:0.0100, T:5.7083(w:5.000)ğŸš€)
Batch 200/365: Loss=7.2291 (C:1.5206, R:0.0100, T:5.7075(w:5.000)ğŸš€)
Batch 225/365: Loss=7.2232 (C:1.5149, R:0.0100, T:5.7073(w:5.000)ğŸš€)
Batch 250/365: Loss=7.2581 (C:1.5499, R:0.0099, T:5.7072(w:5.000)ğŸš€)
Batch 275/365: Loss=7.1898 (C:1.4818, R:0.0100, T:5.7071(w:5.000)ğŸš€)
Batch 300/365: Loss=7.2210 (C:1.5130, R:0.0099, T:5.7070(w:5.000)ğŸš€)
Batch 325/365: Loss=7.2379 (C:1.5298, R:0.0099, T:5.7071(w:5.000)ğŸš€)
Batch 350/365: Loss=7.1228 (C:1.4148, R:0.0100, T:5.7070(w:5.000)ğŸš€)
ğŸ‰ MILESTONE: First topological learning detected at epoch 1!
   Initial topological loss: 5.7102
ğŸ“ˆ New best topological loss: 5.7102

ğŸ“Š EPOCH 1 TRAINING SUMMARY:
  Total Loss: 7.3630
  Contrastive: 1.6518
  Reconstruction: 0.0100
  Topological: 5.7102 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 7.0890
  Contrastive: 1.3812
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 1/50 COMPLETE (222.6s)
Train Loss: 7.3630 (C:1.6518, R:0.0100, T:5.7102)
Val Loss:   7.0890 (C:1.3812, R:0.0100, T:5.7068)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 365 | Topological Weight: 5.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=7.1355 (C:1.4274, R:0.0100, T:5.7071(w:5.000)ğŸš€)
Batch  25/365: Loss=7.1743 (C:1.4663, R:0.0100, T:5.7070(w:5.000)ğŸš€)
Batch  50/365: Loss=7.1575 (C:1.4496, R:0.0100, T:5.7070(w:5.000)ğŸš€)
Batch  75/365: Loss=7.0882 (C:1.3802, R:0.0099, T:5.7070(w:5.000)ğŸš€)
Batch 100/365: Loss=7.1340 (C:1.4260, R:0.0099, T:5.7070(w:5.000)ğŸš€)
Batch 125/365: Loss=7.0853 (C:1.3773, R:0.0100, T:5.7070(w:5.000)ğŸš€)
Batch 150/365: Loss=7.0758 (C:1.3680, R:0.0100, T:5.7069(w:5.000)ğŸš€)
Batch 175/365: Loss=7.1312 (C:1.4233, R:0.0100, T:5.7069(w:5.000)ğŸš€)
Batch 200/365: Loss=7.0850 (C:1.3772, R:0.0100, T:5.7069(w:5.000)ğŸš€)
Batch 225/365: Loss=7.0863 (C:1.3784, R:0.0100, T:5.7069(w:5.000)ğŸš€)
Batch 250/365: Loss=7.1169 (C:1.4090, R:0.0100, T:5.7069(w:5.000)ğŸš€)
Batch 275/365: Loss=7.0773 (C:1.3695, R:0.0099, T:5.7069(w:5.000)ğŸš€)
Batch 300/365: Loss=7.1070 (C:1.3992, R:0.0100, T:5.7069(w:5.000)ğŸš€)
Batch 325/365: Loss=7.0958 (C:1.3879, R:0.0100, T:5.7069(w:5.000)ğŸš€)
Batch 350/365: Loss=7.0816 (C:1.3737, R:0.0099, T:5.7069(w:5.000)ğŸš€)
ğŸ“ˆ New best topological loss: 5.7069

ğŸ“Š EPOCH 2 TRAINING SUMMARY:
  Total Loss: 7.1200
  Contrastive: 1.4121
  Reconstruction: 0.0100
  Topological: 5.7069 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 7.0044
  Contrastive: 1.2966
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 2/50 COMPLETE (177.3s)
Train Loss: 7.1200 (C:1.4121, R:0.0100, T:5.7069)
Val Loss:   7.0044 (C:1.2966, R:0.0100, T:5.7068)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 365 | Topological Weight: 5.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=7.1104 (C:1.4026, R:0.0100, T:5.7069(w:5.000)ğŸš€)
Batch  25/365: Loss=7.0654 (C:1.3575, R:0.0099, T:5.7069(w:5.000)ğŸš€)
Batch  50/365: Loss=7.0462 (C:1.3383, R:0.0099, T:5.7069(w:5.000)ğŸš€)
Batch  75/365: Loss=7.0529 (C:1.3451, R:0.0099, T:5.7069(w:5.000)ğŸš€)
Batch 100/365: Loss=7.0545 (C:1.3466, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 125/365: Loss=7.0734 (C:1.3656, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 150/365: Loss=7.0763 (C:1.3685, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 175/365: Loss=7.0575 (C:1.3497, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 200/365: Loss=7.1179 (C:1.4101, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 225/365: Loss=7.1658 (C:1.4580, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 250/365: Loss=7.1123 (C:1.4045, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 275/365: Loss=7.1030 (C:1.3952, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 300/365: Loss=7.1086 (C:1.4008, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 325/365: Loss=7.0880 (C:1.3802, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 350/365: Loss=7.0762 (C:1.3684, R:0.0100, T:5.7068(w:5.000)ğŸš€)
ğŸ“ˆ New best topological loss: 5.7068

ğŸ“Š EPOCH 3 TRAINING SUMMARY:
  Total Loss: 7.0801
  Contrastive: 1.3723
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 6.9976
  Contrastive: 1.2898
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 3/50 COMPLETE (161.9s)
Train Loss: 7.0801 (C:1.3723, R:0.0100, T:5.7068)
Val Loss:   6.9976 (C:1.2898, R:0.0100, T:5.7068)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 365 | Topological Weight: 5.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=7.0984 (C:1.3906, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch  25/365: Loss=7.0283 (C:1.3205, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch  50/365: Loss=7.0791 (C:1.3713, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch  75/365: Loss=7.1226 (C:1.4148, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 100/365: Loss=7.0438 (C:1.3360, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 125/365: Loss=7.0613 (C:1.3535, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 150/365: Loss=7.0354 (C:1.3276, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 175/365: Loss=7.0493 (C:1.3415, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 200/365: Loss=7.0480 (C:1.3402, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 225/365: Loss=7.0785 (C:1.3707, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 250/365: Loss=7.0140 (C:1.3062, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 275/365: Loss=6.9966 (C:1.2888, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 300/365: Loss=7.0074 (C:1.2996, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 325/365: Loss=7.0737 (C:1.3659, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 350/365: Loss=7.0600 (C:1.3522, R:0.0100, T:5.7068(w:5.000)ğŸš€)
ğŸ“ˆ New best topological loss: 5.7068

ğŸ“Š EPOCH 4 TRAINING SUMMARY:
  Total Loss: 7.0593
  Contrastive: 1.3515
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 6.9754
  Contrastive: 1.2676
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 4/50 COMPLETE (156.2s)
Train Loss: 7.0593 (C:1.3515, R:0.0100, T:5.7068)
Val Loss:   6.9754 (C:1.2676, R:0.0100, T:5.7068)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 365 | Topological Weight: 5.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=7.0796 (C:1.3718, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch  25/365: Loss=7.0237 (C:1.3159, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch  50/365: Loss=7.0460 (C:1.3382, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch  75/365: Loss=7.0206 (C:1.3128, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 100/365: Loss=7.0372 (C:1.3294, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 125/365: Loss=7.0495 (C:1.3417, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 150/365: Loss=7.0574 (C:1.3496, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 175/365: Loss=7.0545 (C:1.3467, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 200/365: Loss=7.0879 (C:1.3801, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 225/365: Loss=6.9700 (C:1.2622, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 250/365: Loss=7.0616 (C:1.3538, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 275/365: Loss=7.0526 (C:1.3448, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 300/365: Loss=7.0580 (C:1.3502, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 325/365: Loss=7.0582 (C:1.3503, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 350/365: Loss=7.0621 (C:1.3543, R:0.0100, T:5.7068(w:5.000)ğŸš€)
ğŸ“ˆ New best topological loss: 5.7068

ğŸ“Š EPOCH 5 TRAINING SUMMARY:
  Total Loss: 7.0455
  Contrastive: 1.3377
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 6.9623
  Contrastive: 1.2544
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 5/50 COMPLETE (143.1s)
Train Loss: 7.0455 (C:1.3377, R:0.0100, T:5.7068)
Val Loss:   6.9623 (C:1.2544, R:0.0100, T:5.7068)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 365 | Topological Weight: 5.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=7.0777 (C:1.3699, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch  25/365: Loss=7.0302 (C:1.3224, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch  50/365: Loss=7.0490 (C:1.3412, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch  75/365: Loss=7.0750 (C:1.3672, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 100/365: Loss=7.0122 (C:1.3044, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 125/365: Loss=7.0205 (C:1.3126, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 150/365: Loss=7.0315 (C:1.3237, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 175/365: Loss=7.0510 (C:1.3432, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 200/365: Loss=7.0787 (C:1.3709, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 225/365: Loss=7.0610 (C:1.3532, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 250/365: Loss=7.0271 (C:1.3193, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 275/365: Loss=7.0677 (C:1.3599, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 300/365: Loss=7.0384 (C:1.3306, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 325/365: Loss=7.0163 (C:1.3085, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 350/365: Loss=7.0923 (C:1.3845, R:0.0100, T:5.7068(w:5.000)ğŸš€)
ğŸ“ˆ New best topological loss: 5.7068

ğŸ“Š EPOCH 6 TRAINING SUMMARY:
  Total Loss: 7.0375
  Contrastive: 1.3297
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 6.9519
  Contrastive: 1.2441
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 6/50 COMPLETE (136.0s)
Train Loss: 7.0375 (C:1.3297, R:0.0100, T:5.7068)
Val Loss:   6.9519 (C:1.2441, R:0.0100, T:5.7068)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 365 | Topological Weight: 5.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=7.0395 (C:1.3317, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch  25/365: Loss=7.0075 (C:1.2997, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch  50/365: Loss=7.0117 (C:1.3038, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch  75/365: Loss=6.9638 (C:1.2560, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 100/365: Loss=7.0096 (C:1.3017, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 125/365: Loss=7.0416 (C:1.3338, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 150/365: Loss=7.0254 (C:1.3176, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 175/365: Loss=7.0394 (C:1.3316, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 200/365: Loss=7.0203 (C:1.3125, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 225/365: Loss=7.0368 (C:1.3290, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 250/365: Loss=6.9812 (C:1.2734, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 275/365: Loss=7.0437 (C:1.3359, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 300/365: Loss=7.0481 (C:1.3403, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 325/365: Loss=6.9720 (C:1.2642, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 350/365: Loss=7.0541 (C:1.3463, R:0.0100, T:5.7068(w:5.000)ğŸš€)

ğŸ“Š EPOCH 7 TRAINING SUMMARY:
  Total Loss: 7.0265
  Contrastive: 1.3187
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 6.9312
  Contrastive: 1.2234
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)
âœ… New best model saved!

ğŸ¯ EPOCH 7/50 COMPLETE (147.3s)
Train Loss: 7.0265 (C:1.3187, R:0.0100, T:5.7068)
Val Loss:   6.9312 (C:1.2234, R:0.0100, T:5.7068)
ğŸš€ Good topological learning progress
â­ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 365 | Topological Weight: 5.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=6.9599 (C:1.2521, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch  25/365: Loss=7.0261 (C:1.3182, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch  50/365: Loss=7.0112 (C:1.3034, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch  75/365: Loss=7.0216 (C:1.3138, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 100/365: Loss=7.0544 (C:1.3466, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 125/365: Loss=7.0242 (C:1.3164, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 150/365: Loss=7.0060 (C:1.2982, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 175/365: Loss=7.0393 (C:1.3315, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 200/365: Loss=7.0151 (C:1.3073, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 225/365: Loss=7.0745 (C:1.3667, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 250/365: Loss=7.0494 (C:1.3416, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 275/365: Loss=6.9945 (C:1.2867, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 300/365: Loss=6.9788 (C:1.2710, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 325/365: Loss=6.9758 (C:1.2680, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 350/365: Loss=6.9859 (C:1.2781, R:0.0100, T:5.7068(w:5.000)ğŸš€)

ğŸ“Š EPOCH 8 TRAINING SUMMARY:
  Total Loss: 7.0196
  Contrastive: 1.3118
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

ğŸ“Š VALIDATION SUMMARY:
  Total Loss: 6.9314
  Contrastive: 1.2236
  Reconstruction: 0.0100
  Topological: 5.7068 (weight: 5.000)
  Batches with topology: 365/365 (100.0%)

ğŸ¯ EPOCH 8/50 COMPLETE (168.0s)
Train Loss: 7.0196 (C:1.3118, R:0.0100, T:5.7068)
Val Loss:   6.9314 (C:1.2236, R:0.0100, T:5.7068)
ğŸš€ Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 365 | Topological Weight: 5.0000
ğŸ§  Full topological learning active
============================================================
Batch   0/365: Loss=6.9751 (C:1.2673, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch  25/365: Loss=7.0360 (C:1.3282, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch  50/365: Loss=6.9880 (C:1.2802, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch  75/365: Loss=6.9746 (C:1.2668, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 100/365: Loss=6.9745 (C:1.2667, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 125/365: Loss=6.9655 (C:1.2577, R:0.0099, T:5.7068(w:5.000)ğŸš€)
Batch 150/365: Loss=7.0095 (C:1.3017, R:0.0100, T:5.7068(w:5.000)ğŸš€)
Batch 175/365: Loss=7.0001 (C:1.2923, R:0.0100, T:5.7068(w:5.000)ğŸš€)
