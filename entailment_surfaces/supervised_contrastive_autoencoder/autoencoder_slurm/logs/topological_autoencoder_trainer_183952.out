Starting Surface Distance Metric Analysis job...
Job ID: 183952
Node: gpuvm14
Time: Sat 19 Jul 10:57:01 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Sat Jul 19 10:57:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:07.0 Off |                    0 |
| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting Topological Training...

============================================================
TOPOLOGICAL AUTOENCODER TRAINING WITH TORCHPH
============================================================
Experiment setup completed:
  Experiment directory: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_105726
  Config saved to: entailment_surfaces/supervised_contrastive_autoencoder/experiments/topological_autoencoder_torchph_phase1_20250719_105726/config.json
Loading data...
========================================
FlexibleEmbedder initialized with type: 'concat'
Output dimension will be: 1536
GlobalDataLoader initialized:
  Embedding type: concat
  Output dimension: 1536
  Sample size: All
Starting data loading pipeline...
============================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 validation samples
Loading test data from data/processed/snli_full_standard_SBERT.pt
  Loaded keys: ['premise_embeddings', 'hypothesis_embeddings', 'labels', 'texts', 'metadata']
  Final dataset: 549367 test samples
Generating embeddings for training...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for validation...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
Generating embeddings for test...
Generating concat embeddings on cuda
Processing 549367 samples in batches of 1000
  Processing batch 1/550
  Processing batch 6/550
  Processing batch 11/550
  Processing batch 16/550
  Processing batch 21/550
  Processing batch 26/550
  Processing batch 31/550
  Processing batch 36/550
  Processing batch 41/550
  Processing batch 46/550
  Processing batch 51/550
  Processing batch 56/550
  Processing batch 61/550
  Processing batch 66/550
  Processing batch 71/550
  Processing batch 76/550
  Processing batch 81/550
  Processing batch 86/550
  Processing batch 91/550
  Processing batch 96/550
  Processing batch 101/550
  Processing batch 106/550
  Processing batch 111/550
  Processing batch 116/550
  Processing batch 121/550
  Processing batch 126/550
  Processing batch 131/550
  Processing batch 136/550
  Processing batch 141/550
  Processing batch 146/550
  Processing batch 151/550
  Processing batch 156/550
  Processing batch 161/550
  Processing batch 166/550
  Processing batch 171/550
  Processing batch 176/550
  Processing batch 181/550
  Processing batch 186/550
  Processing batch 191/550
  Processing batch 196/550
  Processing batch 201/550
  Processing batch 206/550
  Processing batch 211/550
  Processing batch 216/550
  Processing batch 221/550
  Processing batch 226/550
  Processing batch 231/550
  Processing batch 236/550
  Processing batch 241/550
  Processing batch 246/550
  Processing batch 251/550
  Processing batch 256/550
  Processing batch 261/550
  Processing batch 266/550
  Processing batch 271/550
  Processing batch 276/550
  Processing batch 281/550
  Processing batch 286/550
  Processing batch 291/550
  Processing batch 296/550
  Processing batch 301/550
  Processing batch 306/550
  Processing batch 311/550
  Processing batch 316/550
  Processing batch 321/550
  Processing batch 326/550
  Processing batch 331/550
  Processing batch 336/550
  Processing batch 341/550
  Processing batch 346/550
  Processing batch 351/550
  Processing batch 356/550
  Processing batch 361/550
  Processing batch 366/550
  Processing batch 371/550
  Processing batch 376/550
  Processing batch 381/550
  Processing batch 386/550
  Processing batch 391/550
  Processing batch 396/550
  Processing batch 401/550
  Processing batch 406/550
  Processing batch 411/550
  Processing batch 416/550
  Processing batch 421/550
  Processing batch 426/550
  Processing batch 431/550
  Processing batch 436/550
  Processing batch 441/550
  Processing batch 446/550
  Processing batch 451/550
  Processing batch 456/550
  Processing batch 461/550
  Processing batch 466/550
  Processing batch 471/550
  Processing batch 476/550
  Processing batch 481/550
  Processing batch 486/550
  Processing batch 491/550
  Processing batch 496/550
  Processing batch 501/550
  Processing batch 506/550
  Processing batch 511/550
  Processing batch 516/550
  Processing batch 521/550
  Processing batch 526/550
  Processing batch 531/550
  Processing batch 536/550
  Processing batch 541/550
  Processing batch 546/550
Generated concat embeddings: torch.Size([549367, 1536])
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
EntailmentDataset created: 549367 samples
  Embedding shape: torch.Size([549367, 1536])
  Class distribution: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}

Data loading pipeline completed!
Output embedding dimension: 1536
Updated model input_dim to: 1536
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class per batch: 500
  Effective batch size: 1500
  Number of batches: 365
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
DataLoaders created:
  Batch size: 1500
  Balanced sampling: True
  Train batches: 365
  Val batches: 365
  Test batches: 367
Data loading completed!
  Train: 549367 samples, 365 batches
  Val: 549367 samples, 365 batches
  Test: 549367 samples, 367 batches
ContrastiveAutoencoder initialized:
  Input dim: 1536
  Latent dim: 75
  Hidden dims: [1024, 768, 512, 256, 128]
  Dropout rate: 0.2
  Total parameters: 5,858,891
torch-topological successfully imported
FullDatasetContrastiveLoss initialized:
  Margin: 2.0
  Update frequency: 3 epochs
  Max global samples: 5000
FullDatasetCombinedLoss initialized:
  Contrastive weight: 0.0
  Base reconstruction weight: 0.5
Pre-processing target diagrams and converting to tensors...
  > entailment: Stored wrapped prototype with tensor shape torch.Size([1490, 2])
  > neutral: Stored wrapped prototype with tensor shape torch.Size([2043, 2])
  > contradiction: Stored wrapped prototype with tensor shape torch.Size([2682, 2])
TorchTopologicalPersistenceLoss Initialized Correctly.
Topological loss initialized with prototypes: entailment_surfaces/supervised_contrastive_autoencoder/src/persistence_diagrams/prototypes_robust.pkl
TopologicalTrainer initialized on device: cuda
Model parameters: 5,858,891
Enhanced with topological loss monitoring
Starting Phase 1: Pure Topological Training
  Contrastive weight: 0.0
  Topological weight: 1.0
  Reconstruction weight: 0.5

======================================================================
🧠 TOPOLOGICAL AUTOENCODER TRAINING STARTED
======================================================================

============================================================
EPOCH 1 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1477 (C:2.0000, R:0.0109, T:1.1422(w:1.000)🚀)
Batch  25/365: Loss=1.1470 (C:2.0000, R:0.0101, T:1.1419(w:1.000)🚀)
Batch  50/365: Loss=1.1470 (C:2.0000, R:0.0100, T:1.1420(w:1.000)🚀)
Batch  75/365: Loss=1.1471 (C:2.0000, R:0.0100, T:1.1421(w:1.000)🚀)
Batch 100/365: Loss=1.1471 (C:2.0000, R:0.0100, T:1.1421(w:1.000)🚀)
Batch 125/365: Loss=1.1470 (C:2.0000, R:0.0100, T:1.1420(w:1.000)🚀)
Batch 150/365: Loss=1.1473 (C:2.0000, R:0.0099, T:1.1424(w:1.000)🚀)
Batch 175/365: Loss=1.1472 (C:2.0000, R:0.0099, T:1.1422(w:1.000)🚀)
Batch 200/365: Loss=1.1472 (C:2.0000, R:0.0100, T:1.1423(w:1.000)🚀)
Batch 225/365: Loss=1.1470 (C:2.0000, R:0.0100, T:1.1420(w:1.000)🚀)
Batch 250/365: Loss=1.1471 (C:2.0000, R:0.0100, T:1.1421(w:1.000)🚀)
Batch 275/365: Loss=1.1469 (C:2.0000, R:0.0100, T:1.1419(w:1.000)🚀)
Batch 300/365: Loss=1.1468 (C:2.0000, R:0.0100, T:1.1418(w:1.000)🚀)
Batch 325/365: Loss=1.1467 (C:2.0000, R:0.0100, T:1.1417(w:1.000)🚀)
Batch 350/365: Loss=1.1466 (C:1.9999, R:0.0100, T:1.1416(w:1.000)🚀)
🎉 MILESTONE: First topological learning detected at epoch 1!
   Initial topological loss: 1.1615
📈 New best topological loss: 1.1615

📊 EPOCH 1 TRAINING SUMMARY:
  Total Loss: 1.1665
  Contrastive: 2.0000
  Reconstruction: 0.0100
  Topological: 1.1615 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 1.9999
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 1/50 COMPLETE (685.3s)
Train Loss: 1.1665 (C:2.0000, R:0.0100, T:1.1615)
Val Loss:   1.1463 (C:1.9999, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 2 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1465 (C:2.0000, R:0.0100, T:1.1415(w:1.000)🚀)
Batch  25/365: Loss=1.1464 (C:2.0000, R:0.0099, T:1.1415(w:1.000)🚀)
Batch  50/365: Loss=1.1464 (C:1.9999, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:1.9999, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 2 TRAINING SUMMARY:
  Total Loss: 1.1464
  Contrastive: 2.0000
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 1.9999
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 2/50 COMPLETE (275.8s)
Train Loss: 1.1464 (C:2.0000, R:0.0100, T:1.1414)
Val Loss:   1.1463 (C:1.9999, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 3 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1464 (C:1.9999, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 3 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 1.9999
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 3/50 COMPLETE (251.5s)
Train Loss: 1.1463 (C:2.0000, R:0.0100, T:1.1414)
Val Loss:   1.1463 (C:1.9999, R:0.0100, T:1.1414)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 4 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:1.9999, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 4 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 1.9999
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 4/50 COMPLETE (259.4s)
Train Loss: 1.1463 (C:2.0000, R:0.0100, T:1.1414)
Val Loss:   1.1463 (C:1.9999, R:0.0100, T:1.1414)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 5 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:1.9999, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1464 (C:1.9999, R:0.0100, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 5 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 1.9999
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 5/50 COMPLETE (268.0s)
Train Loss: 1.1463 (C:2.0000, R:0.0100, T:1.1414)
Val Loss:   1.1463 (C:1.9999, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 6 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:1.9999, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 6 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 1.9999
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 6/50 COMPLETE (262.1s)
Train Loss: 1.1463 (C:2.0000, R:0.0100, T:1.1414)
Val Loss:   1.1463 (C:1.9999, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 7 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:1.9999, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:1.9999, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 7 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 1.9999
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 7/50 COMPLETE (249.4s)
Train Loss: 1.1463 (C:2.0000, R:0.0100, T:1.1414)
Val Loss:   1.1463 (C:1.9999, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 8 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1464 (C:1.9999, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:1.9999, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 8 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 1.9999
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 8/50 COMPLETE (225.6s)
Train Loss: 1.1463 (C:2.0000, R:0.0100, T:1.1414)
Val Loss:   1.1463 (C:1.9999, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 9 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:1.9999, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 9 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 1.9999
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 9/50 COMPLETE (220.0s)
Train Loss: 1.1463 (C:2.0000, R:0.0100, T:1.1414)
Val Loss:   1.1463 (C:1.9999, R:0.0100, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 10 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 10 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0100
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 10/50 COMPLETE (260.7s)
Train Loss: 1.1463 (C:2.0000, R:0.0100, T:1.1414)
Val Loss:   1.1463 (C:2.0000, R:0.0100, T:1.1414)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 11 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:1.9999, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 11 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 11/50 COMPLETE (337.9s)
Train Loss: 1.1463 (C:2.0000, R:0.0099, T:1.1414)
Val Loss:   1.1463 (C:2.0000, R:0.0099, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 12 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 12 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 12/50 COMPLETE (190.4s)
Train Loss: 1.1463 (C:2.0000, R:0.0099, T:1.1414)
Val Loss:   1.1463 (C:2.0000, R:0.0099, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 13 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:1.9999, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 13 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 13/50 COMPLETE (153.1s)
Train Loss: 1.1463 (C:2.0000, R:0.0099, T:1.1414)
Val Loss:   1.1463 (C:2.0000, R:0.0099, T:1.1414)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 14 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1464 (C:1.9999, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)

📊 EPOCH 14 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 14/50 COMPLETE (125.0s)
Train Loss: 1.1463 (C:2.0000, R:0.0099, T:1.1414)
Val Loss:   1.1463 (C:2.0000, R:0.0099, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 15 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:1.9999, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 15 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 15/50 COMPLETE (114.7s)
Train Loss: 1.1463 (C:2.0000, R:0.0099, T:1.1414)
Val Loss:   1.1463 (C:2.0000, R:0.0099, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 16 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 16 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)
✅ New best model saved!

🎯 EPOCH 16/50 COMPLETE (126.8s)
Train Loss: 1.1463 (C:2.0000, R:0.0099, T:1.1414)
Val Loss:   1.1463 (C:2.0000, R:0.0099, T:1.1414)
🚀 Good topological learning progress
⭐ Best model so far!
------------------------------------------------------------

============================================================
EPOCH 17 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1464 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 175/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 200/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 225/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 250/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 275/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 300/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 325/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 350/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
📈 New best topological loss: 1.1414

📊 EPOCH 17 TRAINING SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

📊 VALIDATION SUMMARY:
  Total Loss: 1.1463
  Contrastive: 2.0000
  Reconstruction: 0.0099
  Topological: 1.1414 (weight: 1.000)
  Batches with topology: 365/365 (100.0%)

🎯 EPOCH 17/50 COMPLETE (134.6s)
Train Loss: 1.1463 (C:2.0000, R:0.0099, T:1.1414)
Val Loss:   1.1463 (C:2.0000, R:0.0099, T:1.1414)
🚀 Good topological learning progress
------------------------------------------------------------

============================================================
EPOCH 18 | Batches: 365 | Topological Weight: 1.0000
🧠 Full topological learning active
============================================================
Batch   0/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  25/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  50/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch  75/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 100/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
Batch 125/365: Loss=1.1463 (C:2.0000, R:0.0100, T:1.1414(w:1.000)🚀)
Batch 150/365: Loss=1.1463 (C:2.0000, R:0.0099, T:1.1414(w:1.000)🚀)
