Starting Surface Distance Metric Analysis job...
Job ID: 181750
Node: gpuvm17
Time: Fri 11 Jul 22:24:29 BST 2025
Loading CUDA...
Activating conda environment...
Activated conda environment: /vol/bitbucket/ahb24/tda_entailment_new
Python location: /vol/bitbucket/ahb24/tda_entailment_new/bin/python
Checking GPU availability...
Fri Jul 11 22:24:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla T4                       Off |   00000000:00:06.0 Off |                    0 |
| N/A   44C    P8             14W /   70W |       0MiB /  15360MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Testing PyTorch and CUDA...
PyTorch version: 2.4.1
CUDA available: True
CUDA device: Tesla T4
GPU memory: 15.6 GB
PyTorch setup verified!

Starting AutoEncoder Pipeline...

SUPERVISED CONTRASTIVE AUTOENCODER - FULL PIPELINE
============================================================
Start time: 2025-07-11 22:25:03.513365

Using device: cuda
Experiment setup complete: entailment_surfaces/supervised_contrastive_autoencoder/experiments/contrastive_autoencoder_20250711_222503
Loading SNLI data...
==================================================
Starting data loading pipeline...
==================================================
Loading training data from data/processed/snli_full_standard_SBERT.pt
Loaded 549367 training samples
Loading validation data from data/processed/snli_full_standard_SBERT_validation.pt
Loaded 9842 validation samples
Loading test data from data/processed/snli_full_standard_SBERT_test.pt
Loaded 9824 test samples
Generating lattice containment embeddings for training...
Generating lattice containment embeddings on cuda
Processing 549367 samples in batches of 1000
Processing batch 1/550
Processing batch 2/550
Processing batch 3/550
Processing batch 4/550
Processing batch 5/550
Processing batch 6/550
Processing batch 7/550
Processing batch 8/550
Processing batch 9/550
Processing batch 10/550
Processing batch 11/550
Processing batch 12/550
Processing batch 13/550
Processing batch 14/550
Processing batch 15/550
Processing batch 16/550
Processing batch 17/550
Processing batch 18/550
Processing batch 19/550
Processing batch 20/550
Processing batch 21/550
Processing batch 22/550
Processing batch 23/550
Processing batch 24/550
Processing batch 25/550
Processing batch 26/550
Processing batch 27/550
Processing batch 28/550
Processing batch 29/550
Processing batch 30/550
Processing batch 31/550
Processing batch 32/550
Processing batch 33/550
Processing batch 34/550
Processing batch 35/550
Processing batch 36/550
Processing batch 37/550
Processing batch 38/550
Processing batch 39/550
Processing batch 40/550
Processing batch 41/550
Processing batch 42/550
Processing batch 43/550
Processing batch 44/550
Processing batch 45/550
Processing batch 46/550
Processing batch 47/550
Processing batch 48/550
Processing batch 49/550
Processing batch 50/550
Processing batch 51/550
Processing batch 52/550
Processing batch 53/550
Processing batch 54/550
Processing batch 55/550
Processing batch 56/550
Processing batch 57/550
Processing batch 58/550
Processing batch 59/550
Processing batch 60/550
Processing batch 61/550
Processing batch 62/550
Processing batch 63/550
Processing batch 64/550
Processing batch 65/550
Processing batch 66/550
Processing batch 67/550
Processing batch 68/550
Processing batch 69/550
Processing batch 70/550
Processing batch 71/550
Processing batch 72/550
Processing batch 73/550
Processing batch 74/550
Processing batch 75/550
Processing batch 76/550
Processing batch 77/550
Processing batch 78/550
Processing batch 79/550
Processing batch 80/550
Processing batch 81/550
Processing batch 82/550
Processing batch 83/550
Processing batch 84/550
Processing batch 85/550
Processing batch 86/550
Processing batch 87/550
Processing batch 88/550
Processing batch 89/550
Processing batch 90/550
Processing batch 91/550
Processing batch 92/550
Processing batch 93/550
Processing batch 94/550
Processing batch 95/550
Processing batch 96/550
Processing batch 97/550
Processing batch 98/550
Processing batch 99/550
Processing batch 100/550
Processing batch 101/550
Processing batch 102/550
Processing batch 103/550
Processing batch 104/550
Processing batch 105/550
Processing batch 106/550
Processing batch 107/550
Processing batch 108/550
Processing batch 109/550
Processing batch 110/550
Processing batch 111/550
Processing batch 112/550
Processing batch 113/550
Processing batch 114/550
Processing batch 115/550
Processing batch 116/550
Processing batch 117/550
Processing batch 118/550
Processing batch 119/550
Processing batch 120/550
Processing batch 121/550
Processing batch 122/550
Processing batch 123/550
Processing batch 124/550
Processing batch 125/550
Processing batch 126/550
Processing batch 127/550
Processing batch 128/550
Processing batch 129/550
Processing batch 130/550
Processing batch 131/550
Processing batch 132/550
Processing batch 133/550
Processing batch 134/550
Processing batch 135/550
Processing batch 136/550
Processing batch 137/550
Processing batch 138/550
Processing batch 139/550
Processing batch 140/550
Processing batch 141/550
Processing batch 142/550
Processing batch 143/550
Processing batch 144/550
Processing batch 145/550
Processing batch 146/550
Processing batch 147/550
Processing batch 148/550
Processing batch 149/550
Processing batch 150/550
Processing batch 151/550
Processing batch 152/550
Processing batch 153/550
Processing batch 154/550
Processing batch 155/550
Processing batch 156/550
Processing batch 157/550
Processing batch 158/550
Processing batch 159/550
Processing batch 160/550
Processing batch 161/550
Processing batch 162/550
Processing batch 163/550
Processing batch 164/550
Processing batch 165/550
Processing batch 166/550
Processing batch 167/550
Processing batch 168/550
Processing batch 169/550
Processing batch 170/550
Processing batch 171/550
Processing batch 172/550
Processing batch 173/550
Processing batch 174/550
Processing batch 175/550
Processing batch 176/550
Processing batch 177/550
Processing batch 178/550
Processing batch 179/550
Processing batch 180/550
Processing batch 181/550
Processing batch 182/550
Processing batch 183/550
Processing batch 184/550
Processing batch 185/550
Processing batch 186/550
Processing batch 187/550
Processing batch 188/550
Processing batch 189/550
Processing batch 190/550
Processing batch 191/550
Processing batch 192/550
Processing batch 193/550
Processing batch 194/550
Processing batch 195/550
Processing batch 196/550
Processing batch 197/550
Processing batch 198/550
Processing batch 199/550
Processing batch 200/550
Processing batch 201/550
Processing batch 202/550
Processing batch 203/550
Processing batch 204/550
Processing batch 205/550
Processing batch 206/550
Processing batch 207/550
Processing batch 208/550
Processing batch 209/550
Processing batch 210/550
Processing batch 211/550
Processing batch 212/550
Processing batch 213/550
Processing batch 214/550
Processing batch 215/550
Processing batch 216/550
Processing batch 217/550
Processing batch 218/550
Processing batch 219/550
Processing batch 220/550
Processing batch 221/550
Processing batch 222/550
Processing batch 223/550
Processing batch 224/550
Processing batch 225/550
Processing batch 226/550
Processing batch 227/550
Processing batch 228/550
Processing batch 229/550
Processing batch 230/550
Processing batch 231/550
Processing batch 232/550
Processing batch 233/550
Processing batch 234/550
Processing batch 235/550
Processing batch 236/550
Processing batch 237/550
Processing batch 238/550
Processing batch 239/550
Processing batch 240/550
Processing batch 241/550
Processing batch 242/550
Processing batch 243/550
Processing batch 244/550
Processing batch 245/550
Processing batch 246/550
Processing batch 247/550
Processing batch 248/550
Processing batch 249/550
Processing batch 250/550
Processing batch 251/550
Processing batch 252/550
Processing batch 253/550
Processing batch 254/550
Processing batch 255/550
Processing batch 256/550
Processing batch 257/550
Processing batch 258/550
Processing batch 259/550
Processing batch 260/550
Processing batch 261/550
Processing batch 262/550
Processing batch 263/550
Processing batch 264/550
Processing batch 265/550
Processing batch 266/550
Processing batch 267/550
Processing batch 268/550
Processing batch 269/550
Processing batch 270/550
Processing batch 271/550
Processing batch 272/550
Processing batch 273/550
Processing batch 274/550
Processing batch 275/550
Processing batch 276/550
Processing batch 277/550
Processing batch 278/550
Processing batch 279/550
Processing batch 280/550
Processing batch 281/550
Processing batch 282/550
Processing batch 283/550
Processing batch 284/550
Processing batch 285/550
Processing batch 286/550
Processing batch 287/550
Processing batch 288/550
Processing batch 289/550
Processing batch 290/550
Processing batch 291/550
Processing batch 292/550
Processing batch 293/550
Processing batch 294/550
Processing batch 295/550
Processing batch 296/550
Processing batch 297/550
Processing batch 298/550
Processing batch 299/550
Processing batch 300/550
Processing batch 301/550
Processing batch 302/550
Processing batch 303/550
Processing batch 304/550
Processing batch 305/550
Processing batch 306/550
Processing batch 307/550
Processing batch 308/550
Processing batch 309/550
Processing batch 310/550
Processing batch 311/550
Processing batch 312/550
Processing batch 313/550
Processing batch 314/550
Processing batch 315/550
Processing batch 316/550
Processing batch 317/550
Processing batch 318/550
Processing batch 319/550
Processing batch 320/550
Processing batch 321/550
Processing batch 322/550
Processing batch 323/550
Processing batch 324/550
Processing batch 325/550
Processing batch 326/550
Processing batch 327/550
Processing batch 328/550
Processing batch 329/550
Processing batch 330/550
Processing batch 331/550
Processing batch 332/550
Processing batch 333/550
Processing batch 334/550
Processing batch 335/550
Processing batch 336/550
Processing batch 337/550
Processing batch 338/550
Processing batch 339/550
Processing batch 340/550
Processing batch 341/550
Processing batch 342/550
Processing batch 343/550
Processing batch 344/550
Processing batch 345/550
Processing batch 346/550
Processing batch 347/550
Processing batch 348/550
Processing batch 349/550
Processing batch 350/550
Processing batch 351/550
Processing batch 352/550
Processing batch 353/550
Processing batch 354/550
Processing batch 355/550
Processing batch 356/550
Processing batch 357/550
Processing batch 358/550
Processing batch 359/550
Processing batch 360/550
Processing batch 361/550
Processing batch 362/550
Processing batch 363/550
Processing batch 364/550
Processing batch 365/550
Processing batch 366/550
Processing batch 367/550
Processing batch 368/550
Processing batch 369/550
Processing batch 370/550
Processing batch 371/550
Processing batch 372/550
Processing batch 373/550
Processing batch 374/550
Processing batch 375/550
Processing batch 376/550
Processing batch 377/550
Processing batch 378/550
Processing batch 379/550
Processing batch 380/550
Processing batch 381/550
Processing batch 382/550
Processing batch 383/550
Processing batch 384/550
Processing batch 385/550
Processing batch 386/550
Processing batch 387/550
Processing batch 388/550
Processing batch 389/550
Processing batch 390/550
Processing batch 391/550
Processing batch 392/550
Processing batch 393/550
Processing batch 394/550
Processing batch 395/550
Processing batch 396/550
Processing batch 397/550
Processing batch 398/550
Processing batch 399/550
Processing batch 400/550
Processing batch 401/550
Processing batch 402/550
Processing batch 403/550
Processing batch 404/550
Processing batch 405/550
Processing batch 406/550
Processing batch 407/550
Processing batch 408/550
Processing batch 409/550
Processing batch 410/550
Processing batch 411/550
Processing batch 412/550
Processing batch 413/550
Processing batch 414/550
Processing batch 415/550
Processing batch 416/550
Processing batch 417/550
Processing batch 418/550
Processing batch 419/550
Processing batch 420/550
Processing batch 421/550
Processing batch 422/550
Processing batch 423/550
Processing batch 424/550
Processing batch 425/550
Processing batch 426/550
Processing batch 427/550
Processing batch 428/550
Processing batch 429/550
Processing batch 430/550
Processing batch 431/550
Processing batch 432/550
Processing batch 433/550
Processing batch 434/550
Processing batch 435/550
Processing batch 436/550
Processing batch 437/550
Processing batch 438/550
Processing batch 439/550
Processing batch 440/550
Processing batch 441/550
Processing batch 442/550
Processing batch 443/550
Processing batch 444/550
Processing batch 445/550
Processing batch 446/550
Processing batch 447/550
Processing batch 448/550
Processing batch 449/550
Processing batch 450/550
Processing batch 451/550
Processing batch 452/550
Processing batch 453/550
Processing batch 454/550
Processing batch 455/550
Processing batch 456/550
Processing batch 457/550
Processing batch 458/550
Processing batch 459/550
Processing batch 460/550
Processing batch 461/550
Processing batch 462/550
Processing batch 463/550
Processing batch 464/550
Processing batch 465/550
Processing batch 466/550
Processing batch 467/550
Processing batch 468/550
Processing batch 469/550
Processing batch 470/550
Processing batch 471/550
Processing batch 472/550
Processing batch 473/550
Processing batch 474/550
Processing batch 475/550
Processing batch 476/550
Processing batch 477/550
Processing batch 478/550
Processing batch 479/550
Processing batch 480/550
Processing batch 481/550
Processing batch 482/550
Processing batch 483/550
Processing batch 484/550
Processing batch 485/550
Processing batch 486/550
Processing batch 487/550
Processing batch 488/550
Processing batch 489/550
Processing batch 490/550
Processing batch 491/550
Processing batch 492/550
Processing batch 493/550
Processing batch 494/550
Processing batch 495/550
Processing batch 496/550
Processing batch 497/550
Processing batch 498/550
Processing batch 499/550
Processing batch 500/550
Processing batch 501/550
Processing batch 502/550
Processing batch 503/550
Processing batch 504/550
Processing batch 505/550
Processing batch 506/550
Processing batch 507/550
Processing batch 508/550
Processing batch 509/550
Processing batch 510/550
Processing batch 511/550
Processing batch 512/550
Processing batch 513/550
Processing batch 514/550
Processing batch 515/550
Processing batch 516/550
Processing batch 517/550
Processing batch 518/550
Processing batch 519/550
Processing batch 520/550
Processing batch 521/550
Processing batch 522/550
Processing batch 523/550
Processing batch 524/550
Processing batch 525/550
Processing batch 526/550
Processing batch 527/550
Processing batch 528/550
Processing batch 529/550
Processing batch 530/550
Processing batch 531/550
Processing batch 532/550
Processing batch 533/550
Processing batch 534/550
Processing batch 535/550
Processing batch 536/550
Processing batch 537/550
Processing batch 538/550
Processing batch 539/550
Processing batch 540/550
Processing batch 541/550
Processing batch 542/550
Processing batch 543/550
Processing batch 544/550
Processing batch 545/550
Processing batch 546/550
Processing batch 547/550
Processing batch 548/550
Processing batch 549/550
Processing batch 550/550
Generated lattice embeddings shape: torch.Size([549367, 768])
Generating lattice containment embeddings for validation...
Generating lattice containment embeddings on cuda
Processing 9842 samples in batches of 1000
Processing batch 1/10
Processing batch 2/10
Processing batch 3/10
Processing batch 4/10
Processing batch 5/10
Processing batch 6/10
Processing batch 7/10
Processing batch 8/10
Processing batch 9/10
Processing batch 10/10
Generated lattice embeddings shape: torch.Size([9842, 768])
Generating lattice containment embeddings for test...
Generating lattice containment embeddings on cuda
Processing 9824 samples in batches of 1000
Processing batch 1/10
Processing batch 2/10
Processing batch 3/10
Processing batch 4/10
Processing batch 5/10
Processing batch 6/10
Processing batch 7/10
Processing batch 8/10
Processing batch 9/10
Processing batch 10/10
Generated lattice embeddings shape: torch.Size([9824, 768])

Dataset Statistics:
------------------------------
Train: {'entailment': 183416, 'neutral': 182764, 'contradiction': 183187}
Validation: {'entailment': 3329, 'neutral': 3235, 'contradiction': 3278}
Test: {'entailment': 3368, 'neutral': 3219, 'contradiction': 3237}

Data loading pipeline completed!
Creating balanced data loaders...
Batch size: 30
Samples per class per batch: 10
Effective batch size: 30
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class: 10
  Batch size: 30
  Number of batches: 18276
  Class 2: 183187 samples
  Class 0: 183416 samples
  Class 1: 182764 samples
BalancedBatchSampler initialized:
  Classes: [2, 0, 1]
  Samples per class: 10
  Batch size: 30
  Number of batches: 323
  Class 2: 3278 samples
  Class 0: 3329 samples
  Class 1: 3235 samples
Data loading completed!
Train: 549367 samples
Val: 9842 samples
Test: 9824 samples
Creating model and trainer...
==================================================
Model created with 1,108,630 parameters
Loss function created (α=1.0, γ=0.0001)
Optimizer created (lr=0.0001)
Trainer initialized on device: cuda
Model parameters: 1,108,630
Starting model training...
==================================================
Starting training...
Beta scheduling enabled:
  Warmup epochs: 0
  Max beta: 2.0
  Schedule type: linear
KL scheduling enabled:
  Warmup epochs: 10
  Max KL: 0.0005
  Schedule type: linear
==================================================

Epoch 1/50
------------------------------
Batch 0 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.106402, 0.123789]
mu std: 0.033880
Batch 0/18276: Loss = 0.0881 (C: 14.7170, R: 0.0881, KL: 0.0000) | β=0.000, γ=0.0000
Batch 100/18276: Loss = 0.0120 (C: 14.7747, R: 0.0120, KL: 0.0000) | β=0.000, γ=0.0000
Batch 200/18276: Loss = 0.0036 (C: 14.5645, R: 0.0036, KL: 0.0000) | β=0.000, γ=0.0000
Batch 300/18276: Loss = 0.0026 (C: 14.7404, R: 0.0026, KL: 0.0000) | β=0.000, γ=0.0000
Batch 400/18276: Loss = 0.0020 (C: 14.6469, R: 0.0020, KL: 0.0000) | β=0.000, γ=0.0000
Batch 500/18276: Loss = 0.0018 (C: 14.5569, R: 0.0018, KL: 0.0000) | β=0.000, γ=0.0000
Batch 600/18276: Loss = 0.0016 (C: 14.4624, R: 0.0016, KL: 0.0000) | β=0.000, γ=0.0000
Batch 700/18276: Loss = 0.0016 (C: 14.6056, R: 0.0016, KL: 0.0000) | β=0.000, γ=0.0000
Batch 800/18276: Loss = 0.0016 (C: 14.6515, R: 0.0016, KL: 0.0000) | β=0.000, γ=0.0000
Batch 900/18276: Loss = 0.0015 (C: 14.4144, R: 0.0015, KL: 0.0000) | β=0.000, γ=0.0000
mu range: [-1.015778, 1.065605]
mu std: 0.254224
Batch 1000/18276: Loss = 0.0015 (C: 14.6152, R: 0.0015, KL: 0.0000) | β=0.000, γ=0.0000
Batch 1100/18276: Loss = 0.0016 (C: 14.7129, R: 0.0016, KL: 0.0000) | β=0.000, γ=0.0000
Batch 1200/18276: Loss = 0.0015 (C: 14.7008, R: 0.0015, KL: 0.0000) | β=0.000, γ=0.0000
Batch 1300/18276: Loss = 0.0014 (C: 14.6952, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 1400/18276: Loss = 0.0015 (C: 14.4669, R: 0.0015, KL: 0.0000) | β=0.000, γ=0.0000
Batch 1500/18276: Loss = 0.0015 (C: 14.7184, R: 0.0015, KL: 0.0000) | β=0.000, γ=0.0000
Batch 1600/18276: Loss = 0.0014 (C: 14.6631, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 1700/18276: Loss = 0.0014 (C: 14.8417, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 1800/18276: Loss = 0.0015 (C: 14.6951, R: 0.0015, KL: 0.0000) | β=0.000, γ=0.0000
Batch 1900/18276: Loss = 0.0014 (C: 14.6910, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 2000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.078844, 0.070592]
mu std: 0.025262

=== DIAGNOSTIC AT BATCH 2000 ===
Contrastive loss stuck at: 14.7595
Contrastive weight (β): 0.0000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.265673
  Max: 1.000000
  Mean: 0.043051
  Std: 0.213598
Average feature variance: 0.013199

Positive pair similarities:
  Mean: 0.010789
  Std: 0.115213

Negative pair similarities:
  Mean: 0.009721
  Std: 0.122748

Separation (pos_mean - neg_mean): 0.001068
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-2.66, 3.38]
  Negative similarities: [-2.50, 3.39]
=== END DIAGNOSTIC ===

Batch 2000/18276: Loss = 0.0014 (C: 14.7595, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 2100/18276: Loss = 0.0014 (C: 14.7094, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 2200/18276: Loss = 0.0013 (C: 14.8146, R: 0.0013, KL: 0.0000) | β=0.000, γ=0.0000
Batch 2300/18276: Loss = 0.0014 (C: 14.8315, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 2400/18276: Loss = 0.0014 (C: 14.8077, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 2500/18276: Loss = 0.0014 (C: 14.7159, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 2600/18276: Loss = 0.0014 (C: 14.8967, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 2700/18276: Loss = 0.0014 (C: 14.7494, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 2800/18276: Loss = 0.0014 (C: 14.6140, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 2900/18276: Loss = 0.0014 (C: 14.9800, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
mu range: [-0.025208, 0.023965]
mu std: 0.010039
Batch 3000/18276: Loss = 0.0014 (C: 14.6715, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 3100/18276: Loss = 0.0014 (C: 14.8273, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 3200/18276: Loss = 0.0014 (C: 14.7629, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 3300/18276: Loss = 0.0014 (C: 14.7876, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 3400/18276: Loss = 0.0014 (C: 14.8414, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 3500/18276: Loss = 0.0014 (C: 14.9498, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 3600/18276: Loss = 0.0014 (C: 14.8370, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 3700/18276: Loss = 0.0014 (C: 14.6003, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 3800/18276: Loss = 0.0014 (C: 14.7314, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 3900/18276: Loss = 0.0014 (C: 14.9294, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 4000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.015061, 0.015379]
mu std: 0.006597

=== DIAGNOSTIC AT BATCH 4000 ===
Contrastive loss stuck at: 14.6559
Contrastive weight (β): 0.0000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.335703
  Max: 1.000000
  Mean: 0.024071
  Std: 0.210583
Average feature variance: 0.013461

Positive pair similarities:
  Mean: -0.006356
  Std: 0.107255

Negative pair similarities:
  Mean: -0.011033
  Std: 0.109707

Separation (pos_mean - neg_mean): 0.004677
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-2.61, 2.83]
  Negative similarities: [-3.36, 3.03]
=== END DIAGNOSTIC ===

Batch 4000/18276: Loss = 0.0014 (C: 14.6559, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 4100/18276: Loss = 0.0014 (C: 14.6926, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 4200/18276: Loss = 0.0014 (C: 14.6829, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 4300/18276: Loss = 0.0014 (C: 14.4283, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 4400/18276: Loss = 0.0014 (C: 14.7090, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 4500/18276: Loss = 0.0014 (C: 14.7560, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 4600/18276: Loss = 0.0014 (C: 14.8909, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 4700/18276: Loss = 0.0014 (C: 14.7186, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 4800/18276: Loss = 0.0014 (C: 14.8593, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 4900/18276: Loss = 0.0014 (C: 14.8493, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
mu range: [-0.010762, 0.009550]
mu std: 0.003965
Batch 5000/18276: Loss = 0.0014 (C: 14.6501, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 5100/18276: Loss = 0.0014 (C: 14.9938, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 5200/18276: Loss = 0.0014 (C: 14.7421, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 5300/18276: Loss = 0.0014 (C: 14.8035, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 5400/18276: Loss = 0.0014 (C: 14.6079, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 5500/18276: Loss = 0.0014 (C: 14.6749, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 5600/18276: Loss = 0.0015 (C: 14.7461, R: 0.0015, KL: 0.0000) | β=0.000, γ=0.0000
Batch 5700/18276: Loss = 0.0014 (C: 14.6796, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 5800/18276: Loss = 0.0014 (C: 14.6798, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 5900/18276: Loss = 0.0014 (C: 14.6145, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 6000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.006550, 0.004389]
mu std: 0.001938

=== DIAGNOSTIC AT BATCH 6000 ===
Contrastive loss stuck at: 14.6949
Contrastive weight (β): 0.0000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.325952
  Max: 1.000000
  Mean: 0.029281
  Std: 0.214325
Average feature variance: 0.013389

Positive pair similarities:
  Mean: -0.000062
  Std: 0.107338

Negative pair similarities:
  Mean: -0.006051
  Std: 0.122210

Separation (pos_mean - neg_mean): 0.005989
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-2.50, 3.26]
  Negative similarities: [-3.26, 3.32]
=== END DIAGNOSTIC ===

Batch 6000/18276: Loss = 0.0014 (C: 14.6949, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 6100/18276: Loss = 0.0014 (C: 14.7180, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 6200/18276: Loss = 0.0014 (C: 14.6423, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 6300/18276: Loss = 0.0014 (C: 14.8875, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 6400/18276: Loss = 0.0014 (C: 14.4160, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 6500/18276: Loss = 0.0014 (C: 14.7300, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 6600/18276: Loss = 0.0014 (C: 14.7192, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 6700/18276: Loss = 0.0014 (C: 14.6186, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 6800/18276: Loss = 0.0014 (C: 14.5916, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 6900/18276: Loss = 0.0013 (C: 14.9627, R: 0.0013, KL: 0.0000) | β=0.000, γ=0.0000
mu range: [-0.003026, 0.001210]
mu std: 0.000712
Batch 7000/18276: Loss = 0.0014 (C: 14.8459, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 7100/18276: Loss = 0.0014 (C: 14.7282, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 7200/18276: Loss = 0.0014 (C: 14.6713, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 7300/18276: Loss = 0.0014 (C: 14.8277, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 7400/18276: Loss = 0.0014 (C: 14.6103, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 7500/18276: Loss = 0.0014 (C: 14.6631, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 7600/18276: Loss = 0.0014 (C: 14.7789, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 7700/18276: Loss = 0.0014 (C: 14.5286, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 7800/18276: Loss = 0.0014 (C: 14.8490, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 7900/18276: Loss = 0.0014 (C: 14.8289, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 8000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.000986, 0.000162]
mu std: 0.000172

=== DIAGNOSTIC AT BATCH 8000 ===
Contrastive loss stuck at: 14.5487
Contrastive weight (β): 0.0000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.275276
  Max: 1.000000
  Mean: 0.030076
  Std: 0.211350
Average feature variance: 0.013378

Positive pair similarities:
  Mean: 0.009131
  Std: 0.120153

Negative pair similarities:
  Mean: -0.008995
  Std: 0.108233

Separation (pos_mean - neg_mean): 0.018126
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-2.44, 3.63]
  Negative similarities: [-2.75, 2.86]
=== END DIAGNOSTIC ===

Batch 8000/18276: Loss = 0.0014 (C: 14.5487, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 8100/18276: Loss = 0.0014 (C: 14.8431, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 8200/18276: Loss = 0.0014 (C: 14.6968, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 8300/18276: Loss = 0.0014 (C: 14.5916, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 8400/18276: Loss = 0.0014 (C: 14.8251, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 8500/18276: Loss = 0.0014 (C: 14.6981, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 8600/18276: Loss = 0.0014 (C: 14.7556, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 8700/18276: Loss = 0.0014 (C: 14.8951, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 8800/18276: Loss = 0.0014 (C: 14.6168, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 8900/18276: Loss = 0.0014 (C: 14.5751, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
mu range: [-0.000153, 0.000009]
mu std: 0.000021
Batch 9000/18276: Loss = 0.0014 (C: 14.6489, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 9100/18276: Loss = 0.0014 (C: 14.8052, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 9200/18276: Loss = 0.0014 (C: 14.8390, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 9300/18276: Loss = 0.0014 (C: 14.7742, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 9400/18276: Loss = 0.0014 (C: 14.7726, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 9500/18276: Loss = 0.0014 (C: 14.7558, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 9600/18276: Loss = 0.0014 (C: 14.6825, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 9700/18276: Loss = 0.0014 (C: 14.7918, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 9800/18276: Loss = 0.0014 (C: 14.7497, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 9900/18276: Loss = 0.0014 (C: 14.7843, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 10000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.000007, 0.000000]
mu std: 0.000001

=== DIAGNOSTIC AT BATCH 10000 ===
Contrastive loss stuck at: 14.7316
Contrastive weight (β): 0.0000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.333483
  Max: 1.000000
  Mean: 0.030116
  Std: 0.209220
Average feature variance: 0.013378

Positive pair similarities:
  Mean: -0.005466
  Std: 0.096458

Negative pair similarities:
  Mean: -0.002367
  Std: 0.113040

Separation (pos_mean - neg_mean): -0.003100
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-2.69, 2.64]
  Negative similarities: [-3.33, 3.11]
=== END DIAGNOSTIC ===

Batch 10000/18276: Loss = 0.0014 (C: 14.7316, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 10100/18276: Loss = 0.0014 (C: 14.6423, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 10200/18276: Loss = 0.0014 (C: 14.7664, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 10300/18276: Loss = 0.0014 (C: 14.5894, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 10400/18276: Loss = 0.0014 (C: 14.8374, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 10500/18276: Loss = 0.0014 (C: 14.7589, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 10600/18276: Loss = 0.0014 (C: 14.7365, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 10700/18276: Loss = 0.0014 (C: 14.9247, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 10800/18276: Loss = 0.0014 (C: 14.7295, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 10900/18276: Loss = 0.0014 (C: 14.8561, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
mu range: [-0.000000, 0.000000]
mu std: 0.000000
Batch 11000/18276: Loss = 0.0014 (C: 14.7692, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 11100/18276: Loss = 0.0014 (C: 14.8251, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 11200/18276: Loss = 0.0014 (C: 14.5020, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 11300/18276: Loss = 0.0014 (C: 14.6861, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 11400/18276: Loss = 0.0013 (C: 14.8159, R: 0.0013, KL: 0.0000) | β=0.000, γ=0.0000
Batch 11500/18276: Loss = 0.0014 (C: 14.9272, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 11600/18276: Loss = 0.0014 (C: 14.6969, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 11700/18276: Loss = 0.0014 (C: 14.9067, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 11800/18276: Loss = 0.0014 (C: 14.5862, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 11900/18276: Loss = 0.0014 (C: 14.9254, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 12000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.000000, 0.000000]
mu std: 0.000000

=== DIAGNOSTIC AT BATCH 12000 ===
Contrastive loss stuck at: 14.8511
Contrastive weight (β): 0.0000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.306867
  Max: 1.000000
  Mean: 0.034251
  Std: 0.210580
Average feature variance: 0.013321

Positive pair similarities:
  Mean: -0.008532
  Std: 0.115153

Negative pair similarities:
  Mean: 0.005217
  Std: 0.110530

Separation (pos_mean - neg_mean): -0.013748
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-3.07, 2.20]
  Negative similarities: [-3.00, 3.13]
=== END DIAGNOSTIC ===

Batch 12000/18276: Loss = 0.0014 (C: 14.8511, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 12100/18276: Loss = 0.0014 (C: 14.6433, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 12200/18276: Loss = 0.0013 (C: 14.8530, R: 0.0013, KL: 0.0000) | β=0.000, γ=0.0000
Batch 12300/18276: Loss = 0.0014 (C: 14.5538, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 12400/18276: Loss = 0.0014 (C: 14.8042, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 12500/18276: Loss = 0.0014 (C: 14.5947, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 12600/18276: Loss = 0.0014 (C: 14.7721, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 12700/18276: Loss = 0.0014 (C: 14.7078, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 12800/18276: Loss = 0.0014 (C: 14.7674, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 12900/18276: Loss = 0.0014 (C: 14.7815, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
mu range: [-0.000000, 0.000000]
mu std: 0.000000
Batch 13000/18276: Loss = 0.0014 (C: 14.6514, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 13100/18276: Loss = 0.0014 (C: 14.9089, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 13200/18276: Loss = 0.0014 (C: 14.6484, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 13300/18276: Loss = 0.0014 (C: 14.7406, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 13400/18276: Loss = 0.0014 (C: 14.6604, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 13500/18276: Loss = 0.0014 (C: 14.8239, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 13600/18276: Loss = 0.0014 (C: 14.8945, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 13700/18276: Loss = 0.0014 (C: 14.7923, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 13800/18276: Loss = 0.0014 (C: 14.6848, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 13900/18276: Loss = 0.0014 (C: 14.9164, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 14000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.000000, 0.000000]
mu std: 0.000000

=== DIAGNOSTIC AT BATCH 14000 ===
Contrastive loss stuck at: 14.6991
Contrastive weight (β): 0.0000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.423711
  Max: 1.000000
  Mean: 0.031783
  Std: 0.215240
Average feature variance: 0.013355

Positive pair similarities:
  Mean: 0.003038
  Std: 0.122094

Negative pair similarities:
  Mean: -0.003692
  Std: 0.119386

Separation (pos_mean - neg_mean): 0.006730
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-3.41, 3.07]
  Negative similarities: [-4.24, 3.05]
=== END DIAGNOSTIC ===

Batch 14000/18276: Loss = 0.0014 (C: 14.6991, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 14100/18276: Loss = 0.0014 (C: 14.7054, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 14200/18276: Loss = 0.0014 (C: 14.8479, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 14300/18276: Loss = 0.0014 (C: 14.6417, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 14400/18276: Loss = 0.0014 (C: 14.6894, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 14500/18276: Loss = 0.0014 (C: 14.7022, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 14600/18276: Loss = 0.0014 (C: 14.6140, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 14700/18276: Loss = 0.0014 (C: 14.7217, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 14800/18276: Loss = 0.0014 (C: 14.6537, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 14900/18276: Loss = 0.0014 (C: 14.7161, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
mu range: [-0.000000, 0.000000]
mu std: 0.000000
Batch 15000/18276: Loss = 0.0014 (C: 14.5817, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 15100/18276: Loss = 0.0014 (C: 14.7126, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 15200/18276: Loss = 0.0014 (C: 14.7232, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 15300/18276: Loss = 0.0014 (C: 14.5849, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 15400/18276: Loss = 0.0014 (C: 14.8026, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 15500/18276: Loss = 0.0014 (C: 14.7839, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 15600/18276: Loss = 0.0014 (C: 14.8320, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 15700/18276: Loss = 0.0014 (C: 14.6904, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 15800/18276: Loss = 0.0014 (C: 14.8085, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 15900/18276: Loss = 0.0014 (C: 14.6243, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 16000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.000000, 0.000000]
mu std: 0.000000

=== DIAGNOSTIC AT BATCH 16000 ===
Contrastive loss stuck at: 14.6958
Contrastive weight (β): 0.0000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.309502
  Max: 1.000000
  Mean: 0.042984
  Std: 0.210710
Average feature variance: 0.013200

Positive pair similarities:
  Mean: 0.012095
  Std: 0.126060

Negative pair similarities:
  Mean: 0.009033
  Std: 0.109744

Separation (pos_mean - neg_mean): 0.003062
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-3.10, 3.61]
  Negative similarities: [-3.06, 2.63]
=== END DIAGNOSTIC ===

Batch 16000/18276: Loss = 0.0014 (C: 14.6958, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 16100/18276: Loss = 0.0014 (C: 14.7165, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 16200/18276: Loss = 0.0014 (C: 14.7630, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 16300/18276: Loss = 0.0014 (C: 14.6189, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 16400/18276: Loss = 0.0014 (C: 14.7441, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 16500/18276: Loss = 0.0014 (C: 14.9445, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 16600/18276: Loss = 0.0013 (C: 14.7105, R: 0.0013, KL: 0.0000) | β=0.000, γ=0.0000
Batch 16700/18276: Loss = 0.0014 (C: 14.7876, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 16800/18276: Loss = 0.0014 (C: 14.7805, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 16900/18276: Loss = 0.0014 (C: 14.4096, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
mu range: [-0.000000, 0.000000]
mu std: 0.000000
Batch 17000/18276: Loss = 0.0014 (C: 14.7817, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 17100/18276: Loss = 0.0014 (C: 14.9826, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 17200/18276: Loss = 0.0014 (C: 14.9422, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 17300/18276: Loss = 0.0013 (C: 14.7078, R: 0.0013, KL: 0.0000) | β=0.000, γ=0.0000
Batch 17400/18276: Loss = 0.0014 (C: 14.8655, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 17500/18276: Loss = 0.0014 (C: 14.9795, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 17600/18276: Loss = 0.0014 (C: 14.6672, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 17700/18276: Loss = 0.0014 (C: 14.6814, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 17800/18276: Loss = 0.0014 (C: 14.7741, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 17900/18276: Loss = 0.0014 (C: 14.8141, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 18000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.000000, 0.000000]
mu std: 0.000000

=== DIAGNOSTIC AT BATCH 18000 ===
Contrastive loss stuck at: 14.8117
Contrastive weight (β): 0.0000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.325432
  Max: 1.000000
  Mean: 0.030649
  Std: 0.213976
Average feature variance: 0.013370

Positive pair similarities:
  Mean: -0.007091
  Std: 0.122645

Negative pair similarities:
  Mean: -0.000836
  Std: 0.115181

Separation (pos_mean - neg_mean): -0.006254
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-2.99, 3.43]
  Negative similarities: [-3.25, 3.15]
=== END DIAGNOSTIC ===

Batch 18000/18276: Loss = 0.0014 (C: 14.8117, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 18100/18276: Loss = 0.0014 (C: 14.9061, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
Batch 18200/18276: Loss = 0.0014 (C: 14.8945, R: 0.0014, KL: 0.0000) | β=0.000, γ=0.0000
DEBUG: validate_epoch called with β=0.000, γ=0.0000
DEBUG: First validation batch:
  contrastive_loss raw: 14.723102569580078
  reconstruction_loss raw: 0.0013984915567561984
  kl_loss raw: 0.0
  total_loss raw: 0.0013984915567561984
Epoch 1 completed in 93.09s
Train Loss: 0.0016 (C: 14.7327, R: 0.0016) (β=0.000)
Val Loss: 0.0014 (C: 14.7453, R: 0.0014)
Checkpoint saved: entailment_surfaces/supervised_contrastive_autoencoder/experiments/contrastive_autoencoder_20250711_222503/checkpoints/best_model.pt
Warmup epoch - model saved (Val Loss: 0.0014)

Epoch 2/50
------------------------------
Batch 0 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.000000, 0.000000]
mu std: 0.000000
Batch 0/18276: Loss = 0.5947 (C: 14.8337, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 100/18276: Loss = 0.5984 (C: 14.9251, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 200/18276: Loss = 0.5902 (C: 14.7195, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 300/18276: Loss = 0.5916 (C: 14.7568, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 400/18276: Loss = 0.5885 (C: 14.6780, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 500/18276: Loss = 0.5931 (C: 14.7941, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 600/18276: Loss = 0.5949 (C: 14.8380, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 700/18276: Loss = 0.5914 (C: 14.7509, R: 0.0013, KL: 0.0000) | β=0.040, γ=0.0000
Batch 800/18276: Loss = 0.5985 (C: 14.9275, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 900/18276: Loss = 0.5906 (C: 14.7308, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
mu range: [-0.020334, 0.034705]
mu std: 0.010188
Batch 1000/18276: Loss = 0.5887 (C: 14.6829, R: 0.0013, KL: 0.0000) | β=0.040, γ=0.0000
Batch 1100/18276: Loss = 0.5750 (C: 14.3396, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 1200/18276: Loss = 0.5838 (C: 14.5606, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 1300/18276: Loss = 0.5907 (C: 14.7323, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 1400/18276: Loss = 0.5832 (C: 14.5453, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 1500/18276: Loss = 0.5891 (C: 14.6925, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 1600/18276: Loss = 0.5931 (C: 14.7924, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 1700/18276: Loss = 0.5901 (C: 14.7166, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 1800/18276: Loss = 0.5960 (C: 14.8632, R: 0.0015, KL: 0.0000) | β=0.040, γ=0.0000
Batch 1900/18276: Loss = 0.5888 (C: 14.6845, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 2000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.032397, 0.040895]
mu std: 0.013147

=== DIAGNOSTIC AT BATCH 2000 ===
Contrastive loss stuck at: 14.5667
Contrastive weight (β): 0.0400
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.346674
  Max: 1.000000
  Mean: 0.025496
  Std: 0.215919
Average feature variance: 0.013441

Positive pair similarities:
  Mean: 0.005783
  Std: 0.115233

Negative pair similarities:
  Mean: -0.014358
  Std: 0.121157

Separation (pos_mean - neg_mean): 0.020141
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-2.89, 3.30]
  Negative similarities: [-3.47, 3.55]
=== END DIAGNOSTIC ===

Batch 2000/18276: Loss = 0.5840 (C: 14.5667, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 2100/18276: Loss = 0.5945 (C: 14.8271, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 2200/18276: Loss = 0.5926 (C: 14.7813, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 2300/18276: Loss = 0.5938 (C: 14.8113, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 2400/18276: Loss = 0.5926 (C: 14.7791, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 2500/18276: Loss = 0.5924 (C: 14.7754, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 2600/18276: Loss = 0.5914 (C: 14.7514, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 2700/18276: Loss = 0.5867 (C: 14.6342, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 2800/18276: Loss = 0.5900 (C: 14.7161, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 2900/18276: Loss = 0.5950 (C: 14.8419, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
mu range: [-0.039247, 0.050340]
mu std: 0.015977
Batch 3000/18276: Loss = 0.5951 (C: 14.8429, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 3100/18276: Loss = 0.5955 (C: 14.8522, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 3200/18276: Loss = 0.5948 (C: 14.8354, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 3300/18276: Loss = 0.5896 (C: 14.7045, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 3400/18276: Loss = 0.5926 (C: 14.7816, R: 0.0013, KL: 0.0000) | β=0.040, γ=0.0000
Batch 3500/18276: Loss = 0.5841 (C: 14.5676, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 3600/18276: Loss = 0.5968 (C: 14.8844, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 3700/18276: Loss = 0.5835 (C: 14.5529, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 3800/18276: Loss = 0.5885 (C: 14.6774, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 3900/18276: Loss = 0.5965 (C: 14.8762, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 4000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.048879, 0.061219]
mu std: 0.019601

=== DIAGNOSTIC AT BATCH 4000 ===
Contrastive loss stuck at: 14.6218
Contrastive weight (β): 0.0400
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.315966
  Max: 1.000000
  Mean: 0.036588
  Std: 0.212162
Average feature variance: 0.013288

Positive pair similarities:
  Mean: 0.011641
  Std: 0.118957

Negative pair similarities:
  Mean: -0.000356
  Std: 0.114318

Separation (pos_mean - neg_mean): 0.011997
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-2.60, 3.65]
  Negative similarities: [-3.16, 3.02]
=== END DIAGNOSTIC ===

Batch 4000/18276: Loss = 0.5862 (C: 14.6218, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 4100/18276: Loss = 0.5993 (C: 14.9482, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 4200/18276: Loss = 0.5962 (C: 14.8706, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 4300/18276: Loss = 0.5967 (C: 14.8830, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 4400/18276: Loss = 0.5932 (C: 14.7950, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 4500/18276: Loss = 0.5868 (C: 14.6355, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 4600/18276: Loss = 0.5925 (C: 14.7770, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 4700/18276: Loss = 0.5872 (C: 14.6462, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 4800/18276: Loss = 0.5947 (C: 14.8306, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 4900/18276: Loss = 0.5965 (C: 14.8796, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
mu range: [-0.056204, 0.069763]
mu std: 0.022872
Batch 5000/18276: Loss = 0.5946 (C: 14.8306, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 5100/18276: Loss = 0.5954 (C: 14.8499, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 5200/18276: Loss = 0.5855 (C: 14.6022, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 5300/18276: Loss = 0.5854 (C: 14.6001, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 5400/18276: Loss = 0.5861 (C: 14.6177, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 5500/18276: Loss = 0.5932 (C: 14.7969, R: 0.0013, KL: 0.0000) | β=0.040, γ=0.0000
Batch 5600/18276: Loss = 0.5909 (C: 14.7387, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 5700/18276: Loss = 0.5921 (C: 14.7672, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 5800/18276: Loss = 0.5923 (C: 14.7732, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 5900/18276: Loss = 0.5897 (C: 14.7081, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 6000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.066910, 0.088181]
mu std: 0.027696

=== DIAGNOSTIC AT BATCH 6000 ===
Contrastive loss stuck at: 14.5901
Contrastive weight (β): 0.0400
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.357501
  Max: 1.000000
  Mean: 0.033632
  Std: 0.212276
Average feature variance: 0.013329

Positive pair similarities:
  Mean: 0.009970
  Std: 0.104607

Negative pair similarities:
  Mean: -0.004038
  Std: 0.119456

Separation (pos_mean - neg_mean): 0.014008
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-2.69, 3.10]
  Negative similarities: [-3.58, 3.02]
=== END DIAGNOSTIC ===

Batch 6000/18276: Loss = 0.5850 (C: 14.5901, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 6100/18276: Loss = 0.5852 (C: 14.5945, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 6200/18276: Loss = 0.5914 (C: 14.7485, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 6300/18276: Loss = 0.5987 (C: 14.9315, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 6400/18276: Loss = 0.5934 (C: 14.8003, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 6500/18276: Loss = 0.5989 (C: 14.9391, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 6600/18276: Loss = 0.5900 (C: 14.7146, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 6700/18276: Loss = 0.5873 (C: 14.6499, R: 0.0013, KL: 0.0000) | β=0.040, γ=0.0000
Batch 6800/18276: Loss = 0.5876 (C: 14.6567, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 6900/18276: Loss = 0.5952 (C: 14.8448, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
mu range: [-0.072550, 0.104413]
mu std: 0.032549
Batch 7000/18276: Loss = 0.5914 (C: 14.7514, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 7100/18276: Loss = 0.5931 (C: 14.7936, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 7200/18276: Loss = 0.5907 (C: 14.7320, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 7300/18276: Loss = 0.5887 (C: 14.6840, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 7400/18276: Loss = 0.5917 (C: 14.7566, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 7500/18276: Loss = 0.5970 (C: 14.8905, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 7600/18276: Loss = 0.5907 (C: 14.7328, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 7700/18276: Loss = 0.5948 (C: 14.8354, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 7800/18276: Loss = 0.5858 (C: 14.6097, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 7900/18276: Loss = 0.5880 (C: 14.6659, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 8000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.087200, 0.118458]
mu std: 0.038072

=== DIAGNOSTIC AT BATCH 8000 ===
Contrastive loss stuck at: 14.5414
Contrastive weight (β): 0.0400
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.319415
  Max: 1.000000
  Mean: 0.034301
  Std: 0.212212
Average feature variance: 0.013320

Positive pair similarities:
  Mean: 0.014899
  Std: 0.113880

Negative pair similarities:
  Mean: -0.005253
  Std: 0.115416

Separation (pos_mean - neg_mean): 0.020152
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-1.98, 3.24]
  Negative similarities: [-3.19, 3.14]
=== END DIAGNOSTIC ===

Batch 8000/18276: Loss = 0.5831 (C: 14.5414, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 8100/18276: Loss = 0.5888 (C: 14.6847, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 8200/18276: Loss = 0.5968 (C: 14.8834, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 8300/18276: Loss = 0.5883 (C: 14.6737, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 8400/18276: Loss = 0.5895 (C: 14.7017, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 8500/18276: Loss = 0.5887 (C: 14.6813, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 8600/18276: Loss = 0.5858 (C: 14.6118, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 8700/18276: Loss = 0.5798 (C: 14.4606, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 8800/18276: Loss = 0.5934 (C: 14.8000, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 8900/18276: Loss = 0.6016 (C: 15.0058, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
mu range: [-0.102320, 0.139430]
mu std: 0.044582
Batch 9000/18276: Loss = 0.5970 (C: 14.8901, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 9100/18276: Loss = 0.5904 (C: 14.7247, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 9200/18276: Loss = 0.5931 (C: 14.7920, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 9300/18276: Loss = 0.5762 (C: 14.3707, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 9400/18276: Loss = 0.5827 (C: 14.5331, R: 0.0013, KL: 0.0000) | β=0.040, γ=0.0000
Batch 9500/18276: Loss = 0.5834 (C: 14.5511, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 9600/18276: Loss = 0.5944 (C: 14.8248, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 9700/18276: Loss = 0.5859 (C: 14.6111, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 9800/18276: Loss = 0.5971 (C: 14.8923, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 9900/18276: Loss = 0.5920 (C: 14.7639, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 10000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.120502, 0.162602]
mu std: 0.051974

=== DIAGNOSTIC AT BATCH 10000 ===
Contrastive loss stuck at: 14.7284
Contrastive weight (β): 0.0400
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.401245
  Max: 1.000000
  Mean: 0.034224
  Std: 0.212909
Average feature variance: 0.013321

Positive pair similarities:
  Mean: 0.001907
  Std: 0.117655

Negative pair similarities:
  Mean: 0.000477
  Std: 0.116152

Separation (pos_mean - neg_mean): 0.001430
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-4.01, 3.07]
  Negative similarities: [-3.31, 4.34]
=== END DIAGNOSTIC ===

Batch 10000/18276: Loss = 0.5905 (C: 14.7284, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 10100/18276: Loss = 0.5950 (C: 14.8389, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 10200/18276: Loss = 0.5859 (C: 14.6130, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 10300/18276: Loss = 0.5891 (C: 14.6937, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 10400/18276: Loss = 0.5864 (C: 14.6251, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 10500/18276: Loss = 0.5930 (C: 14.7904, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 10600/18276: Loss = 0.5911 (C: 14.7447, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 10700/18276: Loss = 0.5893 (C: 14.6981, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 10800/18276: Loss = 0.5882 (C: 14.6693, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 10900/18276: Loss = 0.5843 (C: 14.5725, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
mu range: [-0.143689, 0.193175]
mu std: 0.061501
Batch 11000/18276: Loss = 0.5813 (C: 14.4995, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 11100/18276: Loss = 0.5919 (C: 14.7642, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 11200/18276: Loss = 0.5851 (C: 14.5927, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 11300/18276: Loss = 0.5875 (C: 14.6534, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 11400/18276: Loss = 0.5830 (C: 14.5411, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 11500/18276: Loss = 0.5887 (C: 14.6832, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 11600/18276: Loss = 0.5913 (C: 14.7484, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 11700/18276: Loss = 0.5865 (C: 14.6290, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 11800/18276: Loss = 0.5885 (C: 14.6772, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 11900/18276: Loss = 0.5994 (C: 14.9493, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 12000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-0.174508, 0.229448]
mu std: 0.074935

=== DIAGNOSTIC AT BATCH 12000 ===
Contrastive loss stuck at: 14.7742
Contrastive weight (β): 0.0400
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.298025
  Max: 1.000000
  Mean: 0.050383
  Std: 0.208958
Average feature variance: 0.013098

Positive pair similarities:
  Mean: 0.013695
  Std: 0.108985

Negative pair similarities:
  Mean: 0.019411
  Std: 0.116043

Separation (pos_mean - neg_mean): -0.005716
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-2.62, 2.76]
  Negative similarities: [-2.98, 3.91]
=== END DIAGNOSTIC ===

Batch 12000/18276: Loss = 0.5924 (C: 14.7742, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 12100/18276: Loss = 0.5868 (C: 14.6369, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 12200/18276: Loss = 0.5974 (C: 14.9011, R: 0.0013, KL: 0.0000) | β=0.040, γ=0.0000
Batch 12300/18276: Loss = 0.5928 (C: 14.7838, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 12400/18276: Loss = 0.5878 (C: 14.6595, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 12500/18276: Loss = 0.5884 (C: 14.6750, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 12600/18276: Loss = 0.5939 (C: 14.8127, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 12700/18276: Loss = 0.5934 (C: 14.7993, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 12800/18276: Loss = 0.5913 (C: 14.7477, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 12900/18276: Loss = 0.5910 (C: 14.7385, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
mu range: [-0.231697, 0.298837]
mu std: 0.096748
Batch 13000/18276: Loss = 0.5893 (C: 14.6976, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 13100/18276: Loss = 0.5911 (C: 14.7436, R: 0.0013, KL: 0.0000) | β=0.040, γ=0.0000
Batch 13200/18276: Loss = 0.5946 (C: 14.8304, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 13300/18276: Loss = 0.5864 (C: 14.6252, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 13400/18276: Loss = 0.5877 (C: 14.6568, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 13500/18276: Loss = 0.5902 (C: 14.7197, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 13600/18276: Loss = 0.5945 (C: 14.8288, R: 0.0013, KL: 0.0000) | β=0.040, γ=0.0000
Batch 13700/18276: Loss = 0.5884 (C: 14.6756, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 13800/18276: Loss = 0.5879 (C: 14.6622, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 13900/18276: Loss = 0.5863 (C: 14.6224, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 14000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-2.857497, 3.499274]
mu std: 0.812586

=== DIAGNOSTIC AT BATCH 14000 ===
Contrastive loss stuck at: 14.8160
Contrastive weight (β): 0.0400
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: -0.017585
  Max: 1.000000
  Mean: 0.416171
  Std: 0.162658
Average feature variance: 0.008053
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.385176
  Std: 0.123052

Negative pair similarities:
  Mean: 0.400927
  Std: 0.123173

Separation (pos_mean - neg_mean): -0.015750
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [-0.18, 6.51]
  Negative similarities: [0.40, 7.08]
=== END DIAGNOSTIC ===

Batch 14000/18276: Loss = 0.5940 (C: 14.8160, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 14100/18276: Loss = 0.5823 (C: 14.5231, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 14200/18276: Loss = 0.5793 (C: 14.4464, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 14300/18276: Loss = 0.5795 (C: 14.4527, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 14400/18276: Loss = 0.5795 (C: 14.4525, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 14500/18276: Loss = 0.5790 (C: 14.4414, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 14600/18276: Loss = 0.5791 (C: 14.4424, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 14700/18276: Loss = 0.5786 (C: 14.4319, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 14800/18276: Loss = 0.5786 (C: 14.4304, R: 0.0013, KL: 0.0000) | β=0.040, γ=0.0000
Batch 14900/18276: Loss = 0.5789 (C: 14.4390, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
mu range: [-5.221289, 6.885271]
mu std: 2.201932
Batch 15000/18276: Loss = 0.5788 (C: 14.4348, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 15100/18276: Loss = 0.5784 (C: 14.4258, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 15200/18276: Loss = 0.5791 (C: 14.4437, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 15300/18276: Loss = 0.5792 (C: 14.4442, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 15400/18276: Loss = 0.5787 (C: 14.4334, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 15500/18276: Loss = 0.5785 (C: 14.4281, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 15600/18276: Loss = 0.5788 (C: 14.4357, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 15700/18276: Loss = 0.5791 (C: 14.4427, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 15800/18276: Loss = 0.5790 (C: 14.4396, R: 0.0015, KL: 0.0000) | β=0.040, γ=0.0000
Batch 15900/18276: Loss = 0.5788 (C: 14.4364, R: 0.0013, KL: 0.0000) | β=0.040, γ=0.0000
Batch 16000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.733671, 6.418059]
mu std: 2.030778

=== DIAGNOSTIC AT BATCH 16000 ===
Contrastive loss stuck at: 14.4332
Contrastive weight (β): 0.0400
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.877123
  Max: 1.000000
  Mean: 0.929466
  Std: 0.019348
Average feature variance: 0.000973
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.927130
  Std: 0.014339

Negative pair similarities:
  Mean: 0.926991
  Std: 0.014551

Separation (pos_mean - neg_mean): 0.000139
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [8.90, 9.52]
  Negative similarities: [8.77, 9.58]
=== END DIAGNOSTIC ===

Batch 16000/18276: Loss = 0.5787 (C: 14.4332, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 16100/18276: Loss = 0.5789 (C: 14.4376, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 16200/18276: Loss = 0.5784 (C: 14.4236, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 16300/18276: Loss = 0.5791 (C: 14.4414, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 16400/18276: Loss = 0.5789 (C: 14.4381, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 16500/18276: Loss = 0.5785 (C: 14.4276, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 16600/18276: Loss = 0.5789 (C: 14.4384, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 16700/18276: Loss = 0.5788 (C: 14.4350, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 16800/18276: Loss = 0.5793 (C: 14.4487, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 16900/18276: Loss = 0.5790 (C: 14.4420, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
mu range: [-4.426119, 6.014747]
mu std: 1.912013
Batch 17000/18276: Loss = 0.5792 (C: 14.4461, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 17100/18276: Loss = 0.5791 (C: 14.4412, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 17200/18276: Loss = 0.5786 (C: 14.4311, R: 0.0013, KL: 0.0000) | β=0.040, γ=0.0000
Batch 17300/18276: Loss = 0.5787 (C: 14.4317, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 17400/18276: Loss = 0.5789 (C: 14.4369, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 17500/18276: Loss = 0.5783 (C: 14.4238, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 17600/18276: Loss = 0.5790 (C: 14.4396, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 17700/18276: Loss = 0.5785 (C: 14.4283, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 17800/18276: Loss = 0.5787 (C: 14.4331, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 17900/18276: Loss = 0.5784 (C: 14.4268, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 18000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.326742, 5.871634]
mu std: 1.873543

=== DIAGNOSTIC AT BATCH 18000 ===
Contrastive loss stuck at: 14.4223
Contrastive weight (β): 0.0400
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.879892
  Max: 1.000000
  Mean: 0.930355
  Std: 0.019435
Average feature variance: 0.000961
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.928820
  Std: 0.015068

Negative pair similarities:
  Mean: 0.927563
  Std: 0.014599

Separation (pos_mean - neg_mean): 0.001257
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [8.80, 9.57]
  Negative similarities: [8.87, 9.58]
=== END DIAGNOSTIC ===

Batch 18000/18276: Loss = 0.5783 (C: 14.4223, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 18100/18276: Loss = 0.5793 (C: 14.4464, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
Batch 18200/18276: Loss = 0.5793 (C: 14.4477, R: 0.0014, KL: 0.0000) | β=0.040, γ=0.0000
DEBUG: validate_epoch called with β=0.040, γ=0.0000
DEBUG: First validation batch:
  contrastive_loss raw: 14.444317817687988
  reconstruction_loss raw: 0.0013803901383653283
  kl_loss raw: 0.0
  total_loss raw: 0.5791530609130859
Epoch 2 completed in 90.92s
Train Loss: 0.5881 (C: 14.6665, R: 0.0014) (β=0.040)
Val Loss: 0.5787 (C: 14.4337, R: 0.0014)
No improvement for 1 epochs

Epoch 3/50
------------------------------
Batch 0 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.570837, 6.218517]
mu std: 1.924577
Batch 0/18276: Loss = 1.1571 (C: 14.4458, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 100/18276: Loss = 1.1563 (C: 14.4360, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 200/18276: Loss = 1.1566 (C: 14.4399, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 300/18276: Loss = 1.1553 (C: 14.4234, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 400/18276: Loss = 1.1559 (C: 14.4316, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 500/18276: Loss = 1.1556 (C: 14.4280, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 600/18276: Loss = 1.1555 (C: 14.4263, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 700/18276: Loss = 1.1553 (C: 14.4238, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 800/18276: Loss = 1.1560 (C: 14.4331, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 900/18276: Loss = 1.1564 (C: 14.4377, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
mu range: [-4.921962, 6.699682]
mu std: 2.098146
Batch 1000/18276: Loss = 1.1559 (C: 14.4315, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 1100/18276: Loss = 1.1566 (C: 14.4403, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 1200/18276: Loss = 1.1560 (C: 14.4330, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 1300/18276: Loss = 1.1558 (C: 14.4311, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 1400/18276: Loss = 1.1564 (C: 14.4380, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 1500/18276: Loss = 1.1557 (C: 14.4285, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 1600/18276: Loss = 1.1564 (C: 14.4375, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 1700/18276: Loss = 1.1565 (C: 14.4382, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 1800/18276: Loss = 1.1563 (C: 14.4372, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 1900/18276: Loss = 1.1560 (C: 14.4323, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 2000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.908864, 6.678257]
mu std: 2.067934

=== DIAGNOSTIC AT BATCH 2000 ===
Contrastive loss stuck at: 14.4205
Contrastive weight (β): 0.0800
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.904076
  Max: 1.000000
  Mean: 0.953077
  Std: 0.015424
Average feature variance: 0.000647
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.952376
  Std: 0.013453

Negative pair similarities:
  Mean: 0.951046
  Std: 0.012695

Separation (pos_mean - neg_mean): 0.001331
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.19, 9.78]
  Negative similarities: [9.04, 9.72]
=== END DIAGNOSTIC ===

Batch 2000/18276: Loss = 1.1550 (C: 14.4205, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 2100/18276: Loss = 1.1545 (C: 14.4137, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 2200/18276: Loss = 1.1564 (C: 14.4378, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 2300/18276: Loss = 1.1557 (C: 14.4277, R: 0.0015, KL: 0.0000) | β=0.080, γ=0.0000
Batch 2400/18276: Loss = 1.1560 (C: 14.4325, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 2500/18276: Loss = 1.1566 (C: 14.4401, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 2600/18276: Loss = 1.1562 (C: 14.4357, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 2700/18276: Loss = 1.1562 (C: 14.4347, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 2800/18276: Loss = 1.1561 (C: 14.4336, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 2900/18276: Loss = 1.1559 (C: 14.4308, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
mu range: [-5.231152, 7.103524]
mu std: 2.185603
Batch 3000/18276: Loss = 1.1558 (C: 14.4299, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 3100/18276: Loss = 1.1560 (C: 14.4319, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 3200/18276: Loss = 1.1561 (C: 14.4336, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 3300/18276: Loss = 1.1564 (C: 14.4371, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 3400/18276: Loss = 1.1565 (C: 14.4392, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 3500/18276: Loss = 1.1560 (C: 14.4321, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 3600/18276: Loss = 1.1557 (C: 14.4287, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 3700/18276: Loss = 1.1554 (C: 14.4251, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 3800/18276: Loss = 1.1566 (C: 14.4403, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 3900/18276: Loss = 1.1561 (C: 14.4347, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 4000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.793172, 6.537744]
mu std: 1.966243

=== DIAGNOSTIC AT BATCH 4000 ===
Contrastive loss stuck at: 14.4349
Contrastive weight (β): 0.0800
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.903006
  Max: 1.000000
  Mean: 0.949886
  Std: 0.014926
Average feature variance: 0.000691
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.948054
  Std: 0.012012

Negative pair similarities:
  Mean: 0.948205
  Std: 0.011808

Separation (pos_mean - neg_mean): -0.000152
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.14, 9.69]
  Negative similarities: [9.03, 9.73]
=== END DIAGNOSTIC ===

Batch 4000/18276: Loss = 1.1561 (C: 14.4349, R: 0.0013, KL: 0.0000) | β=0.080, γ=0.0000
Batch 4100/18276: Loss = 1.1560 (C: 14.4327, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 4200/18276: Loss = 1.1560 (C: 14.4317, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 4300/18276: Loss = 1.1561 (C: 14.4348, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 4400/18276: Loss = 1.1559 (C: 14.4315, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 4500/18276: Loss = 1.1565 (C: 14.4392, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 4600/18276: Loss = 1.1561 (C: 14.4331, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 4700/18276: Loss = 1.1556 (C: 14.4281, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 4800/18276: Loss = 1.1565 (C: 14.4384, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 4900/18276: Loss = 1.1554 (C: 14.4251, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
mu range: [-4.531796, 6.248025]
mu std: 1.952868
Batch 5000/18276: Loss = 1.1563 (C: 14.4364, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 5100/18276: Loss = 1.1563 (C: 14.4365, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 5200/18276: Loss = 1.1559 (C: 14.4320, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 5300/18276: Loss = 1.1560 (C: 14.4331, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 5400/18276: Loss = 1.1559 (C: 14.4313, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 5500/18276: Loss = 1.1551 (C: 14.4214, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 5600/18276: Loss = 1.1558 (C: 14.4310, R: 0.0013, KL: 0.0000) | β=0.080, γ=0.0000
Batch 5700/18276: Loss = 1.1560 (C: 14.4321, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 5800/18276: Loss = 1.1558 (C: 14.4300, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 5900/18276: Loss = 1.1562 (C: 14.4356, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 6000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.970822, 6.777634]
mu std: 2.150856

=== DIAGNOSTIC AT BATCH 6000 ===
Contrastive loss stuck at: 14.4350
Contrastive weight (β): 0.0800
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.926432
  Max: 1.000000
  Mean: 0.963449
  Std: 0.010802
Average feature variance: 0.000504
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.962004
  Std: 0.008661

Negative pair similarities:
  Mean: 0.962272
  Std: 0.008496

Separation (pos_mean - neg_mean): -0.000268
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.26, 9.77]
  Negative similarities: [9.37, 9.79]
=== END DIAGNOSTIC ===

Batch 6000/18276: Loss = 1.1562 (C: 14.4350, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 6100/18276: Loss = 1.1560 (C: 14.4332, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 6200/18276: Loss = 1.1560 (C: 14.4331, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 6300/18276: Loss = 1.1556 (C: 14.4277, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 6400/18276: Loss = 1.1554 (C: 14.4252, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 6500/18276: Loss = 1.1563 (C: 14.4358, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 6600/18276: Loss = 1.1567 (C: 14.4410, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 6700/18276: Loss = 1.1560 (C: 14.4323, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 6800/18276: Loss = 1.1563 (C: 14.4365, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 6900/18276: Loss = 1.1557 (C: 14.4287, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
mu range: [-4.823558, 6.561876]
mu std: 2.031311
Batch 7000/18276: Loss = 1.1559 (C: 14.4320, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 7100/18276: Loss = 1.1557 (C: 14.4288, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 7200/18276: Loss = 1.1565 (C: 14.4385, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 7300/18276: Loss = 1.1560 (C: 14.4320, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 7400/18276: Loss = 1.1564 (C: 14.4381, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 7500/18276: Loss = 1.1562 (C: 14.4355, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 7600/18276: Loss = 1.1562 (C: 14.4357, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 7700/18276: Loss = 1.1555 (C: 14.4273, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 7800/18276: Loss = 1.1561 (C: 14.4328, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 7900/18276: Loss = 1.1562 (C: 14.4352, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 8000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.473283, 7.472579]
mu std: 2.189515

=== DIAGNOSTIC AT BATCH 8000 ===
Contrastive loss stuck at: 14.4301
Contrastive weight (β): 0.0800
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.936904
  Max: 1.000000
  Mean: 0.966714
  Std: 0.010058
Average feature variance: 0.000459
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.965716
  Std: 0.008087

Negative pair similarities:
  Mean: 0.965499
  Std: 0.008065

Separation (pos_mean - neg_mean): 0.000217
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.37, 9.84]
  Negative similarities: [9.44, 9.87]
=== END DIAGNOSTIC ===

Batch 8000/18276: Loss = 1.1558 (C: 14.4301, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 8100/18276: Loss = 1.1557 (C: 14.4289, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 8200/18276: Loss = 1.1556 (C: 14.4271, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 8300/18276: Loss = 1.1559 (C: 14.4315, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 8400/18276: Loss = 1.1563 (C: 14.4366, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 8500/18276: Loss = 1.1557 (C: 14.4289, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 8600/18276: Loss = 1.1559 (C: 14.4305, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 8700/18276: Loss = 1.1561 (C: 14.4338, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 8800/18276: Loss = 1.1566 (C: 14.4406, R: 0.0013, KL: 0.0000) | β=0.080, γ=0.0000
Batch 8900/18276: Loss = 1.1549 (C: 14.4189, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
mu range: [-4.858188, 6.630157]
mu std: 2.030409
Batch 9000/18276: Loss = 1.1558 (C: 14.4304, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 9100/18276: Loss = 1.1560 (C: 14.4320, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 9200/18276: Loss = 1.1562 (C: 14.4359, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 9300/18276: Loss = 1.1561 (C: 14.4346, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 9400/18276: Loss = 1.1558 (C: 14.4293, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 9500/18276: Loss = 1.1564 (C: 14.4374, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 9600/18276: Loss = 1.1561 (C: 14.4330, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 9700/18276: Loss = 1.1556 (C: 14.4270, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 9800/18276: Loss = 1.1560 (C: 14.4329, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 9900/18276: Loss = 1.1564 (C: 14.4373, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 10000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.178806, 7.018529]
mu std: 2.077438

=== DIAGNOSTIC AT BATCH 10000 ===
Contrastive loss stuck at: 14.4399
Contrastive weight (β): 0.0800
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.924546
  Max: 1.000000
  Mean: 0.960749
  Std: 0.012208
Average feature variance: 0.000541
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.958893
  Std: 0.010784

Negative pair similarities:
  Mean: 0.959621
  Std: 0.009564

Separation (pos_mean - neg_mean): -0.000728
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.30, 9.83]
  Negative similarities: [9.25, 9.80]
=== END DIAGNOSTIC ===

Batch 10000/18276: Loss = 1.1566 (C: 14.4399, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 10100/18276: Loss = 1.1563 (C: 14.4367, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 10200/18276: Loss = 1.1564 (C: 14.4378, R: 0.0013, KL: 0.0000) | β=0.080, γ=0.0000
Batch 10300/18276: Loss = 1.1559 (C: 14.4311, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 10400/18276: Loss = 1.1560 (C: 14.4322, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 10500/18276: Loss = 1.1560 (C: 14.4325, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 10600/18276: Loss = 1.1563 (C: 14.4362, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 10700/18276: Loss = 1.1558 (C: 14.4302, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 10800/18276: Loss = 1.1555 (C: 14.4257, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 10900/18276: Loss = 1.1560 (C: 14.4322, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
mu range: [-4.899154, 6.624444]
mu std: 2.126448
Batch 11000/18276: Loss = 1.1562 (C: 14.4350, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 11100/18276: Loss = 1.1564 (C: 14.4385, R: 0.0013, KL: 0.0000) | β=0.080, γ=0.0000
Batch 11200/18276: Loss = 1.1561 (C: 14.4341, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 11300/18276: Loss = 1.1568 (C: 14.4422, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 11400/18276: Loss = 1.1563 (C: 14.4363, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 11500/18276: Loss = 1.1558 (C: 14.4302, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 11600/18276: Loss = 1.1554 (C: 14.4247, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 11700/18276: Loss = 1.1559 (C: 14.4305, R: 0.0015, KL: 0.0000) | β=0.080, γ=0.0000
Batch 11800/18276: Loss = 1.1557 (C: 14.4281, R: 0.0015, KL: 0.0000) | β=0.080, γ=0.0000
Batch 11900/18276: Loss = 1.1560 (C: 14.4327, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 12000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.219543, 7.090698]
mu std: 2.150847

=== DIAGNOSTIC AT BATCH 12000 ===
Contrastive loss stuck at: 14.4335
Contrastive weight (β): 0.0800
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.937297
  Max: 1.000000
  Mean: 0.965509
  Std: 0.011217
Average feature variance: 0.000476
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.964252
  Std: 0.009947

Negative pair similarities:
  Mean: 0.964350
  Std: 0.009098

Separation (pos_mean - neg_mean): -0.000098
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.44, 9.84]
  Negative similarities: [9.37, 9.83]
=== END DIAGNOSTIC ===

Batch 12000/18276: Loss = 1.1561 (C: 14.4335, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 12100/18276: Loss = 1.1557 (C: 14.4290, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 12200/18276: Loss = 1.1555 (C: 14.4264, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 12300/18276: Loss = 1.1557 (C: 14.4279, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 12400/18276: Loss = 1.1556 (C: 14.4282, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 12500/18276: Loss = 1.1564 (C: 14.4373, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 12600/18276: Loss = 1.1560 (C: 14.4331, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 12700/18276: Loss = 1.1562 (C: 14.4349, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 12800/18276: Loss = 1.1557 (C: 14.4291, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 12900/18276: Loss = 1.1564 (C: 14.4379, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
mu range: [-4.718892, 6.448528]
mu std: 2.020494
Batch 13000/18276: Loss = 1.1562 (C: 14.4347, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 13100/18276: Loss = 1.1557 (C: 14.4286, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 13200/18276: Loss = 1.1563 (C: 14.4365, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 13300/18276: Loss = 1.1568 (C: 14.4424, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 13400/18276: Loss = 1.1553 (C: 14.4237, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 13500/18276: Loss = 1.1559 (C: 14.4320, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 13600/18276: Loss = 1.1562 (C: 14.4351, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 13700/18276: Loss = 1.1561 (C: 14.4339, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 13800/18276: Loss = 1.1555 (C: 14.4265, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 13900/18276: Loss = 1.1564 (C: 14.4373, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 14000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.974536, 6.833899]
mu std: 2.158389

=== DIAGNOSTIC AT BATCH 14000 ===
Contrastive loss stuck at: 14.4321
Contrastive weight (β): 0.0800
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.929078
  Max: 1.000000
  Mean: 0.966329
  Std: 0.010178
Average feature variance: 0.000464
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.965178
  Std: 0.008383

Negative pair similarities:
  Mean: 0.965164
  Std: 0.008074

Separation (pos_mean - neg_mean): 0.000014
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.40, 9.82]
  Negative similarities: [9.29, 9.80]
=== END DIAGNOSTIC ===

Batch 14000/18276: Loss = 1.1560 (C: 14.4321, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 14100/18276: Loss = 1.1556 (C: 14.4279, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 14200/18276: Loss = 1.1565 (C: 14.4393, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 14300/18276: Loss = 1.1558 (C: 14.4310, R: 0.0013, KL: 0.0000) | β=0.080, γ=0.0000
Batch 14400/18276: Loss = 1.1559 (C: 14.4313, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 14500/18276: Loss = 1.1557 (C: 14.4287, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 14600/18276: Loss = 1.1557 (C: 14.4289, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 14700/18276: Loss = 1.1552 (C: 14.4224, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 14800/18276: Loss = 1.1564 (C: 14.4373, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 14900/18276: Loss = 1.1556 (C: 14.4274, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
mu range: [-4.837707, 6.517602]
mu std: 2.031736
Batch 15000/18276: Loss = 1.1560 (C: 14.4320, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 15100/18276: Loss = 1.1558 (C: 14.4306, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 15200/18276: Loss = 1.1552 (C: 14.4225, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 15300/18276: Loss = 1.1561 (C: 14.4347, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 15400/18276: Loss = 1.1561 (C: 14.4347, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 15500/18276: Loss = 1.1568 (C: 14.4418, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 15600/18276: Loss = 1.1565 (C: 14.4394, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 15700/18276: Loss = 1.1563 (C: 14.4361, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 15800/18276: Loss = 1.1553 (C: 14.4234, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 15900/18276: Loss = 1.1559 (C: 14.4316, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 16000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-3.957383, 5.490413]
mu std: 1.769365

=== DIAGNOSTIC AT BATCH 16000 ===
Contrastive loss stuck at: 14.4346
Contrastive weight (β): 0.0800
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.875902
  Max: 1.000000
  Mean: 0.931254
  Std: 0.019616
Average feature variance: 0.000948
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.928900
  Std: 0.015212

Negative pair similarities:
  Mean: 0.928877
  Std: 0.015124

Separation (pos_mean - neg_mean): 0.000023
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [8.84, 9.62]
  Negative similarities: [8.76, 9.60]
=== END DIAGNOSTIC ===

Batch 16000/18276: Loss = 1.1562 (C: 14.4346, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 16100/18276: Loss = 1.1558 (C: 14.4303, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 16200/18276: Loss = 1.1560 (C: 14.4318, R: 0.0015, KL: 0.0000) | β=0.080, γ=0.0000
Batch 16300/18276: Loss = 1.1560 (C: 14.4319, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 16400/18276: Loss = 1.1556 (C: 14.4279, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 16500/18276: Loss = 1.1566 (C: 14.4398, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 16600/18276: Loss = 1.1560 (C: 14.4330, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 16700/18276: Loss = 1.1563 (C: 14.4367, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 16800/18276: Loss = 1.1558 (C: 14.4305, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 16900/18276: Loss = 1.1567 (C: 14.4405, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
mu range: [-4.895483, 6.801355]
mu std: 2.044925
Batch 17000/18276: Loss = 1.1557 (C: 14.4292, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 17100/18276: Loss = 1.1565 (C: 14.4391, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 17200/18276: Loss = 1.1558 (C: 14.4306, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 17300/18276: Loss = 1.1561 (C: 14.4353, R: 0.0013, KL: 0.0000) | β=0.080, γ=0.0000
Batch 17400/18276: Loss = 1.1559 (C: 14.4315, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 17500/18276: Loss = 1.1563 (C: 14.4365, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 17600/18276: Loss = 1.1559 (C: 14.4309, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 17700/18276: Loss = 1.1564 (C: 14.4371, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 17800/18276: Loss = 1.1553 (C: 14.4233, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 17900/18276: Loss = 1.1563 (C: 14.4367, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 18000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.070724, 7.103686]
mu std: 2.083410

=== DIAGNOSTIC AT BATCH 18000 ===
Contrastive loss stuck at: 14.4448
Contrastive weight (β): 0.0800
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.931739
  Max: 1.000000
  Mean: 0.959543
  Std: 0.012192
Average feature variance: 0.000558
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.957300
  Std: 0.010969

Negative pair similarities:
  Mean: 0.958529
  Std: 0.009154

Separation (pos_mean - neg_mean): -0.001229
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.32, 9.76]
  Negative similarities: [9.33, 9.82]
=== END DIAGNOSTIC ===

Batch 18000/18276: Loss = 1.1570 (C: 14.4448, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 18100/18276: Loss = 1.1564 (C: 14.4381, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
Batch 18200/18276: Loss = 1.1559 (C: 14.4311, R: 0.0014, KL: 0.0000) | β=0.080, γ=0.0000
DEBUG: validate_epoch called with β=0.080, γ=0.0000
DEBUG: First validation batch:
  contrastive_loss raw: 14.431002616882324
  reconstruction_loss raw: 0.0013948985142633319
  kl_loss raw: 0.0
  total_loss raw: 1.1558750867843628
Epoch 3 completed in 91.63s
Train Loss: 1.1560 (C: 14.4328, R: 0.0014) (β=0.080)
Val Loss: 1.1560 (C: 14.4323, R: 0.0014)
No improvement for 2 epochs

Epoch 4/50
------------------------------
Batch 0 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.892752, 6.762696]
mu std: 2.057035
Batch 0/18276: Loss = 1.7329 (C: 14.4292, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 100/18276: Loss = 1.7329 (C: 14.4293, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 200/18276: Loss = 1.7342 (C: 14.4396, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 300/18276: Loss = 1.7327 (C: 14.4282, R: 0.0013, KL: 0.0000) | β=0.120, γ=0.0000
Batch 400/18276: Loss = 1.7328 (C: 14.4286, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 500/18276: Loss = 1.7335 (C: 14.4341, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 600/18276: Loss = 1.7331 (C: 14.4308, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 700/18276: Loss = 1.7334 (C: 14.4333, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 800/18276: Loss = 1.7334 (C: 14.4334, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 900/18276: Loss = 1.7339 (C: 14.4377, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
mu range: [-4.819682, 6.682891]
mu std: 2.115086
Batch 1000/18276: Loss = 1.7327 (C: 14.4274, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 1100/18276: Loss = 1.7329 (C: 14.4288, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 1200/18276: Loss = 1.7336 (C: 14.4353, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 1300/18276: Loss = 1.7322 (C: 14.4233, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 1400/18276: Loss = 1.7348 (C: 14.4454, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 1500/18276: Loss = 1.7334 (C: 14.4338, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 1600/18276: Loss = 1.7331 (C: 14.4305, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 1700/18276: Loss = 1.7332 (C: 14.4314, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 1800/18276: Loss = 1.7339 (C: 14.4373, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 1900/18276: Loss = 1.7334 (C: 14.4333, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 2000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.384975, 7.431928]
mu std: 2.269485

=== DIAGNOSTIC AT BATCH 2000 ===
Contrastive loss stuck at: 14.4272
Contrastive weight (β): 0.1200
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.947082
  Max: 1.000000
  Mean: 0.972574
  Std: 0.009331
Average feature variance: 0.000378
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.971974
  Std: 0.008829

Negative pair similarities:
  Mean: 0.971472
  Std: 0.007525

Separation (pos_mean - neg_mean): 0.000502
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.52, 9.86]
  Negative similarities: [9.47, 9.89]
=== END DIAGNOSTIC ===

Batch 2000/18276: Loss = 1.7326 (C: 14.4272, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 2100/18276: Loss = 1.7333 (C: 14.4326, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 2200/18276: Loss = 1.7338 (C: 14.4367, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 2300/18276: Loss = 1.7335 (C: 14.4341, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 2400/18276: Loss = 1.7333 (C: 14.4332, R: 0.0013, KL: 0.0000) | β=0.120, γ=0.0000
Batch 2500/18276: Loss = 1.7331 (C: 14.4309, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 2600/18276: Loss = 1.7339 (C: 14.4371, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 2700/18276: Loss = 1.7335 (C: 14.4340, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 2800/18276: Loss = 1.7323 (C: 14.4248, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 2900/18276: Loss = 1.7329 (C: 14.4291, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
mu range: [-5.169549, 7.138527]
mu std: 2.233748
Batch 3000/18276: Loss = 1.7340 (C: 14.4388, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 3100/18276: Loss = 1.7339 (C: 14.4374, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 3200/18276: Loss = 1.7324 (C: 14.4256, R: 0.0013, KL: 0.0000) | β=0.120, γ=0.0000
Batch 3300/18276: Loss = 1.7333 (C: 14.4330, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 3400/18276: Loss = 1.7331 (C: 14.4313, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 3500/18276: Loss = 1.7334 (C: 14.4334, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 3600/18276: Loss = 1.7336 (C: 14.4354, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 3700/18276: Loss = 1.7326 (C: 14.4273, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 3800/18276: Loss = 1.7338 (C: 14.4372, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 3900/18276: Loss = 1.7326 (C: 14.4267, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 4000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.858149, 6.749012]
mu std: 2.127789

=== DIAGNOSTIC AT BATCH 4000 ===
Contrastive loss stuck at: 14.4284
Contrastive weight (β): 0.1200
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.940241
  Max: 1.000000
  Mean: 0.964333
  Std: 0.009986
Average feature variance: 0.000492
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.963367
  Std: 0.007313

Negative pair similarities:
  Mean: 0.962984
  Std: 0.007726

Separation (pos_mean - neg_mean): 0.000383
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.43, 9.79]
  Negative similarities: [9.40, 9.79]
=== END DIAGNOSTIC ===

Batch 4000/18276: Loss = 1.7328 (C: 14.4284, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 4100/18276: Loss = 1.7328 (C: 14.4284, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 4200/18276: Loss = 1.7338 (C: 14.4367, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 4300/18276: Loss = 1.7338 (C: 14.4369, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 4400/18276: Loss = 1.7329 (C: 14.4291, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 4500/18276: Loss = 1.7335 (C: 14.4344, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 4600/18276: Loss = 1.7331 (C: 14.4310, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 4700/18276: Loss = 1.7332 (C: 14.4321, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 4800/18276: Loss = 1.7338 (C: 14.4368, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 4900/18276: Loss = 1.7333 (C: 14.4320, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
mu range: [-5.309498, 7.460047]
mu std: 2.261879
Batch 5000/18276: Loss = 1.7333 (C: 14.4322, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 5100/18276: Loss = 1.7335 (C: 14.4345, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 5200/18276: Loss = 1.7328 (C: 14.4281, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 5300/18276: Loss = 1.7335 (C: 14.4344, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 5400/18276: Loss = 1.7340 (C: 14.4388, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 5500/18276: Loss = 1.7331 (C: 14.4311, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 5600/18276: Loss = 1.7336 (C: 14.4357, R: 0.0013, KL: 0.0000) | β=0.120, γ=0.0000
Batch 5700/18276: Loss = 1.7327 (C: 14.4278, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 5800/18276: Loss = 1.7333 (C: 14.4328, R: 0.0013, KL: 0.0000) | β=0.120, γ=0.0000
Batch 5900/18276: Loss = 1.7328 (C: 14.4286, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 6000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.577672, 6.399201]
mu std: 2.034987

=== DIAGNOSTIC AT BATCH 6000 ===
Contrastive loss stuck at: 14.4341
Contrastive weight (β): 0.1200
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.930351
  Max: 1.000000
  Mean: 0.960008
  Std: 0.011759
Average feature variance: 0.000552
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.958523
  Std: 0.009423

Negative pair similarities:
  Mean: 0.958677
  Std: 0.009208

Separation (pos_mean - neg_mean): -0.000155
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.30, 9.77]
  Negative similarities: [9.34, 9.79]
=== END DIAGNOSTIC ===

Batch 6000/18276: Loss = 1.7335 (C: 14.4341, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 6100/18276: Loss = 1.7341 (C: 14.4398, R: 0.0013, KL: 0.0000) | β=0.120, γ=0.0000
Batch 6200/18276: Loss = 1.7333 (C: 14.4329, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 6300/18276: Loss = 1.7334 (C: 14.4338, R: 0.0013, KL: 0.0000) | β=0.120, γ=0.0000
Batch 6400/18276: Loss = 1.7338 (C: 14.4369, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 6500/18276: Loss = 1.7322 (C: 14.4231, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 6600/18276: Loss = 1.7333 (C: 14.4320, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 6700/18276: Loss = 1.7324 (C: 14.4252, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 6800/18276: Loss = 1.7324 (C: 14.4246, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 6900/18276: Loss = 1.7325 (C: 14.4260, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
mu range: [-4.438069, 6.169148]
mu std: 1.974067
Batch 7000/18276: Loss = 1.7328 (C: 14.4286, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 7100/18276: Loss = 1.7329 (C: 14.4290, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 7200/18276: Loss = 1.7321 (C: 14.4223, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 7300/18276: Loss = 1.7326 (C: 14.4271, R: 0.0013, KL: 0.0000) | β=0.120, γ=0.0000
Batch 7400/18276: Loss = 1.7334 (C: 14.4340, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 7500/18276: Loss = 1.7342 (C: 14.4397, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 7600/18276: Loss = 1.7334 (C: 14.4339, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 7700/18276: Loss = 1.7333 (C: 14.4325, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 7800/18276: Loss = 1.7331 (C: 14.4305, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 7900/18276: Loss = 1.7329 (C: 14.4294, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 8000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.987404, 6.955892]
mu std: 2.116862

=== DIAGNOSTIC AT BATCH 8000 ===
Contrastive loss stuck at: 14.4386
Contrastive weight (β): 0.1200
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.926484
  Max: 1.000000
  Mean: 0.965135
  Std: 0.011164
Average feature variance: 0.000481
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.963508
  Std: 0.010273

Negative pair similarities:
  Mean: 0.964124
  Std: 0.008749

Separation (pos_mean - neg_mean): -0.000616
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.38, 9.85]
  Negative similarities: [9.26, 9.81]
=== END DIAGNOSTIC ===

Batch 8000/18276: Loss = 1.7340 (C: 14.4386, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 8100/18276: Loss = 1.7329 (C: 14.4292, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 8200/18276: Loss = 1.7341 (C: 14.4389, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 8300/18276: Loss = 1.7330 (C: 14.4301, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 8400/18276: Loss = 1.7334 (C: 14.4336, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 8500/18276: Loss = 1.7335 (C: 14.4343, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 8600/18276: Loss = 1.7336 (C: 14.4344, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 8700/18276: Loss = 1.7334 (C: 14.4333, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 8800/18276: Loss = 1.7338 (C: 14.4361, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 8900/18276: Loss = 1.7332 (C: 14.4320, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
mu range: [-4.517338, 6.337866]
mu std: 2.019520
Batch 9000/18276: Loss = 1.7337 (C: 14.4361, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 9100/18276: Loss = 1.7338 (C: 14.4367, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 9200/18276: Loss = 1.7334 (C: 14.4334, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 9300/18276: Loss = 1.7331 (C: 14.4307, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 9400/18276: Loss = 1.7338 (C: 14.4366, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 9500/18276: Loss = 1.7339 (C: 14.4377, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 9600/18276: Loss = 1.7332 (C: 14.4319, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 9700/18276: Loss = 1.7325 (C: 14.4262, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 9800/18276: Loss = 1.7333 (C: 14.4325, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 9900/18276: Loss = 1.7327 (C: 14.4274, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 10000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.038267, 7.080246]
mu std: 2.170427

=== DIAGNOSTIC AT BATCH 10000 ===
Contrastive loss stuck at: 14.4313
Contrastive weight (β): 0.1200
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.935670
  Max: 1.000000
  Mean: 0.967368
  Std: 0.010592
Average feature variance: 0.000450
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.966316
  Std: 0.009553

Negative pair similarities:
  Mean: 0.966209
  Std: 0.008498

Separation (pos_mean - neg_mean): 0.000107
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.36, 9.84]
  Negative similarities: [9.42, 9.85]
=== END DIAGNOSTIC ===

Batch 10000/18276: Loss = 1.7332 (C: 14.4313, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 10100/18276: Loss = 1.7335 (C: 14.4347, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 10200/18276: Loss = 1.7338 (C: 14.4370, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 10300/18276: Loss = 1.7334 (C: 14.4332, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 10400/18276: Loss = 1.7340 (C: 14.4381, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 10500/18276: Loss = 1.7331 (C: 14.4307, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 10600/18276: Loss = 1.7343 (C: 14.4403, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 10700/18276: Loss = 1.7330 (C: 14.4294, R: 0.0015, KL: 0.0000) | β=0.120, γ=0.0000
Batch 10800/18276: Loss = 1.7332 (C: 14.4315, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 10900/18276: Loss = 1.7329 (C: 14.4291, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
mu range: [-4.983788, 6.978830]
mu std: 2.118508
Batch 11000/18276: Loss = 1.7329 (C: 14.4293, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 11100/18276: Loss = 1.7328 (C: 14.4277, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 11200/18276: Loss = 1.7336 (C: 14.4355, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 11300/18276: Loss = 1.7336 (C: 14.4349, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 11400/18276: Loss = 1.7335 (C: 14.4342, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 11500/18276: Loss = 1.7341 (C: 14.4389, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 11600/18276: Loss = 1.7334 (C: 14.4334, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 11700/18276: Loss = 1.7324 (C: 14.4251, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 11800/18276: Loss = 1.7337 (C: 14.4358, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 11900/18276: Loss = 1.7329 (C: 14.4293, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 12000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.110843, 7.237987]
mu std: 2.193552

=== DIAGNOSTIC AT BATCH 12000 ===
Contrastive loss stuck at: 14.4325
Contrastive weight (β): 0.1200
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.936648
  Max: 1.000000
  Mean: 0.969621
  Std: 0.010594
Average feature variance: 0.000419
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.968571
  Std: 0.009491

Negative pair similarities:
  Mean: 0.968575
  Std: 0.008954

Separation (pos_mean - neg_mean): -0.000004
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.37, 9.81]
  Negative similarities: [9.39, 9.85]
=== END DIAGNOSTIC ===

Batch 12000/18276: Loss = 1.7333 (C: 14.4325, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 12100/18276: Loss = 1.7344 (C: 14.4416, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 12200/18276: Loss = 1.7338 (C: 14.4364, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 12300/18276: Loss = 1.7330 (C: 14.4297, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 12400/18276: Loss = 1.7335 (C: 14.4341, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 12500/18276: Loss = 1.7335 (C: 14.4350, R: 0.0013, KL: 0.0000) | β=0.120, γ=0.0000
Batch 12600/18276: Loss = 1.7338 (C: 14.4366, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 12700/18276: Loss = 1.7336 (C: 14.4354, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 12800/18276: Loss = 1.7330 (C: 14.4305, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 12900/18276: Loss = 1.7331 (C: 14.4313, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
mu range: [-5.171564, 7.200744]
mu std: 2.223029
Batch 13000/18276: Loss = 1.7327 (C: 14.4274, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 13100/18276: Loss = 1.7335 (C: 14.4340, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 13200/18276: Loss = 1.7340 (C: 14.4379, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 13300/18276: Loss = 1.7334 (C: 14.4328, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 13400/18276: Loss = 1.7337 (C: 14.4355, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 13500/18276: Loss = 1.7328 (C: 14.4282, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 13600/18276: Loss = 1.7334 (C: 14.4336, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 13700/18276: Loss = 1.7328 (C: 14.4282, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 13800/18276: Loss = 1.7335 (C: 14.4342, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 13900/18276: Loss = 1.7329 (C: 14.4295, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 14000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.896784, 6.797670]
mu std: 2.095367

=== DIAGNOSTIC AT BATCH 14000 ===
Contrastive loss stuck at: 14.4363
Contrastive weight (β): 0.1200
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.933434
  Max: 1.000000
  Mean: 0.964428
  Std: 0.011041
Average feature variance: 0.000491
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.962934
  Std: 0.009273

Negative pair similarities:
  Mean: 0.963322
  Std: 0.008874

Separation (pos_mean - neg_mean): -0.000388
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.33, 9.83]
  Negative similarities: [9.36, 9.84]
=== END DIAGNOSTIC ===

Batch 14000/18276: Loss = 1.7338 (C: 14.4363, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 14100/18276: Loss = 1.7327 (C: 14.4277, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 14200/18276: Loss = 1.7337 (C: 14.4360, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 14300/18276: Loss = 1.7332 (C: 14.4314, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 14400/18276: Loss = 1.7328 (C: 14.4285, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 14500/18276: Loss = 1.7334 (C: 14.4337, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 14600/18276: Loss = 1.7337 (C: 14.4362, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 14700/18276: Loss = 1.7335 (C: 14.4339, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 14800/18276: Loss = 1.7329 (C: 14.4295, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 14900/18276: Loss = 1.7339 (C: 14.4373, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
mu range: [-4.775590, 6.642449]
mu std: 2.145754
Batch 15000/18276: Loss = 1.7331 (C: 14.4310, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 15100/18276: Loss = 1.7333 (C: 14.4326, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 15200/18276: Loss = 1.7331 (C: 14.4310, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 15300/18276: Loss = 1.7329 (C: 14.4290, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 15400/18276: Loss = 1.7326 (C: 14.4265, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 15500/18276: Loss = 1.7330 (C: 14.4299, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 15600/18276: Loss = 1.7334 (C: 14.4341, R: 0.0013, KL: 0.0000) | β=0.120, γ=0.0000
Batch 15700/18276: Loss = 1.7325 (C: 14.4259, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 15800/18276: Loss = 1.7328 (C: 14.4284, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 15900/18276: Loss = 1.7333 (C: 14.4327, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 16000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.169971, 5.768210]
mu std: 1.847777

=== DIAGNOSTIC AT BATCH 16000 ===
Contrastive loss stuck at: 14.4429
Contrastive weight (β): 0.1200
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.903498
  Max: 1.000000
  Mean: 0.943742
  Std: 0.015144
Average feature variance: 0.000776
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.941131
  Std: 0.011047

Negative pair similarities:
  Mean: 0.942104
  Std: 0.011186

Separation (pos_mean - neg_mean): -0.000973
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.10, 9.64]
  Negative similarities: [9.03, 9.68]
=== END DIAGNOSTIC ===

Batch 16000/18276: Loss = 1.7346 (C: 14.4429, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 16100/18276: Loss = 1.7333 (C: 14.4328, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 16200/18276: Loss = 1.7336 (C: 14.4353, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 16300/18276: Loss = 1.7335 (C: 14.4338, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 16400/18276: Loss = 1.7325 (C: 14.4257, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 16500/18276: Loss = 1.7334 (C: 14.4333, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 16600/18276: Loss = 1.7324 (C: 14.4253, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 16700/18276: Loss = 1.7333 (C: 14.4331, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 16800/18276: Loss = 1.7333 (C: 14.4319, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 16900/18276: Loss = 1.7331 (C: 14.4311, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
mu range: [-5.485953, 7.482212]
mu std: 2.395499
Batch 17000/18276: Loss = 1.7332 (C: 14.4316, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 17100/18276: Loss = 1.7328 (C: 14.4284, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 17200/18276: Loss = 1.7337 (C: 14.4360, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 17300/18276: Loss = 1.7325 (C: 14.4262, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 17400/18276: Loss = 1.7329 (C: 14.4292, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 17500/18276: Loss = 1.7327 (C: 14.4281, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 17600/18276: Loss = 1.7332 (C: 14.4319, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 17700/18276: Loss = 1.7338 (C: 14.4363, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 17800/18276: Loss = 1.7328 (C: 14.4282, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 17900/18276: Loss = 1.7331 (C: 14.4311, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 18000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.849712, 6.766823]
mu std: 2.166026

=== DIAGNOSTIC AT BATCH 18000 ===
Contrastive loss stuck at: 14.4308
Contrastive weight (β): 0.1200
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.926488
  Max: 1.000000
  Mean: 0.967679
  Std: 0.011560
Average feature variance: 0.000446
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.966692
  Std: 0.010934

Negative pair similarities:
  Mean: 0.966506
  Std: 0.009630

Separation (pos_mean - neg_mean): 0.000186
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.33, 9.87]
  Negative similarities: [9.26, 9.84]
=== END DIAGNOSTIC ===

Batch 18000/18276: Loss = 1.7331 (C: 14.4308, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 18100/18276: Loss = 1.7335 (C: 14.4341, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
Batch 18200/18276: Loss = 1.7334 (C: 14.4336, R: 0.0014, KL: 0.0000) | β=0.120, γ=0.0000
DEBUG: validate_epoch called with β=0.120, γ=0.0000
DEBUG: First validation batch:
  contrastive_loss raw: 14.424086570739746
  reconstruction_loss raw: 0.0013923338847234845
  kl_loss raw: 0.0
  total_loss raw: 1.7322827577590942
Epoch 4 completed in 92.07s
Train Loss: 1.7333 (C: 14.4323, R: 0.0014) (β=0.120)
Val Loss: 1.7332 (C: 14.4317, R: 0.0014)
No improvement for 3 epochs

Epoch 5/50
------------------------------
Batch 0 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.270375, 7.285020]
mu std: 2.196534
Batch 0/18276: Loss = 2.3100 (C: 14.4288, R: 0.0013, KL: 0.0000) | β=0.160, γ=0.0000
Batch 100/18276: Loss = 2.3098 (C: 14.4275, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 200/18276: Loss = 2.3100 (C: 14.4286, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 300/18276: Loss = 2.3097 (C: 14.4267, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 400/18276: Loss = 2.3099 (C: 14.4282, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 500/18276: Loss = 2.3109 (C: 14.4344, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 600/18276: Loss = 2.3097 (C: 14.4272, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 700/18276: Loss = 2.3108 (C: 14.4341, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 800/18276: Loss = 2.3105 (C: 14.4324, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 900/18276: Loss = 2.3116 (C: 14.4386, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
mu range: [-4.447658, 6.138847]
mu std: 1.961851
Batch 1000/18276: Loss = 2.3099 (C: 14.4279, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 1100/18276: Loss = 2.3105 (C: 14.4317, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 1200/18276: Loss = 2.3107 (C: 14.4331, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 1300/18276: Loss = 2.3101 (C: 14.4292, R: 0.0015, KL: 0.0000) | β=0.160, γ=0.0000
Batch 1400/18276: Loss = 2.3114 (C: 14.4380, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 1500/18276: Loss = 2.3108 (C: 14.4331, R: 0.0015, KL: 0.0000) | β=0.160, γ=0.0000
Batch 1600/18276: Loss = 2.3118 (C: 14.4403, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 1700/18276: Loss = 2.3107 (C: 14.4327, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 1800/18276: Loss = 2.3111 (C: 14.4356, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 1900/18276: Loss = 2.3107 (C: 14.4329, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 2000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.172246, 7.201834]
mu std: 2.216624

=== DIAGNOSTIC AT BATCH 2000 ===
Contrastive loss stuck at: 14.4352
Contrastive weight (β): 0.1600
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.943402
  Max: 1.000000
  Mean: 0.971039
  Std: 0.009011
Average feature variance: 0.000399
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.969822
  Std: 0.008606

Negative pair similarities:
  Mean: 0.970138
  Std: 0.006716

Separation (pos_mean - neg_mean): -0.000317
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.43, 9.86]
  Negative similarities: [9.52, 9.84]
=== END DIAGNOSTIC ===

Batch 2000/18276: Loss = 2.3111 (C: 14.4352, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 2100/18276: Loss = 2.3096 (C: 14.4262, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 2200/18276: Loss = 2.3107 (C: 14.4336, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 2300/18276: Loss = 2.3116 (C: 14.4387, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 2400/18276: Loss = 2.3105 (C: 14.4317, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 2500/18276: Loss = 2.3104 (C: 14.4310, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 2600/18276: Loss = 2.3103 (C: 14.4307, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 2700/18276: Loss = 2.3101 (C: 14.4294, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 2800/18276: Loss = 2.3108 (C: 14.4338, R: 0.0013, KL: 0.0000) | β=0.160, γ=0.0000
Batch 2900/18276: Loss = 2.3109 (C: 14.4344, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
mu range: [-5.282849, 7.396075]
mu std: 2.288848
Batch 3000/18276: Loss = 2.3105 (C: 14.4319, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 3100/18276: Loss = 2.3093 (C: 14.4247, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 3200/18276: Loss = 2.3101 (C: 14.4297, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 3300/18276: Loss = 2.3103 (C: 14.4306, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 3400/18276: Loss = 2.3104 (C: 14.4319, R: 0.0013, KL: 0.0000) | β=0.160, γ=0.0000
Batch 3500/18276: Loss = 2.3098 (C: 14.4275, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 3600/18276: Loss = 2.3103 (C: 14.4305, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 3700/18276: Loss = 2.3100 (C: 14.4288, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 3800/18276: Loss = 2.3107 (C: 14.4325, R: 0.0015, KL: 0.0000) | β=0.160, γ=0.0000
Batch 3900/18276: Loss = 2.3102 (C: 14.4303, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 4000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.925309, 6.739120]
mu std: 2.166686

=== DIAGNOSTIC AT BATCH 4000 ===
Contrastive loss stuck at: 14.4324
Contrastive weight (β): 0.1600
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.942113
  Max: 1.000000
  Mean: 0.969962
  Std: 0.009140
Average feature variance: 0.000414
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.968908
  Std: 0.007738

Negative pair similarities:
  Mean: 0.968934
  Std: 0.007193

Separation (pos_mean - neg_mean): -0.000026
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.42, 9.84]
  Negative similarities: [9.42, 9.83]
=== END DIAGNOSTIC ===

Batch 4000/18276: Loss = 2.3106 (C: 14.4324, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 4100/18276: Loss = 2.3104 (C: 14.4311, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 4200/18276: Loss = 2.3106 (C: 14.4327, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 4300/18276: Loss = 2.3114 (C: 14.4376, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 4400/18276: Loss = 2.3095 (C: 14.4258, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 4500/18276: Loss = 2.3099 (C: 14.4279, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 4600/18276: Loss = 2.3110 (C: 14.4351, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 4700/18276: Loss = 2.3114 (C: 14.4377, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 4800/18276: Loss = 2.3108 (C: 14.4337, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 4900/18276: Loss = 2.3104 (C: 14.4315, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
mu range: [-5.311056, 7.344561]
mu std: 2.222496
Batch 5000/18276: Loss = 2.3091 (C: 14.4233, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 5100/18276: Loss = 2.3104 (C: 14.4312, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 5200/18276: Loss = 2.3108 (C: 14.4341, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 5300/18276: Loss = 2.3103 (C: 14.4305, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 5400/18276: Loss = 2.3106 (C: 14.4324, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 5500/18276: Loss = 2.3108 (C: 14.4337, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 5600/18276: Loss = 2.3114 (C: 14.4376, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 5700/18276: Loss = 2.3105 (C: 14.4317, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 5800/18276: Loss = 2.3094 (C: 14.4249, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 5900/18276: Loss = 2.3109 (C: 14.4346, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 6000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.452391, 7.335927]
mu std: 2.305510

=== DIAGNOSTIC AT BATCH 6000 ===
Contrastive loss stuck at: 14.4339
Contrastive weight (β): 0.1600
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.957801
  Max: 1.000000
  Mean: 0.976176
  Std: 0.007511
Average feature variance: 0.000329
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.975210
  Std: 0.006471

Negative pair similarities:
  Mean: 0.975420
  Std: 0.006037

Separation (pos_mean - neg_mean): -0.000210
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.58, 9.87]
  Negative similarities: [9.58, 9.91]
=== END DIAGNOSTIC ===

Batch 6000/18276: Loss = 2.3108 (C: 14.4339, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 6100/18276: Loss = 2.3107 (C: 14.4329, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 6200/18276: Loss = 2.3099 (C: 14.4280, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 6300/18276: Loss = 2.3109 (C: 14.4342, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 6400/18276: Loss = 2.3106 (C: 14.4326, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 6500/18276: Loss = 2.3112 (C: 14.4361, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 6600/18276: Loss = 2.3102 (C: 14.4299, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 6700/18276: Loss = 2.3110 (C: 14.4353, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 6800/18276: Loss = 2.3109 (C: 14.4341, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 6900/18276: Loss = 2.3121 (C: 14.4418, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
mu range: [-5.063773, 7.007408]
mu std: 2.186095
Batch 7000/18276: Loss = 2.3088 (C: 14.4211, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 7100/18276: Loss = 2.3105 (C: 14.4316, R: 0.0015, KL: 0.0000) | β=0.160, γ=0.0000
Batch 7200/18276: Loss = 2.3109 (C: 14.4346, R: 0.0013, KL: 0.0000) | β=0.160, γ=0.0000
Batch 7300/18276: Loss = 2.3118 (C: 14.4401, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 7400/18276: Loss = 2.3096 (C: 14.4262, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 7500/18276: Loss = 2.3111 (C: 14.4355, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 7600/18276: Loss = 2.3098 (C: 14.4271, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 7700/18276: Loss = 2.3102 (C: 14.4298, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 7800/18276: Loss = 2.3115 (C: 14.4379, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 7900/18276: Loss = 2.3110 (C: 14.4347, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 8000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.549563, 7.655598]
mu std: 2.373051

=== DIAGNOSTIC AT BATCH 8000 ===
Contrastive loss stuck at: 14.4373
Contrastive weight (β): 0.1600
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.950309
  Max: 1.000000
  Mean: 0.978045
  Std: 0.007442
Average feature variance: 0.000303
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.976906
  Std: 0.006033

Negative pair similarities:
  Mean: 0.977460
  Std: 0.006459

Separation (pos_mean - neg_mean): -0.000554
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.63, 9.88]
  Negative similarities: [9.50, 9.89]
=== END DIAGNOSTIC ===

Batch 8000/18276: Loss = 2.3114 (C: 14.4373, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 8100/18276: Loss = 2.3116 (C: 14.4386, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 8200/18276: Loss = 2.3111 (C: 14.4359, R: 0.0013, KL: 0.0000) | β=0.160, γ=0.0000
Batch 8300/18276: Loss = 2.3106 (C: 14.4324, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 8400/18276: Loss = 2.3100 (C: 14.4288, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 8500/18276: Loss = 2.3103 (C: 14.4308, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 8600/18276: Loss = 2.3097 (C: 14.4272, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 8700/18276: Loss = 2.3110 (C: 14.4354, R: 0.0013, KL: 0.0000) | β=0.160, γ=0.0000
Batch 8800/18276: Loss = 2.3116 (C: 14.4387, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 8900/18276: Loss = 2.3109 (C: 14.4342, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
mu range: [-5.008623, 6.821749]
mu std: 2.125693
Batch 9000/18276: Loss = 2.3100 (C: 14.4290, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 9100/18276: Loss = 2.3112 (C: 14.4362, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 9200/18276: Loss = 2.3113 (C: 14.4369, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 9300/18276: Loss = 2.3107 (C: 14.4331, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 9400/18276: Loss = 2.3097 (C: 14.4266, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 9500/18276: Loss = 2.3106 (C: 14.4326, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 9600/18276: Loss = 2.3103 (C: 14.4308, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 9700/18276: Loss = 2.3105 (C: 14.4320, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 9800/18276: Loss = 2.3094 (C: 14.4254, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 9900/18276: Loss = 2.3100 (C: 14.4290, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 10000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.412834, 7.488541]
mu std: 2.160536

=== DIAGNOSTIC AT BATCH 10000 ===
Contrastive loss stuck at: 14.4354
Contrastive weight (β): 0.1600
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.936738
  Max: 1.000000
  Mean: 0.968707
  Std: 0.009959
Average feature variance: 0.000432
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.967407
  Std: 0.009051

Negative pair similarities:
  Mean: 0.967727
  Std: 0.007828

Separation (pos_mean - neg_mean): -0.000319
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.42, 9.84]
  Negative similarities: [9.37, 9.87]
=== END DIAGNOSTIC ===

Batch 10000/18276: Loss = 2.3111 (C: 14.4354, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 10100/18276: Loss = 2.3094 (C: 14.4251, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 10200/18276: Loss = 2.3111 (C: 14.4354, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 10300/18276: Loss = 2.3101 (C: 14.4298, R: 0.0013, KL: 0.0000) | β=0.160, γ=0.0000
Batch 10400/18276: Loss = 2.3102 (C: 14.4299, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 10500/18276: Loss = 2.3106 (C: 14.4325, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 10600/18276: Loss = 2.3096 (C: 14.4265, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 10700/18276: Loss = 2.3117 (C: 14.4391, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 10800/18276: Loss = 2.3094 (C: 14.4252, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 10900/18276: Loss = 2.3110 (C: 14.4349, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
mu range: [-5.249104, 7.230667]
mu std: 2.172430
Batch 11000/18276: Loss = 2.3097 (C: 14.4274, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 11100/18276: Loss = 2.3102 (C: 14.4302, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 11200/18276: Loss = 2.3112 (C: 14.4365, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 11300/18276: Loss = 2.3112 (C: 14.4361, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 11400/18276: Loss = 2.3103 (C: 14.4309, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 11500/18276: Loss = 2.3097 (C: 14.4268, R: 0.0015, KL: 0.0000) | β=0.160, γ=0.0000
Batch 11600/18276: Loss = 2.3106 (C: 14.4324, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 11700/18276: Loss = 2.3101 (C: 14.4292, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 11800/18276: Loss = 2.3101 (C: 14.4293, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 11900/18276: Loss = 2.3111 (C: 14.4355, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 12000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.190935, 7.190008]
mu std: 2.218127

=== DIAGNOSTIC AT BATCH 12000 ===
Contrastive loss stuck at: 14.4322
Contrastive weight (β): 0.1600
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.946109
  Max: 1.000000
  Mean: 0.970405
  Std: 0.009046
Average feature variance: 0.000408
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.969378
  Std: 0.007475

Negative pair similarities:
  Mean: 0.969387
  Std: 0.007235

Separation (pos_mean - neg_mean): -0.000010
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.46, 9.84]
  Negative similarities: [9.51, 9.85]
=== END DIAGNOSTIC ===

Batch 12000/18276: Loss = 2.3106 (C: 14.4322, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 12100/18276: Loss = 2.3109 (C: 14.4340, R: 0.0015, KL: 0.0000) | β=0.160, γ=0.0000
Batch 12200/18276: Loss = 2.3111 (C: 14.4360, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 12300/18276: Loss = 2.3110 (C: 14.4346, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 12400/18276: Loss = 2.3107 (C: 14.4333, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 12500/18276: Loss = 2.3094 (C: 14.4252, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 12600/18276: Loss = 2.3112 (C: 14.4367, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 12700/18276: Loss = 2.3108 (C: 14.4338, R: 0.0013, KL: 0.0000) | β=0.160, γ=0.0000
Batch 12800/18276: Loss = 2.3110 (C: 14.4349, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 12900/18276: Loss = 2.3115 (C: 14.4382, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
mu range: [-5.562806, 7.725577]
mu std: 2.276665
Batch 13000/18276: Loss = 2.3113 (C: 14.4370, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 13100/18276: Loss = 2.3111 (C: 14.4354, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 13200/18276: Loss = 2.3102 (C: 14.4304, R: 0.0013, KL: 0.0000) | β=0.160, γ=0.0000
Batch 13300/18276: Loss = 2.3102 (C: 14.4302, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 13400/18276: Loss = 2.3116 (C: 14.4389, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 13500/18276: Loss = 2.3106 (C: 14.4328, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 13600/18276: Loss = 2.3112 (C: 14.4362, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 13700/18276: Loss = 2.3108 (C: 14.4338, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 13800/18276: Loss = 2.3102 (C: 14.4302, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 13900/18276: Loss = 2.3113 (C: 14.4370, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 14000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.957358, 6.868721]
mu std: 2.134436

=== DIAGNOSTIC AT BATCH 14000 ===
Contrastive loss stuck at: 14.4250
Contrastive weight (β): 0.1600
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.940540
  Max: 1.000000
  Mean: 0.967510
  Std: 0.009367
Average feature variance: 0.000448
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.966887
  Std: 0.006507

Negative pair similarities:
  Mean: 0.966166
  Std: 0.007604

Separation (pos_mean - neg_mean): 0.000721
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.46, 9.80]
  Negative similarities: [9.41, 9.82]
=== END DIAGNOSTIC ===

Batch 14000/18276: Loss = 2.3093 (C: 14.4250, R: 0.0013, KL: 0.0000) | β=0.160, γ=0.0000
Batch 14100/18276: Loss = 2.3085 (C: 14.4195, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 14200/18276: Loss = 2.3108 (C: 14.4341, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 14300/18276: Loss = 2.3112 (C: 14.4358, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 14400/18276: Loss = 2.3102 (C: 14.4302, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 14500/18276: Loss = 2.3107 (C: 14.4331, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 14600/18276: Loss = 2.3103 (C: 14.4308, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 14700/18276: Loss = 2.3109 (C: 14.4344, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 14800/18276: Loss = 2.3102 (C: 14.4301, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 14900/18276: Loss = 2.3101 (C: 14.4294, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
mu range: [-4.945319, 6.818558]
mu std: 2.207744
Batch 15000/18276: Loss = 2.3102 (C: 14.4296, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 15100/18276: Loss = 2.3111 (C: 14.4361, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 15200/18276: Loss = 2.3100 (C: 14.4286, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 15300/18276: Loss = 2.3103 (C: 14.4310, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 15400/18276: Loss = 2.3100 (C: 14.4290, R: 0.0013, KL: 0.0000) | β=0.160, γ=0.0000
Batch 15500/18276: Loss = 2.3100 (C: 14.4288, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 15600/18276: Loss = 2.3109 (C: 14.4339, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 15700/18276: Loss = 2.3097 (C: 14.4268, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 15800/18276: Loss = 2.3106 (C: 14.4326, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 15900/18276: Loss = 2.3113 (C: 14.4370, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 16000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.427689, 7.568974]
mu std: 2.375517

=== DIAGNOSTIC AT BATCH 16000 ===
Contrastive loss stuck at: 14.4292
Contrastive weight (β): 0.1600
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.952763
  Max: 1.000000
  Mean: 0.979208
  Std: 0.006646
Average feature variance: 0.000287
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.978665
  Std: 0.005218

Negative pair similarities:
  Mean: 0.978413
  Std: 0.005625

Separation (pos_mean - neg_mean): 0.000252
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.59, 9.88]
  Negative similarities: [9.53, 9.87]
=== END DIAGNOSTIC ===

Batch 16000/18276: Loss = 2.3100 (C: 14.4292, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 16100/18276: Loss = 2.3100 (C: 14.4288, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 16200/18276: Loss = 2.3094 (C: 14.4253, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 16300/18276: Loss = 2.3106 (C: 14.4328, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 16400/18276: Loss = 2.3103 (C: 14.4307, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 16500/18276: Loss = 2.3095 (C: 14.4258, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 16600/18276: Loss = 2.3103 (C: 14.4307, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 16700/18276: Loss = 2.3102 (C: 14.4297, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 16800/18276: Loss = 2.3112 (C: 14.4359, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 16900/18276: Loss = 2.3099 (C: 14.4283, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
mu range: [-5.335205, 7.359405]
mu std: 2.240173
Batch 17000/18276: Loss = 2.3109 (C: 14.4347, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 17100/18276: Loss = 2.3101 (C: 14.4294, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 17200/18276: Loss = 2.3105 (C: 14.4324, R: 0.0013, KL: 0.0000) | β=0.160, γ=0.0000
Batch 17300/18276: Loss = 2.3102 (C: 14.4297, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 17400/18276: Loss = 2.3100 (C: 14.4287, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 17500/18276: Loss = 2.3107 (C: 14.4327, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 17600/18276: Loss = 2.3102 (C: 14.4298, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 17700/18276: Loss = 2.3108 (C: 14.4335, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 17800/18276: Loss = 2.3107 (C: 14.4332, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 17900/18276: Loss = 2.3099 (C: 14.4282, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 18000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.316517, 7.309430]
mu std: 2.286915

=== DIAGNOSTIC AT BATCH 18000 ===
Contrastive loss stuck at: 14.4306
Contrastive weight (β): 0.1600
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.953310
  Max: 1.000000
  Mean: 0.975460
  Std: 0.007556
Average feature variance: 0.000338
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.974702
  Std: 0.006381

Negative pair similarities:
  Mean: 0.974575
  Std: 0.006016

Separation (pos_mean - neg_mean): 0.000127
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.57, 9.88]
  Negative similarities: [9.53, 9.86]
=== END DIAGNOSTIC ===

Batch 18000/18276: Loss = 2.3103 (C: 14.4306, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 18100/18276: Loss = 2.3099 (C: 14.4282, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
Batch 18200/18276: Loss = 2.3107 (C: 14.4332, R: 0.0014, KL: 0.0000) | β=0.160, γ=0.0000
DEBUG: validate_epoch called with β=0.160, γ=0.0000
DEBUG: First validation batch:
  contrastive_loss raw: 14.439152717590332
  reconstruction_loss raw: 0.0014417363563552499
  kl_loss raw: 0.0
  total_loss raw: 2.311706066131592
Epoch 5 completed in 93.22s
Train Loss: 2.3105 (C: 14.4320, R: 0.0014) (β=0.160)
Val Loss: 2.3105 (C: 14.4318, R: 0.0014)
No improvement for 4 epochs

Epoch 6/50
------------------------------
Batch 0 labels: {0: 10, 1: 10, 2: 10}
mu range: [-4.951632, 6.857914]
mu std: 2.143093
Batch 0/18276: Loss = 2.8870 (C: 14.4279, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 100/18276: Loss = 2.8897 (C: 14.4415, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 200/18276: Loss = 2.8878 (C: 14.4318, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 300/18276: Loss = 2.8874 (C: 14.4300, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 400/18276: Loss = 2.8879 (C: 14.4326, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 500/18276: Loss = 2.8885 (C: 14.4355, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 600/18276: Loss = 2.8869 (C: 14.4279, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 700/18276: Loss = 2.8885 (C: 14.4358, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 800/18276: Loss = 2.8871 (C: 14.4288, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 900/18276: Loss = 2.8875 (C: 14.4308, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
mu range: [-5.204622, 7.143826]
mu std: 2.207027
Batch 1000/18276: Loss = 2.8875 (C: 14.4303, R: 0.0015, KL: 0.0000) | β=0.200, γ=0.0000
Batch 1100/18276: Loss = 2.8894 (C: 14.4400, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 1200/18276: Loss = 2.8881 (C: 14.4335, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 1300/18276: Loss = 2.8869 (C: 14.4276, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 1400/18276: Loss = 2.8880 (C: 14.4331, R: 0.0013, KL: 0.0000) | β=0.200, γ=0.0000
Batch 1500/18276: Loss = 2.8883 (C: 14.4343, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 1600/18276: Loss = 2.8898 (C: 14.4418, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 1700/18276: Loss = 2.8881 (C: 14.4334, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 1800/18276: Loss = 2.8872 (C: 14.4290, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 1900/18276: Loss = 2.8907 (C: 14.4465, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 2000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.671923, 7.853585]
mu std: 2.350908

=== DIAGNOSTIC AT BATCH 2000 ===
Contrastive loss stuck at: 14.4345
Contrastive weight (β): 0.2000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.955418
  Max: 1.000000
  Mean: 0.976450
  Std: 0.007899
Average feature variance: 0.000325
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.975458
  Std: 0.006854

Negative pair similarities:
  Mean: 0.975720
  Std: 0.006618

Separation (pos_mean - neg_mean): -0.000262
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.58, 9.90]
  Negative similarities: [9.55, 9.91]
=== END DIAGNOSTIC ===

Batch 2000/18276: Loss = 2.8882 (C: 14.4345, R: 0.0013, KL: 0.0000) | β=0.200, γ=0.0000
Batch 2100/18276: Loss = 2.8893 (C: 14.4394, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 2200/18276: Loss = 2.8874 (C: 14.4301, R: 0.0013, KL: 0.0000) | β=0.200, γ=0.0000
Batch 2300/18276: Loss = 2.8884 (C: 14.4348, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 2400/18276: Loss = 2.8881 (C: 14.4332, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 2500/18276: Loss = 2.8877 (C: 14.4314, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 2600/18276: Loss = 2.8874 (C: 14.4300, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 2700/18276: Loss = 2.8894 (C: 14.4397, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 2800/18276: Loss = 2.8895 (C: 14.4405, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 2900/18276: Loss = 2.8878 (C: 14.4323, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
mu range: [-5.534521, 7.742780]
mu std: 2.407088
Batch 3000/18276: Loss = 2.8874 (C: 14.4300, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 3100/18276: Loss = 2.8883 (C: 14.4344, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 3200/18276: Loss = 2.8870 (C: 14.4284, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 3300/18276: Loss = 2.8872 (C: 14.4289, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 3400/18276: Loss = 2.8866 (C: 14.4257, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 3500/18276: Loss = 2.8888 (C: 14.4373, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 3600/18276: Loss = 2.8868 (C: 14.4272, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 3700/18276: Loss = 2.8877 (C: 14.4317, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 3800/18276: Loss = 2.8870 (C: 14.4281, R: 0.0013, KL: 0.0000) | β=0.200, γ=0.0000
Batch 3900/18276: Loss = 2.8888 (C: 14.4371, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 4000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.258730, 7.269175]
mu std: 2.260039

=== DIAGNOSTIC AT BATCH 4000 ===
Contrastive loss stuck at: 14.4363
Contrastive weight (β): 0.2000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.955612
  Max: 1.000000
  Mean: 0.974854
  Std: 0.007813
Average feature variance: 0.000347
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.973677
  Std: 0.006776

Negative pair similarities:
  Mean: 0.974127
  Std: 0.006179

Separation (pos_mean - neg_mean): -0.000450
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.57, 9.89]
  Negative similarities: [9.56, 9.88]
=== END DIAGNOSTIC ===

Batch 4000/18276: Loss = 2.8886 (C: 14.4363, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 4100/18276: Loss = 2.8884 (C: 14.4353, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 4200/18276: Loss = 2.8872 (C: 14.4289, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 4300/18276: Loss = 2.8873 (C: 14.4297, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 4400/18276: Loss = 2.8880 (C: 14.4328, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 4500/18276: Loss = 2.8878 (C: 14.4322, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 4600/18276: Loss = 2.8877 (C: 14.4313, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 4700/18276: Loss = 2.8875 (C: 14.4305, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 4800/18276: Loss = 2.8884 (C: 14.4351, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 4900/18276: Loss = 2.8874 (C: 14.4301, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
mu range: [-5.796713, 8.050591]
mu std: 2.404012
Batch 5000/18276: Loss = 2.8887 (C: 14.4369, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 5100/18276: Loss = 2.8868 (C: 14.4272, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 5200/18276: Loss = 2.8885 (C: 14.4354, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 5300/18276: Loss = 2.8866 (C: 14.4259, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 5400/18276: Loss = 2.8873 (C: 14.4292, R: 0.0015, KL: 0.0000) | β=0.200, γ=0.0000
Batch 5500/18276: Loss = 2.8875 (C: 14.4306, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 5600/18276: Loss = 2.8878 (C: 14.4321, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 5700/18276: Loss = 2.8874 (C: 14.4299, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 5800/18276: Loss = 2.8872 (C: 14.4294, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 5900/18276: Loss = 2.8876 (C: 14.4310, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 6000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.352818, 7.383934]
mu std: 2.268034

=== DIAGNOSTIC AT BATCH 6000 ===
Contrastive loss stuck at: 14.4283
Contrastive weight (β): 0.2000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.941961
  Max: 1.000000
  Mean: 0.973039
  Std: 0.008115
Average feature variance: 0.000372
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.972361
  Std: 0.007765

Negative pair similarities:
  Mean: 0.971996
  Std: 0.005835

Separation (pos_mean - neg_mean): 0.000365
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.42, 9.85]
  Negative similarities: [9.55, 9.85]
=== END DIAGNOSTIC ===

Batch 6000/18276: Loss = 2.8871 (C: 14.4283, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 6100/18276: Loss = 2.8865 (C: 14.4257, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 6200/18276: Loss = 2.8875 (C: 14.4307, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 6300/18276: Loss = 2.8882 (C: 14.4343, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 6400/18276: Loss = 2.8889 (C: 14.4373, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 6500/18276: Loss = 2.8877 (C: 14.4314, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 6600/18276: Loss = 2.8868 (C: 14.4271, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 6700/18276: Loss = 2.8878 (C: 14.4321, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 6800/18276: Loss = 2.8883 (C: 14.4348, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 6900/18276: Loss = 2.8878 (C: 14.4322, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
mu range: [-5.299992, 7.401892]
mu std: 2.254598
Batch 7000/18276: Loss = 2.8874 (C: 14.4301, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 7100/18276: Loss = 2.8889 (C: 14.4375, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 7200/18276: Loss = 2.8882 (C: 14.4337, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 7300/18276: Loss = 2.8875 (C: 14.4308, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 7400/18276: Loss = 2.8875 (C: 14.4304, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 7500/18276: Loss = 2.8875 (C: 14.4305, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 7600/18276: Loss = 2.8876 (C: 14.4313, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 7700/18276: Loss = 2.8885 (C: 14.4355, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 7800/18276: Loss = 2.8871 (C: 14.4285, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 7900/18276: Loss = 2.8878 (C: 14.4319, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 8000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.915138, 8.307375]
mu std: 2.376998

=== DIAGNOSTIC AT BATCH 8000 ===
Contrastive loss stuck at: 14.4343
Contrastive weight (β): 0.2000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.954117
  Max: 1.000000
  Mean: 0.979283
  Std: 0.007111
Average feature variance: 0.000286
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.978395
  Std: 0.006431

Negative pair similarities:
  Mean: 0.978647
  Std: 0.005921

Separation (pos_mean - neg_mean): -0.000252
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.54, 9.91]
  Negative similarities: [9.58, 9.90]
=== END DIAGNOSTIC ===

Batch 8000/18276: Loss = 2.8882 (C: 14.4343, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 8100/18276: Loss = 2.8868 (C: 14.4272, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 8200/18276: Loss = 2.8885 (C: 14.4355, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 8300/18276: Loss = 2.8877 (C: 14.4318, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 8400/18276: Loss = 2.8882 (C: 14.4341, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 8500/18276: Loss = 2.8878 (C: 14.4319, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 8600/18276: Loss = 2.8877 (C: 14.4312, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 8700/18276: Loss = 2.8887 (C: 14.4368, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 8800/18276: Loss = 2.8880 (C: 14.4329, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 8900/18276: Loss = 2.8886 (C: 14.4363, R: 0.0013, KL: 0.0000) | β=0.200, γ=0.0000
mu range: [-5.038209, 7.024341]
mu std: 2.208856
Batch 9000/18276: Loss = 2.8877 (C: 14.4314, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 9100/18276: Loss = 2.8886 (C: 14.4358, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 9200/18276: Loss = 2.8889 (C: 14.4371, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 9300/18276: Loss = 2.8879 (C: 14.4328, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 9400/18276: Loss = 2.8884 (C: 14.4350, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 9500/18276: Loss = 2.8878 (C: 14.4323, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 9600/18276: Loss = 2.8873 (C: 14.4296, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 9700/18276: Loss = 2.8879 (C: 14.4326, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 9800/18276: Loss = 2.8872 (C: 14.4289, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 9900/18276: Loss = 2.8869 (C: 14.4274, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 10000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.591541, 7.777140]
mu std: 2.282179

=== DIAGNOSTIC AT BATCH 10000 ===
Contrastive loss stuck at: 14.4396
Contrastive weight (β): 0.2000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.948985
  Max: 1.000000
  Mean: 0.974744
  Std: 0.008502
Average feature variance: 0.000348
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.973338
  Std: 0.007537

Negative pair similarities:
  Mean: 0.974114
  Std: 0.007053

Separation (pos_mean - neg_mean): -0.000775
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.49, 9.87]
  Negative similarities: [9.51, 9.89]
=== END DIAGNOSTIC ===

Batch 10000/18276: Loss = 2.8893 (C: 14.4396, R: 0.0013, KL: 0.0000) | β=0.200, γ=0.0000
Batch 10100/18276: Loss = 2.8882 (C: 14.4336, R: 0.0015, KL: 0.0000) | β=0.200, γ=0.0000
Batch 10200/18276: Loss = 2.8870 (C: 14.4282, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 10300/18276: Loss = 2.8882 (C: 14.4342, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 10400/18276: Loss = 2.8884 (C: 14.4349, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 10500/18276: Loss = 2.8876 (C: 14.4310, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 10600/18276: Loss = 2.8865 (C: 14.4257, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 10700/18276: Loss = 2.8871 (C: 14.4282, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 10800/18276: Loss = 2.8872 (C: 14.4293, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 10900/18276: Loss = 2.8885 (C: 14.4355, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
mu range: [-5.917620, 8.293596]
mu std: 2.482440
Batch 11000/18276: Loss = 2.8880 (C: 14.4330, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 11100/18276: Loss = 2.8876 (C: 14.4308, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 11200/18276: Loss = 2.8873 (C: 14.4294, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 11300/18276: Loss = 2.8882 (C: 14.4340, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 11400/18276: Loss = 2.8879 (C: 14.4326, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 11500/18276: Loss = 2.8881 (C: 14.4336, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 11600/18276: Loss = 2.8879 (C: 14.4327, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 11700/18276: Loss = 2.8884 (C: 14.4352, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 11800/18276: Loss = 2.8876 (C: 14.4311, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 11900/18276: Loss = 2.8877 (C: 14.4316, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 12000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-5.430676, 7.653440]
mu std: 2.325105

=== DIAGNOSTIC AT BATCH 12000 ===
Contrastive loss stuck at: 14.4335
Contrastive weight (β): 0.2000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.957762
  Max: 1.000000
  Mean: 0.978141
  Std: 0.006631
Average feature variance: 0.000301
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.977262
  Std: 0.005855

Negative pair similarities:
  Mean: 0.977444
  Std: 0.005083

Separation (pos_mean - neg_mean): -0.000183
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.59, 9.87]
  Negative similarities: [9.58, 9.87]
=== END DIAGNOSTIC ===

Batch 12000/18276: Loss = 2.8881 (C: 14.4335, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 12100/18276: Loss = 2.8881 (C: 14.4333, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 12200/18276: Loss = 2.8873 (C: 14.4296, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 12300/18276: Loss = 2.8866 (C: 14.4263, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 12400/18276: Loss = 2.8887 (C: 14.4365, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 12500/18276: Loss = 2.8872 (C: 14.4288, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 12600/18276: Loss = 2.8874 (C: 14.4298, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 12700/18276: Loss = 2.8868 (C: 14.4268, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 12800/18276: Loss = 2.8878 (C: 14.4324, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 12900/18276: Loss = 2.8872 (C: 14.4289, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
mu range: [-5.291297, 7.345980]
mu std: 2.281856
Batch 13000/18276: Loss = 2.8881 (C: 14.4339, R: 0.0013, KL: 0.0000) | β=0.200, γ=0.0000
Batch 13100/18276: Loss = 2.8874 (C: 14.4301, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 13200/18276: Loss = 2.8877 (C: 14.4317, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 13300/18276: Loss = 2.8895 (C: 14.4407, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 13400/18276: Loss = 2.8894 (C: 14.4401, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 13500/18276: Loss = 2.8879 (C: 14.4327, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 13600/18276: Loss = 2.8890 (C: 14.4375, R: 0.0015, KL: 0.0000) | β=0.200, γ=0.0000
Batch 13700/18276: Loss = 2.8873 (C: 14.4295, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 13800/18276: Loss = 2.8872 (C: 14.4293, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 13900/18276: Loss = 2.8873 (C: 14.4296, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 14000 labels: {0: 10, 1: 10, 2: 10}
mu range: [-6.193077, 8.728645]
mu std: 2.509756

=== DIAGNOSTIC AT BATCH 14000 ===
Contrastive loss stuck at: 14.4284
Contrastive weight (β): 0.2000
=== CONTRASTIVE PLATEAU DIAGNOSIS ===
Similarity matrix stats:
  Min: 0.964636
  Max: 1.000000
  Mean: 0.982981
  Std: 0.006840
Average feature variance: 0.000235
⚠️  PROBLEM: Features have very low variance - encoder produces similar outputs

Positive pair similarities:
  Mean: 0.982629
  Std: 0.006989

Negative pair similarities:
  Mean: 0.982288
  Std: 0.005766

Separation (pos_mean - neg_mean): 0.000341
⚠️  PROBLEM: Very poor separation between positive and negative pairs
   The encoder isn't learning class-discriminative features

After temperature scaling (T=0.1):
  Positive similarities: [9.65, 9.92]
  Negative similarities: [9.65, 9.94]
=== END DIAGNOSTIC ===

Batch 14000/18276: Loss = 2.8870 (C: 14.4284, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 14100/18276: Loss = 2.8881 (C: 14.4339, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 14200/18276: Loss = 2.8876 (C: 14.4310, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
Batch 14300/18276: Loss = 2.8888 (C: 14.4369, R: 0.0014, KL: 0.0000) | β=0.200, γ=0.0000
